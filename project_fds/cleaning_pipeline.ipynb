{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d481ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.csv as pac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from project_fds.location_cleaner import LocationCleaner\n",
    "from collections import Counter\n",
    "from scipy.stats import ks_2samp\n",
    "import re\n",
    "import gc\n",
    "import ast\n",
    "from dateutil import parser\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68177d",
   "metadata": {},
   "source": [
    "# **1. ANIME TABLE CLEANING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e8b78",
   "metadata": {},
   "source": [
    "## Preprocessing, comparison, cleaning and merging of datasets: **azatoth42** (full version), **azatoth42** (cleaned version) & **hernan4444** (full version):\n",
    "\n",
    "* we will call **azathoth42** dataset as ***original***, **azathoth42-cleaned** as ***main*** dataset, and **hernan4444** as ***secondary*** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bff07",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "Number of unique titles:\n",
    "\n",
    "**original**: **14478**\n",
    "\n",
    "**main**: **6668**\n",
    "\n",
    "**secondary**: **17562**\n",
    "\n",
    "========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a82f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yaroslav/FCUL/MARS_1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e2101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_df = pd.read_csv('data/datasets/anime_hernan4444/anime.csv')\n",
    "original_df = pd.read_csv('data/datasets/anime_azathoth42/AnimeList.csv')\n",
    "main_df = pd.read_csv('data/datasets/anime_azathoth42/anime_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c161af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secondary dataset columns:\n",
      "----------------\n",
      "Index(['MAL_ID', 'Name', 'Score', 'Genres', 'English name', 'Japanese name',\n",
      "       'Type', 'Episodes', 'Aired', 'Premiered', 'Producers', 'Licensors',\n",
      "       'Studios', 'Source', 'Duration', 'Rating', 'Ranked', 'Popularity',\n",
      "       'Members', 'Favorites', 'Watching', 'Completed', 'On-Hold', 'Dropped',\n",
      "       'Plan to Watch', 'Score-10', 'Score-9', 'Score-8', 'Score-7', 'Score-6',\n",
      "       'Score-5', 'Score-4', 'Score-3', 'Score-2', 'Score-1'],\n",
      "      dtype='object')\n",
      "\n",
      "original dataset columns:\n",
      "----------------\n",
      "Index(['anime_id', 'title', 'title_english', 'title_japanese',\n",
      "       'title_synonyms', 'image_url', 'type', 'source', 'episodes', 'status',\n",
      "       'airing', 'aired_string', 'aired', 'duration', 'rating', 'score',\n",
      "       'scored_by', 'rank', 'popularity', 'members', 'favorites', 'background',\n",
      "       'premiered', 'broadcast', 'related', 'producer', 'licensor', 'studio',\n",
      "       'genre', 'opening_theme', 'ending_theme'],\n",
      "      dtype='object')\n",
      "\n",
      "main dataset columns:\n",
      "----------------\n",
      "Index(['anime_id', 'title', 'title_english', 'title_japanese',\n",
      "       'title_synonyms', 'image_url', 'type', 'source', 'episodes', 'status',\n",
      "       'airing', 'aired_string', 'aired', 'duration', 'rating', 'score',\n",
      "       'scored_by', 'rank', 'popularity', 'members', 'favorites', 'background',\n",
      "       'premiered', 'broadcast', 'related', 'producer', 'licensor', 'studio',\n",
      "       'genre', 'opening_theme', 'ending_theme', 'duration_min',\n",
      "       'aired_from_year'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of unique anime:\n",
      "secondary: 17562\n",
      "original: 14478\n",
      "main: 6668\n"
     ]
    }
   ],
   "source": [
    "print(f\"secondary dataset columns:\\n----------------\\n{secondary_df.columns}\")\n",
    "print(f\"\\noriginal dataset columns:\\n----------------\\n{original_df.columns}\")\n",
    "print(f\"\\nmain dataset columns:\\n----------------\\n{main_df.columns}\")\n",
    "print(\"\\nNumber of unique anime:\")\n",
    "print(f\"secondary: {len(set(secondary_df[\"MAL_ID\"]))}\")\n",
    "print(f\"original: {len(set(original_df[\"anime_id\"]))}\")\n",
    "print(f\"main: {len(set(main_df[\"anime_id\"]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0b89b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "### Get only the columns we will work with\n",
    "\n",
    "- We should remove columns related to ***secondary*** dataset which based in any sense on users or user votes, because we use the users and users votes tables only from ***main***. These are: score_X_secondary. Also we remove columns \"background_main\" and \"broadcast_main\".\n",
    "\n",
    "There are more of them. But discus them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e64d5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Primarly in secondary:\n",
    "==========================\n",
    "'MAL_ID', 'Name', 'Score', 'Genres', 'English name', 'Japanese name',\n",
    "       'Type', 'Episodes', 'Aired', 'Premiered', 'Producers', 'Licensors',\n",
    "       'Studios', 'Source', 'Duration', 'Rating', 'Ranked', 'Popularity',\n",
    "       'Members', 'Favorites', 'Watching', 'Completed', 'On-Hold', 'Dropped',\n",
    "       'Plan to Watch', 'Score-10', 'Score-9', 'Score-8', 'Score-7', 'Score-6',\n",
    "       'Score-5', 'Score-4', 'Score-3', 'Score-2', 'Score-1'\n",
    "\"\"\"\n",
    "secondary_df.drop(\n",
    "    columns=['Japanese name','Producers', 'Licensors', 'Ranked', 'Popularity', 'Watching',\n",
    "             'Completed', 'On-Hold', 'Dropped', 'Plan to Watch', 'Score-10', 'Score-9',\n",
    "             'Score-8', 'Score-7', 'Score-6', 'Score-5', 'Score-4', 'Score-3', 'Score-2', 'Score-1'],\n",
    "             inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c76785bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Primarly in main:\n",
    "==========================\n",
    "'anime_id', 'title', 'title_english', 'title_japanese',\n",
    "       'title_synonyms', 'image_url', 'type', 'source', 'episodes', 'status',\n",
    "       'airing', 'aired_string', 'aired', 'duration', 'rating', 'score',\n",
    "       'scored_by', 'rank', 'popularity', 'members', 'favorites', 'background',\n",
    "       'premiered', 'broadcast', 'related', 'producer', 'licensor', 'studio',\n",
    "       'genre', 'opening_theme', 'ending_theme', 'duration_min',\n",
    "       'aired_from_year'\n",
    "\"\"\"\n",
    "main_df.drop(\n",
    "    columns=['title_japanese', 'status', 'airing', 'aired_string', 'duration', 'popularity',\n",
    "             'background', 'broadcast', 'related', 'producer', 'licensor'],\n",
    "             inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4453c",
   "metadata": {},
   "source": [
    "### Metrics comparison\n",
    "##### (**original** (2018) vs **secondary** (2020))\n",
    "\n",
    "We see that p-value is very small due to big amount of titles. In this case it is not suitable metric.\n",
    "KS-statistic is more suitable. It shows a mild drift to bigger scores within 2 years. But almost no drift in average members and average favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1be4176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\tKstestResult(statistic=np.float64(0.11009226884152352), pvalue=np.float64(5.056203583857332e-71), statistic_location=np.float64(5.21), statistic_sign=np.int8(1))\n",
      "Score:\t6.142481696366902, 6.509999194911844\n",
      "Score:\t1.4639305139139216, 0.8866814542368036\n",
      "\n",
      "Members:\tKstestResult(statistic=np.float64(0.0423449633394031), pvalue=np.float64(8.397513523923764e-13), statistic_location=np.float64(0.010360547036883548), statistic_sign=np.int8(1))\n",
      "Members:\t1.586296634889361, 1.973496157450135\n",
      "Members:\t5.178806945428294, 7.133502706100809\n",
      "\n",
      "Favorites:\tKstestResult(statistic=np.float64(0.07978029457698221), pvalue=np.float64(2.2021609447723465e-44), statistic_location=np.float64(5.6941122878943175e-05), statistic_sign=np.int8(-1))\n",
      "Favorites:\t0.021525736033928204, 0.02606458662774464\n",
      "Favorites:\t0.18065091043085962, 0.23137214568274153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_metric = np.array(original_df[original_df['score'] != 'Unknown']['score'], dtype=float)\n",
    "secondary_metric = np.array(secondary_df[secondary_df['Score'] != 'Unknown']['Score'], dtype=float)\n",
    "print(f\"{'Score'}:\\t{ks_2samp(original_metric, secondary_metric)}\")\n",
    "print(f\"{'Score'}:\\t{np.mean(original_metric)}, {np.mean(secondary_metric)}\")\n",
    "print(f\"{'Score'}:\\t{np.std(original_metric)}, {np.std(secondary_metric)}\\n\")\n",
    "\n",
    "for metric in ['Members', 'Favorites']:\n",
    "    original_metric = np.array(original_df[original_df[metric.lower()] != 'Unknown'][metric.lower()], dtype=int) / len(original_df)\n",
    "    secondary_metric = np.array(secondary_df[secondary_df[metric] != 'Unknown'][metric], dtype=int) / len(secondary_df)\n",
    "    print(f\"{metric}:\\t{ks_2samp(original_metric, secondary_metric)}\")\n",
    "    print(f\"{metric}:\\t{np.mean(original_metric)}, {np.mean(secondary_metric)}\")\n",
    "    print(f\"{metric}:\\t{np.std(original_metric)}, {np.std(secondary_metric)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a91ed0",
   "metadata": {},
   "source": [
    "### What changes cleaning made?\n",
    "\n",
    "We see a noticeable shift to bigger scores, average members and average favorites, due to the fact that cleaning primarly acted to the low-scored, almost unknown anime titles.\n",
    "\n",
    "Also we see that after cleaning std of the score was reduced, because cleaned anime are mostly in the right tail, in \"outlier region\", where titles can have only a few votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8308e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:\t6.142481696366902, 6.848998200359928\n",
      "score:\t1.4639305139139216, 0.9273779813068229\n",
      "\n",
      "members:\t1.586296634889361, 7.122130980222672\n",
      "members:\t5.178806945428294, 15.763837029693017\n",
      "\n",
      "favorites:\t0.021525736033928204, 0.100534737083777\n",
      "favorites:\t0.18065091043085962, 0.5733032618744727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_metric = np.array(original_df[original_df['score'] != 'Unknown']['score'], dtype=float)\n",
    "main_metric = np.array(main_df[main_df['score'] != 'Unknown']['score'], dtype=float)\n",
    "print(f\"{'score'}:\\t{np.mean(original_metric)}, {np.mean(main_metric)}\")\n",
    "print(f\"{'score'}:\\t{np.std(original_metric)}, {np.std(main_metric)}\\n\")\n",
    "\n",
    "for metric in ['members', 'favorites']:\n",
    "    original_metric = np.array(original_df[original_df[metric] != 'Unknown'][metric], dtype=int) / len(original_metric)\n",
    "    main_metric = np.array(main_df[main_df[metric] != 'Unknown'][metric], dtype=int) / len(main_metric)\n",
    "    print(f\"{metric}:\\t{np.mean(original_metric)}, {np.mean(main_metric)}\")\n",
    "    print(f\"{metric}:\\t{np.std(original_metric)}, {np.std(main_metric)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12c1573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del original_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2773c8",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "Number of titles from **main**, that are missed in **secondary**: **12**\n",
    "\n",
    "========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e533c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anime ID in main dataset missing for secondary: 12\n",
      "\n",
      "Exact ID's:\n",
      "{34848, 10945, 36519, 36327, 37032, 37005, 12723, 10520, 32089, 31930, 37019, 16287}\n"
     ]
    }
   ],
   "source": [
    "set_of_anime_id_secondary = set(secondary_df[\"MAL_ID\"])\n",
    "set_of_anime_id_main = set(main_df[\"anime_id\"])\n",
    "main_secondary_diff = set_of_anime_id_main - set_of_anime_id_secondary\n",
    "print(f\"Number of anime ID in main dataset missing for secondary: {len(main_secondary_diff)}\")\n",
    "print(f\"\\nExact ID's:\\n{main_secondary_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470400b",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "What if we remove these titles from **main** dataset?\n",
    "\n",
    "We see that the distribution is the same. Removing of these titles is close to the removing of 12 random titles.\n",
    "\n",
    "========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46a53761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=np.float64(0.0008187965772230169), pvalue=np.float64(1.0), statistic_location=np.float64(6.84), statistic_sign=np.int8(1))\n"
     ]
    }
   ],
   "source": [
    "df_cut = main_df[~main_df['anime_id'].isin(main_secondary_diff)]\n",
    "print(ks_2samp(main_df['score'], df_cut['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7eec0",
   "metadata": {},
   "source": [
    "### So, we can use this cut and do a merge. Columns, which are presented in both datasets, we name with suffix *\"_main\"* or *\"_secondary\"*. We will merge them later, using the **secondary** dataset to fill missing values in the **main** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad20c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anime_id', 'title', 'title_english', 'title_synonyms', 'image_url',\n",
       "       'type_main', 'source_main', 'episodes_main', 'aired_main',\n",
       "       'rating_main', 'score_main', 'scored_by', 'rank', 'members_main',\n",
       "       'favorites_main', 'premiered_main', 'studios_main', 'genres_main',\n",
       "       'opening_theme', 'ending_theme', 'duration_min', 'aired_from_year',\n",
       "       'name', 'score_secondary', 'genres_secondary', 'english_name',\n",
       "       'type_secondary', 'episodes_secondary', 'aired_secondary',\n",
       "       'premiered_secondary', 'studios_secondary', 'source_secondary',\n",
       "       'duration', 'rating_secondary', 'members_secondary',\n",
       "       'favorites_secondary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = df_cut.copy()\n",
    "main_df = main_df.rename(columns={'genre': 'genres', 'studio': 'studios'})\n",
    "\n",
    "secondary_df = secondary_df.rename(columns={col: (\"anime_id\" if col == \"MAL_ID\" else f\"{col.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}\") for col in secondary_df.columns})\n",
    "main_df = main_df.rename(columns={col: (\"anime_id\" if col == \"anime_id\" else f\"{col.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}\") for col in main_df.columns})\n",
    "\n",
    "common_columns = set(secondary_df.columns).intersection(set(main_df.columns))\n",
    "common_columns.remove('anime_id')\n",
    "\n",
    "secondary_df = secondary_df.rename(columns={col: (col if col not in common_columns else f\"{col}_secondary\") for col in secondary_df.columns})\n",
    "main_df = main_df.rename(columns={col: (col if col not in common_columns else f\"{col}_main\") for col in main_df.columns})\n",
    "df_main = pd.merge(main_df, secondary_df, on=\"anime_id\", how=\"left\")\n",
    "df_main = df_main.sort_values(\"anime_id\").reset_index(drop=True)\n",
    "df_main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40960f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del secondary_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9161697",
   "metadata": {},
   "source": [
    "### SOURCE, TYPE, STUDIO columns cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b5762",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "Then we will merge the pairs \"X_main\" & \"X_secondary\". For example - \"type\", \"source\", etc.\n",
    "\n",
    "- In the cases of conflicts we will prefer values from **secondary** dataset (if such present) as:\n",
    "\n",
    "- - Blind check (for every column - 10 random manual checks in the MyAnimeList website) shows that in these cases the **secondary** dataset is much more often presents \"the groung truth\" ***in all columns***. By \"ground truth\" we understand real value on the website at the moment of blind check.\n",
    "\n",
    "- - It is logical because the **secondary** dataset is parse 2 years later than the **original**.\n",
    "\n",
    "- In the case of missing values in both datasets we...will see how many it will be.\n",
    "\n",
    "========================================================\n",
    "\n",
    "For the source values we see that the difference mostly related to the misty borders of the, for example, manga and web-manga. Such differences are so speculative. We believe in the **secondary** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b5a0a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source counters: main and secondary:\n",
      "Counter({'Manga': 2490, 'Original': 1785, 'Light novel': 511, 'Visual novel': 481, 'Game': 424, 'Novel': 281, '4-koma manga': 194, 'Other': 169, 'Web manga': 111, 'Music': 55, 'Picture book': 53, 'Book': 45, 'Card game': 45, 'Digital manga': 7, 'Radio': 5})\n",
      "Counter({'Manga': 2479, 'Original': 1785, 'Light novel': 522, 'Visual novel': 482, 'Game': 430, 'Novel': 268, '4-koma manga': 196, 'Other': 172, 'Web manga': 117, 'Picture book': 54, 'Music': 52, 'Book': 44, 'Card game': 43, 'Digital manga': 7, 'Radio': 5})\n",
      "\n",
      "Type counters: main and secondary:\n",
      "Counter({'TV': 2980, 'OVA': 1344, 'Special': 920, 'Movie': 906, 'ONA': 408, 'Music': 98})\n",
      "Counter({'TV': 2976, 'OVA': 1344, 'Special': 914, 'Movie': 912, 'ONA': 412, 'Music': 98})\n",
      "\n",
      "Studio counters: main and secondary:\n",
      "Counter({'Toei Animation': 402, 'Sunrise': 277, 'Madhouse': 243, 'Studio Pierrot': 235, 'J.C.Staff': 233, 'Studio Deen': 204, 'Production I.G': 177, 'A-1 Pictures': 158, 'TMS Entertainment': 154, 'OLM': 135, 'Nippon Animation': 107, 'Gonzo': 107, 'Shaft': 107, 'Bones': 103, 'Kyoto Animation': 97, 'DLE': 90, 'Xebec': 88, 'AIC': 84, 'Brain&#039;s Base': 79, 'Tatsunoko Production': 68, 'Silver Link.': 66, 'Satelight': 65, 'Arms': 63, 'Shin-Ei Animation': 63, 'Doga Kobo': 59, 'Gainax': 57, 'Zexcs': 52, 'feel.': 51, 'ufotable': 51, 'Seven': 46, 'Studio Ghibli': 45, 'PoRO': 42, 'Production Reed': 41, 'Kachidoki Studio': 40, 'Diomedea': 38, 'P.A. Works': 38, 'Studio Gallop': 36, 'Studio Hibari': 35, 'Lerche': 35, 'Artland': 32, 'Group TAC': 31, '8bit': 31, 'Haoliners Animation League': 30, 'Studio Gokumi': 29, 'Wit Studio': 29, 'TNK': 28, 'MAPPA': 28, 'Daume': 27, 'TYO Animations': 27, 'Studio Comet': 26, 'Studio 4°C': 26, 'White Fox': 26, 'Nomad': 25, 'Actas': 25, 'SynergySP': 25, 'T-Rex': 25, 'Tokyo Movie Shinsha': 24, 'Hal Film Maker': 24, 'Kinema Citrus': 24, 'Gathering': 24, 'Tezuka Productions': 23, 'Bee Train': 22, 'Manglobe': 22, 'Production IMS': 22, 'A.C.G.T.': 21, 'Studio Jam': 20, 'Office Takeout': 20, 'LIDENFILMS': 20, 'Polygon Pictures': 20, 'David Production': 19, 'Bandai Namco Pictures': 19, 'Ajia-Do': 18, 'Mushi Production': 18, 'Asread': 18, 'GoHands': 18, 'Trigger': 18, 'Telecom Animation Film': 17, 'Magic Bus': 17, 'Asahi Production': 17, 'AIC Plus+': 17, 'RG Animation Studios': 17, 'Seven Arcs': 16, 'AIC A.S.T.A.': 16, 'Eiken': 16, 'Seven Arcs Pictures': 16, 'APPP': 15, 'Y.O.U.C': 15, 'Flavors Soft': 15, 'Hoods Entertainment': 15, 'Fanworks': 15, 'dwarf': 15, 'Sparkly Key Animation Studio': 15, 'Millepensee': 15, 'Studio Fantasia': 14, 'Studio 9 MAiami': 14, 'Studio PuYUKAI': 14, 'Shuka': 14, 'Creators in Pack': 14, 'AT-2': 13, 'ILCA': 13, 'CoMix Wave Films': 12, 'Production I.G, Xebec': 12, 'Suzuki Mirano': 12, 'Collaboration Works': 12, 'Animate Film': 11, 'SANZIGEN': 11, 'SANZIGEN, LIDENFILMS': 11, 'EMT²': 10, 'Triangle Staff': 9, 'Kanaban Graphics': 9, 'Passione': 9, 'Pie in The Sky': 9, 'Shanghai Foch Film Culture Investment': 9, 'Radix': 8, 'E&amp;G Films': 8, 'Yumeta Company': 8, 'Pierrot Plus': 8, 'Khara': 8, 'Bridge': 8, 'AIC Build': 8, 'NAZ': 8, 'C-Station': 8, 'Lay-duce': 8, 'Signal. MD': 8, 'G.CMay Animation &amp; Film': 8, 'Trans Arts': 7, 'Tokyo Kids': 7, 'Ashi Production': 7, 'The Answer Studio': 7, 'Studio Eromatick': 7, 'G-Lam': 7, 'Silver Link., Connect': 7, 'Onionskin': 7, 'Digital Frontier': 6, 'feel., Zexcs': 6, 'Gainax, Shaft': 6, 'Pastel': 6, 'PrimeTime': 6, 'Marza Animation Planet': 6, 'Xebec, OLM': 6, 'Graphinica': 6, 'Tatsunoko Production, Dongwoo A&amp;E': 6, 'Studio 1st': 6, 'Yaoyorozu': 6, 'Gakken Eigakyoku': 6, 'Studio 3Hz': 6, 'BreakBottle': 6, 'Pine Jam': 6, 'Minakata Laboratory': 6, 'AIC Spirits': 5, 'WAO World': 5, 'Vega Entertainment': 5, 'Phoenix Entertainment': 5, 'Office Take Off': 5, 'Ishikawa Pro': 5, 'Knack Productions': 5, 'Satelight, A-1 Pictures': 5, 'Shanghai Animation Film Studio': 5, 'Encourage Films': 5, 'Kamikaze Douga': 5, 'Next Media Animation': 5, 'Project No.9': 5, 'Opera House': 5, 'Studio Colorido': 5, 'Fukushima Gainax': 5, 'Zero-G': 5, 'Usagi Ou': 5, 'Pops Inc.': 5, 'pH Studio': 5, 'Studio Gallop, Studio Deen': 4, 'Imagin, Studio Live': 4, 'Chaos Project': 4, 'Studio Rikka': 4, 'OLM Digital': 4, 'AIC, Artmic': 4, 'Oh! Production': 4, 'Artmic': 4, 'AIC, Artmic, Animate Film': 4, 'Telescreen BV': 4, 'Shochiku Animation Institute': 4, 'Production Reed, Asahi Production': 4, 'Blue Cat': 4, 'Milky Cartoon': 4, 'Studio Deen, DAX Production': 4, 'AIC Classic': 4, 'Rising Force': 4, 'Nexus': 4, 'Charaction': 4, 'Production I.G, Madhouse': 4, 'Telecom Animation Film, Graphinica': 4, 'TROYCA': 4, 'L²Studio': 4, 'W-Toon Studio': 4, '10Gauge': 4, 'Xebec, Issen': 4, 'Millepensee, GEMBA': 4, 'Queen Bee': 4, 'Artland, TNK': 4, 'TYPHOON GRAPHICS': 4, 'Science SARU': 4, 'Gainax, Production I.G': 3, 'Palm Studio': 3, 'Square Enix': 3, 'Trinet Entertainment, Picture Magic': 3, 'M.S.C': 3, 'Studio Junio': 3, 'Planet': 3, 'Picture Magic': 3, 'Remic': 3, 'Agent 21': 3, 'Studio Egg': 3, 'Sugar Boy, Blue Cat': 3, 'Venet': 3, 'Image House': 3, 'Lilix': 3, 'Gonzo, Asread': 3, 'Studio Sign': 3, 'Madhouse, TMS Entertainment': 3, 'Topcraft': 3, 'Anime Antenna Iinkai': 3, 'A-1 Pictures, Ordet': 3, 'Bones, Kinema Citrus': 3, 'Satelight, 8bit': 3, 'Brain&#039;s Base, Marvy Jack': 3, 'Egg': 3, 'Ordet': 3, 'ChuChu': 3, 'Tatsunoko Production, SynergySP': 3, 'Studio Pierrot, Pierrot Plus': 3, 'Production I.G, M.S.C': 3, 'Majin': 3, 'Kenji Studio': 3, 'C2C': 3, 'Ordet, LIDENFILMS': 3, 'Barnum Studio, Project No.9': 3, 'Xebec Zwei': 3, 'Madhouse, MAPPA': 3, 'J.C.Staff, A.C.G.T.': 3, 'A-1 Pictures, TROYCA': 3, 'Ascension': 3, 'Steve N&#039; Steven': 3, 'Mili Pictures': 3, 'Studio A-CAT': 3, 'Tomovies': 3, 'Shirogumi': 3, 'Hoods Drifters Studio': 3, 'Oddjob': 3, 'Studio Gokumi, AXsiZ': 3, 'W-Toon Studio, DMM.futureworks': 3, 'Kyotoma': 3, 'Echoes': 3, 'OLM Digital, Signal. MD': 3, 'Platinum Vision': 3, 'Vasoon Animation': 3, 'CG Year': 3, 'Yamamura Animation, Inc.': 3, 'Geno Studio': 3, '2:10 Animation': 3, 'Sunrise, Studio Deen': 2, 'Studio Matrix': 2, 'Gainax, J.C.Staff': 2, 'Madhouse, Imagin': 2, 'Studio Fantasia, Animate Film': 2, 'Gonzo, Production I.G': 2, 'Plum': 2, 'Trinet Entertainment': 2, 'Studio Ghibli, Studio Hibari': 2, 'Actas, SynergySP': 2, 'Radix, Marine Entertainment': 2, 'Kyoto Animation, Tatsunoko Production': 2, 'Studio Flag': 2, 'Madhouse, Telecom Animation Film': 2, 'Studio Live': 2, 'Toei Animation, Production Reed': 2, 'Production I.G, Production Reed': 2, 'Sunrise, Toei Animation': 2, 'Artland, Tatsunoko Production': 2, 'ACC Production': 2, 'Toei Animation, Studio World': 2, 'Madhouse, Studio 4°C': 2, 'Imagin': 2, 'Gonzo, AIC': 2, 'JCF': 2, 'Tsuchida Productions': 2, 'Himajin Planning': 2, 'Sanrio': 2, 'Visual 80': 2, 'Madhouse, Toei Animation': 2, 'Oxybot': 2, 'Pink Pineapple': 2, 'Telecom Animation Film, Shirogumi': 2, 'Eiken, Studio Live': 2, 'Anpro': 2, 'Tatsunoko Production, Studio World': 2, 'Kokusai Eigasha': 2, 'Studio Zero': 2, 'Production I.G, Trans Arts': 2, 'Studio Comet, KeyEast, REALTHING': 2, 'Gainax, feel.': 2, 'Natural High': 2, 'Schoolzone': 2, 'Studio Blanc': 2, 'Daewon Media': 2, 'Life Work': 2, 'A-1 Pictures, Bridge': 2, 'Production I.G, Xebec, OLM': 2, 'WAO World, MooGoo': 2, 'Digital Media Lab': 2, 'J.C.Staff, Artland': 2, 'Gainax, Asahi Production': 2, 'Studio Chizu': 2, 'Hotline': 2, 'Sunrise, Bandai Namco Pictures': 2, 'An DerCen': 2, 'Kyoto Animation, Animation Do': 2, 'Hoods Entertainment, teamKG': 2, 'Tatsunoko Production, Ordet': 2, 'Kinema Citrus, Orange': 2, 'Circle Tribute': 2, 'A-Real': 2, 'TMS Entertainment, Telecom Animation Film': 2, 'Ordet, Millepensee': 2, 'Studio! Cucuri': 2, 'MAPPA, Studio VOLN': 2, 'Studio Binzo': 2, 'Jumondo': 2, 'Qualia Animation': 2, 'Kinema Citrus, EMT²': 2, 'Studio Animal': 2, 'KAGAYA Studio': 2, 'Bridge, Husio Studio': 2, 'Joker Films': 2, 'DR Movie': 2, 'NUT': 2, 'Moss Design Unit': 2, 'Neft Film': 2, 'domerica': 2, 'Orange': 2, 'J.C.Staff, Egg Firm': 2, 'production doA': 2, 'Haoliners Animation League, Pb Animation Co. Ltd.': 2, 'Team YokkyuFuman': 2, 'Bouncy': 2, 'LandQ studios': 2, 'Tengu Kobo': 2, 'Puzzle Animation Studio Limited': 2, 'Thundray': 2, 'GARDEN LODGE': 2, 'M2': 2, 'TMS Entertainment, Jinnis Animation Studios': 2, 'Gathering, Lesprit': 2, 'Sotsu': 2, 'Craftar': 2, 'Haoliners Animation League, Studio LAN': 2, 'drop': 2, 'Actas, Studio 3Hz': 2, 'Jinnis Animation Studios': 2, 'D &amp; D Pictures': 2, 'Light Chaser Animation Studios': 2, 'Gainax, Tatsunoko Production': 1, 'Bee Train, Xebec': 1, 'Xebec, Asread': 1, 'Sunrise, Studio Hibari': 1, 'Sunrise, Nakamura Production': 1, 'ufotable, feel., Studio Flag': 1, 'Gainax, Toei Animation': 1, 'Studio Gallop, Studio Comet': 1, 'Chaos Project, GANSIS': 1, 'Gainax, Madhouse': 1, 'TNK, Production Reed': 1, 'Studio Fantasia, Amber Film Works': 1, 'Picture Magic, Rikuentai': 1, 'TMS Entertainment, Tokyo Kids, Minami Machi Bugyousho': 1, 'Milky Animation Label': 1, 'Group TAC, Japan Vistec': 1, 'Bones, Sunrise': 1, 'G&amp;G Entertainment': 1, 'Arcs Create': 1, 'Kyoto Animation, Sunrise': 1, 'Karaku': 1, 'Satelight, Production Reed': 1, 'OLM, Production Reed': 1, 'Madhouse, Studio Deen': 1, 'Studio 4°C, Sunrise': 1, 'Studio Deen, Studio Hibari, Production Reed': 1, 'Madhouse, feel.': 1, 'OLM, AIC A.S.T.A.': 1, 'Radix, Chaos Project': 1, 'J.C.Staff, Studio Ghibli': 1, 'Group TAC, View Works': 1, 'Madhouse, Satelight, Graphinica': 1, 'Group TAC, Ginga Ya': 1, 'Xebec, Production Reed': 1, 'Studio Pierrot, Arms': 1, 'Artland, Magic Bus': 1, 'Shinkuukan': 1, 'Triangle Staff, Studio Wombat': 1, 'Studio Hibari, Production Reed': 1, 'AIC Spirits, Group TAC': 1, 'Enoki Films, Dai Nippon Printing': 1, 'Gonzo, Satelight': 1, 'Gainax, Studio Deen': 1, 'AIC, APPP': 1, 'Group TAC, Amuse': 1, 'View Works': 1, 'Production I.G, Studio Deen': 1, 'Yamato Works': 1, 'Trinet Entertainment, Studio Hibari': 1, 'Tamura Shigeru Studio': 1, 'Bee Media': 1, 'Madhouse, Production Reed': 1, 'AIC, Animate Film': 1, 'Studio Pierrot, Studio Deen': 1, 'J.C.Staff, Production Reed': 1, 'AIC, Artmic, Darts': 1, 'Darts': 1, 'D.A.S.T.': 1, 'Studio Junio, Annapuru': 1, 'Sunrise, Production Reed': 1, 'Studio Pierrot, AIC A.S.T.A.': 1, 'Ishimori Entertainment': 1, 'Madhouse, Group TAC': 1, 'Shaft, TNK': 1, 'Toei Animation, TMS Entertainment': 1, 'Japan Vistec': 1, 'Echo': 1, 'Ginga Ya': 1, 'Kitty Films': 1, 'J.C.Staff, Animate Film': 1, 'Genco, Radix': 1, 'Studio G-1Neo': 1, 'Studio Pierrot, Ajia-Do': 1, 'Front Line': 1, 'Anime R, Aubec': 1, 'AIC Spirits, BeSTACK': 1, 'Gonzo, Palm Studio': 1, 'Xebec, Group TAC': 1, 'J.C.Staff, Life Work': 1, 'Madhouse, Studio Deen, Magic Bus': 1, 'Artland, AIC, Artmic': 1, 'Mook DLE': 1, 'Studio Gallop, TMS Entertainment': 1, 'Ajia-Do, Studio Deen': 1, 'Gainax, Studio 4°C': 1, 'Big Bang': 1, 'AIC, Studio Hakk': 1, 'SOEISHINSHA': 1, 'Tokyo Kids, Minami Machi Bugyousho': 1, 'PPM': 1, 'AIC, Darts': 1, 'Front Line, Studio G-1Neo': 1, 'Studio Core': 1, 'Studio Wombat': 1, 'Bee Train, Cookie Jar Entertainment': 1, 'Ajia-Do, TMS Entertainment': 1, 'Sunwoo Entertainment': 1, 'Studio Kyuuma, Studio Kikan, Azeta Pictures': 1, 'Madhouse, Studio Fantasia': 1, 'Tatsunoko Production, Production Reed, Asahi Production': 1, 'Panmedia, Meruhensha': 1, 'Gonzo, Gainax, Production I.G, Madhouse, Studio 4°C, Satelight, CoMix Wave Films': 1, 'Japan Taps': 1, 'Studio Bogey': 1, 'Madhouse, Tezuka Productions': 1, 'Artland, Madhouse': 1, 'Piko Studio': 1, 'Think Corporation': 1, 'Studio Unicorn, Hiro Media': 1, 'Kazuki Production': 1, 'Nippon Animation, Production Reed': 1, 'Actas, Brain&#039;s Base': 1, 'Arms, Studio Kikan': 1, 'Bones, Telecom Animation Film': 1, 'J.C.Staff, Production I.G': 1, 'DAX Production': 1, 'Daichi Doga, Dongyang Animation': 1, 'Triple X': 1, 'Toei Animation, Daewon Media': 1, 'Nippon Animation, SynergySP, Shirogumi': 1, 'Bee Train, Production I.G, Madhouse, Studio 4°C': 1, 'Xebec, Kanaban Graphics': 1, 'Sunrise, Kino Production': 1, 'Oz': 1, 'Green Bunny': 1, 'Studio Ghibli, Ajia-Do': 1, 'AIC Spirits, Asread': 1, 'Aubec': 1, 'Madhouse, Tatsunoko Production': 1, 'Tele-Cartoon Japan': 1, 'Ajia-Do, Group TAC': 1, 'Gainax, Magic Bus': 1, 'TMS Entertainment, Tezuka Productions': 1, 'Actas, TMS Entertainment': 1, 'Bee Media, Code': 1, 'AIC Spirits, Digital Frontier': 1, 'Office AO': 1, 'Gonzo, Picture Magic': 1, 'AIC, BeSTACK': 1, 'Kyoto Animation, Production I.G, Shin-Ei Animation': 1, 'Ripple Film': 1, 'Heewon Entertainment': 1, 'Tatsunoko Production, SynergySP, Seven': 1, 'Production I.G, Polygon Pictures': 1, 'Animate Film, Visual 80': 1, 'Picograph': 1, 'OLM, P.A. Works': 1, 'Studio Anima': 1, 'Artmic, Animate Film': 1, 'Tama Production': 1, 'Studio Pierrot, Studio Hibari': 1, 'Studio Take Off': 1, 'Bones, Production I.G, Studio 4°C, Toei Animation': 1, 'Future Planet, Beijing Huihuang Animation Company': 1, 'Indeprox': 1, 'Studio Korumi': 1, 'Asahi Production, Shochiku Animation Institute': 1, 'Sting Ray': 1, 'Arms, TNK': 1, 'Panda Factory, Studio PuYUKAI': 1, 'Hal Film Maker, TYO Animations': 1, 'Bee Media, 81 Produce': 1, 'Sunrise, Ascension': 1, 'Shogakukan Music &amp; Digital Entertainment': 1, 'Studio World': 1, 'Panda Factory': 1, 'Animaruya': 1, 'Studio Pierrot, Shin-Ei Animation': 1, 'Production I.G, Animate Film': 1, 'Studio Bogey, Public &amp; Basic': 1, 'Studio Unicorn': 1, 'Public &amp; Basic, Ripple Film': 1, 'Primastea': 1, 'TAKI Corporation': 1, 'AIC Takarazuka': 1, 'Studio Pierrot, David Production': 1, 'Artland, Nippon Animation': 1, 'Bones, Production Reed': 1, 'Toei Animation, Studio Nue': 1, 'B&amp;T': 1, 'Barnum Studio, Project No.9, Studio Blanc': 1, 'AIC, Remic': 1, 'Studio Fantasia, Rabbit Gate': 1, 'Satelight, ixtl': 1, 'TNK, Kinema Citrus': 1, 'Studio Flag, Studio Bogey': 1, 'Ordet, SANZIGEN': 1, 'Satelight, Encourage Films': 1, 'LMD': 1, 'Madhouse, Studio Gokumi': 1, 'Xebec, AIC': 1, 'J.C.Staff, Kitty Films': 1, 'Marvy Jack': 1, 'Gonzo, DLE': 1, 'Studio Rikka, Purple Cow Studio Japan': 1, 'Tezuka Productions, MAPPA': 1, 'Studio Pierrot, D.A.S.T.': 1, 'Odolttogi': 1, 'NHK': 1, 'Tomason': 1, 'Dongwoo A&amp;E': 1, 'Production I.G, Studio 4°C, Shaft': 1, 'Shueisha': 1, 'Anpro, teamKG': 1, 'Kazami Gakuen Koushiki Douga-bu': 1, 'Fifth Avenue': 1, 'Gonzo, ufotable': 1, 'Mary Jane': 1, 'Doga Kobo, Orange': 1, 'Hoods Entertainment, Production IMS': 1, 'Nippon Animation, Studio WHO': 1, 'J.C.Staff, Toei Animation': 1, 'Studio Pierrot, Studio Gallop': 1, 'AIC Frontier': 1, 'Ordet, Encourage Films': 1, 'MooGoo': 1, 'Studio Deen, Daume': 1, 'Studio Moriken': 1, 'Toei Animation, Tatsunoko Production': 1, 'Studio Pierrot, Kyoto Animation': 1, 'C2C, Lay-duce': 1, 'Shirogumi, Shin-Ei Animation': 1, 'Tatsunoko Production, 10Gauge': 1, 'Project No.9, Tri-Slash': 1, 'Studio Ghibli, Polygon Pictures': 1, 'Studio Comet, Zexcs': 1, 'Manglobe, Geno Studio': 1, 'Fuji TV': 1, 'Studio Deen, Wit Studio': 1, 'MMDGP': 1, 'Tonko House': 1, 'Ordet, Studio Moriken': 1, 'Production I.G, Zexcs': 1, 'Dynamo Pictures': 1, 'J.C.Staff, Nomad': 1, 'Production I.G, DLE': 1, 'J.C.Staff, SANZIGEN': 1, 'Artland, Hoods Entertainment': 1, 'Steve N&#039; Steven, Rockwell Eyes': 1, 'Khara, Trigger': 1, 'Toei Animation, Bridge': 1, 'Shirogumi, Encourage Films': 1, 'Madhouse, Nexus': 1, 'Brain&#039;s Base, Studio A-CAT': 1, 'TMS Entertainment, 3xCube': 1, 'Kinema Citrus, White Fox': 1, 'Media Bank': 1, 'Lerche, 10Gauge': 1, 'Silver Link., Nexus': 1, 'Barnum Studio, Silver Link., Connect': 1, 'Production I.G, OLM': 1, 'Office DCI': 1, 'ixtl, LIDENFILMS': 1, 'HS Pictures Studio': 1, 'Buemon': 1, 'Actas, Bee Media': 1, 'Orange, Studio 3Hz': 1, 'Brain&#039;s Base, Platinum Vision': 1, 'OLM, OLM Digital, Sprite Animation Studios': 1, 'Larx Entertainment': 1, 'TYO Animations, LIDENFILMS': 1, 'AXsiZ': 1, 'Studio Meditation With a Pencil': 1, 'Pollyanna Graphics': 1, 'Kyotoma, Office Nobu': 1, 'KOO-KI': 1, 'Idea Factory': 1, 'Production IMS, Orange': 1, 'Sakura Create': 1, 'Shimogumi': 1, 'Gainax, Fukushima Gainax': 1, 'Seven Arcs, Seven Arcs Pictures': 1, 'October Media': 1, 'OLM, OLM Digital': 1, 'Rabbit Machine': 1, 'CoMix Wave Films, FOREST Hunting One': 1, 'Ekura Animal': 1, 'Orange, Seven Arcs Pictures': 1, 'Asura Film': 1, 'Ordet, W-Toon Studio': 1, 'Robot Communications': 1, 'EDGE': 1, 'Production I.G, OLM, Signal. MD': 1, 'TMS Entertainment, Shin-Ei Animation': 1, 'Namu Animation': 1, 'TUBA': 1, 'Ripromo': 1, 'Satelight, C2C': 1, 'Blade': 1, 'Studio Ppuri': 1, 'Creators in Pack, Namu Animation': 1, 'Chippai': 1, 'Beijing Rocen Digital': 1, 'Will Palette': 1, 'Kamikaze Douga, Nishiki Studio': 1, 'Toei Video': 1, 'Calf Studio': 1, 'TMS Entertainment, TOCSIS': 1, 'TMS Entertainment, Studio Comet': 1, 'Dwango, LIDENFILMS': 1, 'Zexcs, Studio A-CAT': 1, 'PRA': 1, 'Project No.9, A-Real': 1, 'Studio Ponoc': 1, 'Felix Film': 1, 'Studio Zealot': 1, 'Asahi Production, Success Co.': 1, 'SELFISH': 1, 'Rockwell Eyes': 1, 'Tsukimidou': 1, 'Tomoyasu Murata Company': 1, 'Karasfilms': 1, 'Gonzo, DandeLion Animation Studio LLC': 1, 'Chiptune': 1, 'A-Line': 1, 'Piso Studio': 1, 'Fanworks, Imagineer': 1, 'Shin-Ei Animation, DLE': 1, 'iDRAGONS Creative Studio': 1, 'Shirogumi, EMT²': 1, 'Studio Khronos': 1, 'A-1 Pictures, CloverWorks': 1, 'Creators Dot Com': 1, 'Yokohama Animation Lab': 1, 'Diomedea, Studio Blanc': 1, 'Emon, Blade': 1, 'Production I.G, NUT, REVOROOT': 1, 'A-1 Pictures, Trigger, CloverWorks': 1, 'Studio Flad': 1, 'G-Lam, Studio CA': 1, 'Kaname Productions': 1, 'Takara Tomy A.R.T.S': 1, 'Cygames': 1, 'Three-d': 1, 'TNK, Zero-G': 1, 'OLM, Shin-Ei Animation': 1, 'NHK Enterprises': 1, 'KIZAWA Studio': 1, 'Coastline Animation Studio': 1, 'Boyan Pictures': 1, 'Nice Boat Animation': 1, 'pH Studio, D &amp; D Pictures': 1, 'HeART-BIT': 1, 'Strawberry Meets Pictures': 1, 'Creators in Pack, Studio Lings': 1, 'BOOTLEG': 1, 'Eiken, TYO Animations': 1, 'helo.inc': 1, 'Wawayu Animation': 1, 'EKACHI EPILKA': 1, 'Madhouse, DLE': 1, 'G-angle': 1})\n",
      "Counter({'Toei Animation': 401, 'Sunrise': 277, 'Madhouse': 241, 'Studio Pierrot': 233, 'J.C.Staff': 232, 'Studio Deen': 204, 'Production I.G': 172, 'A-1 Pictures': 157, 'TMS Entertainment': 154, 'OLM': 136, 'Shaft': 107, 'Nippon Animation': 106, 'Gonzo': 106, 'Bones': 103, 'Kyoto Animation': 97, 'DLE': 90, 'Xebec': 88, 'AIC': 82, \"Brain's Base\": 79, 'Tatsunoko Production': 69, 'SILVER LINK.': 65, 'Shin-Ei Animation': 64, 'Satelight': 64, 'Arms': 63, 'Doga Kobo': 59, 'Gainax': 56, 'Zexcs': 52, 'ufotable': 51, 'feel.': 48, 'Studio Ghibli': 46, 'Seven': 46, 'PoRO': 42, 'Production Reed': 41, 'Kachidoki Studio': 40, 'Diomedéa': 38, 'P.A. Works': 38, 'Gallop': 35, 'Studio Hibari': 35, 'Lerche': 35, 'Artland': 32, 'Group TAC': 31, '8bit': 30, 'Haoliners Animation League': 30, 'Studio Gokumi': 29, 'Wit Studio': 29, 'TNK': 28, 'Daume': 27, 'TYO Animations': 27, 'MAPPA': 27, 'Studio Comet': 26, 'Studio 4°C': 26, 'White Fox': 26, 'Actas': 25, 'SynergySP': 25, 'T-Rex': 25, 'Tokyo Movie Shinsha': 24, 'Hal Film Maker': 24, 'Nomad': 24, 'Kinema Citrus': 24, 'Bee Train': 23, 'Tezuka Productions': 23, 'Manglobe': 22, 'Production IMS': 22, 'A.C.G.T.': 21, 'Studio Jam': 20, 'Office Takeout': 20, 'LIDENFILMS': 20, 'David Production': 19, 'Gathering': 19, 'Polygon Pictures': 19, 'Bandai Namco Pictures': 19, 'Asread': 18, 'Telecom Animation Film': 18, 'Ajia-Do': 18, 'Magic Bus': 18, 'Mushi Production': 18, 'GoHands': 18, 'Trigger': 18, 'Asahi Production': 17, 'AIC PLUS+': 17, 'RG Animation Studios': 17, 'Seven Arcs': 16, 'AIC ASTA': 16, 'Seven Arcs Pictures': 16, 'APPP': 15, 'Eiken': 15, 'Y.O.U.C': 15, 'Flavors Soft': 15, 'Fanworks': 15, 'Sparkly Key Animation Studio': 15, 'Millepensee': 15, 'Studio Fantasia': 14, 'Studio 9 MAiami': 14, 'dwarf': 14, 'Studio PuYUKAI': 14, 'Shuka': 14, 'Creators in Pack': 14, 'AT-2': 13, 'Hoods Entertainment': 13, 'ILCA': 13, 'CoMix Wave Films': 12, 'Production I.G, Xebec': 12, 'Suzuki Mirano': 12, 'Collaboration Works': 12, 'animate Film': 11, 'SANZIGEN': 11, 'SANZIGEN, LIDENFILMS': 11, 'EMT Squared': 10, 'Trans Arts': 9, 'Radix': 9, 'Triangle Staff': 9, 'Kanaban Graphics': 9, 'Passione': 9, 'Pie in the sky': 9, 'Shanghai Foch Film Culture Investment': 9, 'E&G Films': 8, 'Yumeta Company': 8, 'Pierrot Plus': 8, 'Khara': 8, 'Bridge': 8, 'AIC Build': 8, 'SILVER LINK., Connect': 8, 'NAZ': 8, 'C-Station': 8, 'Lay-duce': 8, 'Signal.MD': 8, 'B.CMAY PICTURES': 8, 'Tokyo Kids': 7, 'Ashi Production': 7, 'Studio Eromatick': 7, 'G-Lam': 7, 'Gathering, Lesprit': 7, 'Onionskin': 7, 'Gainax, Shaft': 6, 'Pastel': 6, 'AIC Spirits': 6, 'The Answer Studio': 6, 'PrimeTime': 6, 'Marza Animation Planet': 6, 'Xebec, OLM': 6, 'Graphinica': 6, 'Tatsunoko Production, Dongwoo A&E': 6, 'Studio 1st': 6, 'Yaoyorozu': 6, 'Gakken Eigakyoku': 6, 'Studio 3Hz': 6, 'BreakBottle': 6, 'Pine Jam': 6, 'Minakata Laboratory': 6, 'Digital Frontier': 5, 'feel., Zexcs': 5, 'WAO World': 5, 'Vega Entertainment': 5, 'Phoenix Entertainment': 5, 'Office Take Off': 5, 'Ishikawa Pro': 5, 'Knack Productions': 5, 'Satelight, A-1 Pictures': 5, 'Shanghai Animation Film Studio': 5, 'Encourage Films': 5, 'Kamikaze Douga': 5, 'Next Media Animation': 5, 'Project No.9': 5, 'Opera House': 5, 'Studio Colorido': 5, 'Gaina': 5, 'Zero-G': 5, 'Usagi Ou': 5, 'Pops Inc.': 5, 'pH Studio': 5, 'Gallop, Studio Deen': 4, 'Imagin, Studio Live': 4, 'Chaos Project': 4, 'Topcraft': 4, 'Studio Rikka': 4, 'OLM Digital': 4, 'AIC, Artmic': 4, 'Oh! Production': 4, 'Artmic': 4, 'AIC, Artmic, animate Film': 4, 'Telescreen': 4, 'Shochiku Animation Institute': 4, 'Production Reed, Asahi Production': 4, 'Unknown': 4, 'Blue Cat': 4, 'Milky Cartoon': 4, 'Studio Deen, DAX Production': 4, 'AIC Classic': 4, 'Rising Force': 4, 'Studio Pierrot, Pierrot Plus': 4, 'Nexus': 4, 'Charaction': 4, 'Production I.G, Madhouse': 4, 'Telecom Animation Film, Graphinica': 4, 'TROYCA': 4, 'L²Studio': 4, 'W-Toon Studio': 4, '10Gauge': 4, 'Xebec, Issen': 4, 'Millepensee, GEMBA': 4, 'W-Toon Studio, DMM.futureworks': 4, 'Queen Bee': 4, 'Artland, TNK': 4, 'Typhoon Graphics': 4, 'Science SARU': 4, 'Gainax, Production I.G': 3, 'Palm Studio': 3, 'Square Enix': 3, 'Trinet Entertainment, Picture Magic': 3, 'M.S.C': 3, 'Studio Junio': 3, 'Planet': 3, 'Picture Magic': 3, 'Remic': 3, 'Agent 21': 3, 'Studio Egg': 3, 'Sugar Boy, Blue Cat': 3, 'Venet': 3, 'Image House': 3, 'Lilix': 3, 'Gonzo, Asread': 3, 'Madhouse, TMS Entertainment': 3, 'Anime Antenna Iinkai': 3, 'A-1 Pictures, Ordet': 3, 'Bones, Kinema Citrus': 3, 'Satelight, 8bit': 3, \"Brain's Base, Marvy Jack\": 3, 'Egg': 3, 'Ordet': 3, 'ChuChu': 3, 'Tatsunoko Production, SynergySP': 3, 'Production I.G, M.S.C': 3, 'Majin': 3, 'Kenji Studio': 3, 'C2C': 3, 'Ordet, LIDENFILMS': 3, 'Barnum Studio, Project No.9': 3, 'Xebec, I.Gzwei': 3, 'Madhouse, MAPPA': 3, 'J.C.Staff, A.C.G.T.': 3, 'A-1 Pictures, TROYCA': 3, 'Ascension': 3, \"Steve N' Steven\": 3, 'Mili Pictures': 3, 'MAPPA, Studio VOLN': 3, 'Studio A-CAT': 3, 'Tomovies': 3, 'Shirogumi': 3, 'Hoods Drifters Studio': 3, 'Studio Gokumi, AXsiZ': 3, 'Kyotoma': 3, 'Echoes': 3, 'OLM Digital, Signal.MD': 3, 'Platinum Vision': 3, 'Vasoon Animation': 3, 'CG Year': 3, 'Yamamura Animation, Inc.': 3, '2:10 AM Animation': 3, 'Sunrise, Studio Deen': 2, 'Studio Matrix': 2, 'Gainax, J.C.Staff': 2, 'Madhouse, Imagin': 2, 'Studio Fantasia, animate Film': 2, 'Gonzo, Production I.G': 2, 'Plum': 2, 'Trinet Entertainment': 2, 'Actas, SynergySP': 2, 'Radix, Marine Entertainment': 2, 'Kyoto Animation, Tatsunoko Production': 2, 'Studio Flag': 2, 'Madhouse, Telecom Animation Film': 2, 'Artland, Magic Bus': 2, 'Studio Live': 2, 'Toei Animation, Production Reed': 2, 'Production I.G, Production Reed': 2, 'Sunrise, Toei Animation': 2, 'ACC Production': 2, 'Toei Animation, Studio World': 2, 'Madhouse, Studio 4°C': 2, 'Imagin': 2, 'Gonzo, AIC': 2, 'JCF': 2, 'AIC Spirits, BeSTACK': 2, 'Tsuchida Productions': 2, 'Himajin Planning': 2, 'Visual 80': 2, 'Madhouse, Toei Animation': 2, 'Oxybot': 2, 'Pink Pineapple': 2, 'Telecom Animation Film, Shirogumi': 2, 'Eiken, Studio Live': 2, 'Anpro': 2, 'Studio Sign': 2, 'Tatsunoko Production, Studio World': 2, 'Kokusai Eigasha': 2, 'Studio Zero': 2, 'Production I.G, Trans Arts': 2, 'Studio Comet, KeyEast, REALTHING': 2, 'Gainax, feel.': 2, 'Natural High': 2, 'Schoolzone': 2, 'Studio Blanc': 2, 'Daewon Media': 2, 'Life Work': 2, 'A-1 Pictures, Bridge': 2, 'Production I.G, Xebec, OLM': 2, 'WAO World, MooGoo': 2, 'Digital Media Lab': 2, 'J.C.Staff, Artland': 2, 'Gainax, Asahi Production': 2, 'Studio Chizu': 2, 'Hotline': 2, 'Sunrise, Bandai Namco Pictures': 2, 'An DerCen': 2, 'feel., Assez Finaud Fabric': 2, 'Kyoto Animation, Animation Do': 2, 'Hoods Entertainment, teamKG': 2, 'Tatsunoko Production, Ordet': 2, 'Kinema Citrus, Orange': 2, 'Circle Tribute': 2, 'Satelight, C2C': 2, 'A-Real': 2, 'TMS Entertainment, Telecom Animation Film': 2, 'Ordet, Millepensee': 2, 'Studio! Cucuri': 2, 'Studio Binzo': 2, 'Jumondo': 2, 'Qualia Animation': 2, 'Oddjob': 2, 'Kinema Citrus, EMT Squared': 2, 'Studio Animal': 2, 'KAGAYA Studio': 2, 'Bridge, Husio Studio': 2, 'Joker Films': 2, 'DR Movie': 2, 'Nut': 2, 'Moss Design Unit': 2, 'Neft Film': 2, 'domerica': 2, 'Orange, Seven Arcs Pictures': 2, 'J.C.Staff, Egg Firm': 2, 'production doA': 2, 'Haoliners Animation League, Pb Animation Co. Ltd.': 2, 'Team YokkyuFuman': 2, 'Bouncy': 2, 'LandQ studios': 2, 'Tengu Kobo': 2, 'Puzzle Animation Studio Limited': 2, 'Thundray': 2, 'GARDEN LODGE': 2, 'Studio M2': 2, 'TMS Entertainment, Jinnis Animation Studios': 2, 'Sotsu': 2, 'Craftar Studios': 2, 'Haoliners Animation League, Studio LAN': 2, 'drop': 2, 'Actas, Studio 3Hz': 2, 'CloverWorks': 2, 'Jinnis Animation Studios': 2, 'Geno Studio': 2, 'D & D Pictures': 2, 'Light Chaser Animation Studios': 2, 'Gainax, Tatsunoko Production': 1, 'Bee Train, Xebec': 1, 'Sunrise, Studio Hibari': 1, 'Sunrise, Nakamura Production': 1, 'ufotable, feel., Studio Flag': 1, 'Gainax, Toei Animation': 1, 'Gallop, Studio Comet': 1, 'Chaos Project, GANSIS': 1, 'Gainax, Madhouse': 1, 'Madhouse, Tokyo Movie Shinsha': 1, 'TNK, Production Reed': 1, 'Studio Fantasia, Amber Film Works': 1, 'Hal Film Maker, Nomad': 1, 'Picture Magic, Rikuentai': 1, 'TMS Entertainment, Tokyo Kids, Minami Machi Bugyousho': 1, 'Milky Animation Label': 1, 'Group TAC, Japan Vistec': 1, 'Bones, Sunrise': 1, 'G&G Entertainment': 1, 'Arcs Create': 1, 'Kyoto Animation, Sunrise': 1, 'Karaku': 1, 'Satelight, Production Reed': 1, 'OLM, Production Reed': 1, 'Madhouse, Studio Deen': 1, 'Studio 4°C, Sunrise': 1, 'Studio Deen, Studio Hibari, Production Reed': 1, 'Madhouse, feel.': 1, 'OLM, AIC ASTA': 1, 'Radix, Chaos Project': 1, 'J.C.Staff, Studio Ghibli': 1, 'Group TAC, View Works': 1, 'Madhouse, Satelight, Graphinica': 1, 'Group TAC, Ginga Ya': 1, 'Xebec, Production Reed': 1, 'Studio Pierrot, Arms': 1, 'Shinkuukan': 1, 'Triangle Staff, Studio Wombat': 1, 'Studio Hibari, Production Reed': 1, 'AIC Spirits, Group TAC': 1, 'Enoki Films, Dai Nippon Printing': 1, 'Gonzo, Satelight': 1, 'Gainax, Studio Deen': 1, 'AIC, APPP': 1, 'Group TAC, Amuse': 1, 'Gallop, Group TAC, Studio Junio': 1, 'View Works': 1, 'Artland, Tatsunoko Production': 1, 'Production I.G, Studio Deen': 1, 'Yamato Works': 1, 'Trinet Entertainment, Studio Hibari': 1, 'Tamura Shigeru Studio': 1, 'Studio Signal': 1, 'Bee Media': 1, 'Madhouse, Production Reed': 1, 'Gainax, Group TAC': 1, 'AIC, animate Film': 1, 'Studio Pierrot, Studio Deen': 1, 'J.C.Staff, Production Reed': 1, 'AIC, Artmic, Darts': 1, 'Darts': 1, 'D.A.S.T.': 1, 'Studio Junio, Annapuru': 1, 'Sunrise, Production Reed': 1, 'Studio Pierrot, AIC ASTA': 1, 'Ishimori Entertainment': 1, 'Madhouse, Group TAC': 1, 'Shaft, TNK': 1, \"Shaft, Brain's Base, Japan Vistec\": 1, 'Echo': 1, 'Ginga Ya': 1, 'Kitty Films': 1, 'J.C.Staff, animate Film': 1, 'Genco, Radix': 1, 'Studio G-1Neo': 1, 'Studio Pierrot, Ajia-Do': 1, 'Front Line': 1, 'Anime R, Aubec': 1, 'Gonzo, Palm Studio': 1, 'Xebec, Group TAC': 1, 'J.C.Staff, Life Work': 1, 'Madhouse, Studio Deen, Magic Bus': 1, 'Artland, AIC, Artmic': 1, 'Mook DLE': 1, 'Gallop, TMS Entertainment': 1, 'Ajia-Do, Studio Deen': 1, 'Gainax, Studio 4°C': 1, 'Big Bang': 1, 'Eiken, Magic Bus': 1, 'AIC, Studio Hakk': 1, 'Soeishinsha': 1, 'Tokyo Kids, Minami Machi Bugyousho': 1, 'PPM': 1, 'Sunrise, Sanrio': 1, 'AIC, Darts': 1, 'Front Line, Studio G-1Neo': 1, 'Studio Comet, Studio Sign': 1, 'Studio Core': 1, 'Studio Wombat': 1, 'Bee Train, Cookie Jar Entertainment': 1, 'Production I.G, Tatsunoko Production': 1, 'Ajia-Do, TMS Entertainment': 1, 'Sunwoo Entertainment': 1, 'Studio Kyuuma, Studio Kikan, Azeta Pictures': 1, 'Madhouse, Studio Fantasia': 1, 'Tatsunoko Production, Production Reed, Asahi Production': 1, 'Panmedia, Meruhensha': 1, 'Gonzo, Gainax, Production I.G, Madhouse, Studio 4°C, Satelight, CoMix Wave Films': 1, 'Japan Taps': 1, 'Studio Bogey': 1, 'Madhouse, Tezuka Productions': 1, 'Artland, Madhouse': 1, 'Piko Studio': 1, 'Think Corporation': 1, 'Studio Unicorn, Hiro Media': 1, 'Kazuki Production': 1, 'Nippon Animation, Production Reed': 1, \"Actas, Brain's Base\": 1, 'Arms, Studio Kikan': 1, 'Bones, Telecom Animation Film': 1, 'J.C.Staff, Production I.G': 1, 'DAX Production': 1, 'Dai-Ichi Douga, Dongyang Animation': 1, 'Triple X': 1, 'Toei Animation, Daewon Media': 1, 'Nippon Animation, SynergySP, Shirogumi': 1, 'Bee Train, Production I.G, Madhouse, Studio 4°C': 1, 'Xebec, Kanaban Graphics': 1, 'Sunrise, Kino Production': 1, 'Oz': 1, 'Green Bunny': 1, 'Studio Ghibli, Ajia-Do': 1, 'AIC Spirits, Asread': 1, 'Aubec': 1, 'Madhouse, Tatsunoko Production': 1, 'Tele-Cartoon Japan': 1, 'AIC, Magic Bus, Ashi Production': 1, 'Ajia-Do, Group TAC': 1, 'Gainax, Magic Bus': 1, 'TMS Entertainment, Tezuka Productions': 1, 'Actas, TMS Entertainment': 1, 'Bee Media, Code': 1, 'AIC Spirits, Digital Frontier': 1, 'Office AO': 1, 'Gonzo, Picture Magic': 1, 'Kyoto Animation, Production I.G, Shin-Ei Animation': 1, 'Ripple Film': 1, 'Heewon Entertainment, NHK Enterprises': 1, 'Tatsunoko Production, SynergySP, Seven': 1, 'Production I.G, Polygon Pictures': 1, 'animate Film, Visual 80': 1, 'Picograph': 1, 'OLM, P.A. Works': 1, 'Anima': 1, 'Artmic, animate Film': 1, 'Tama Production': 1, 'Studio Pierrot, Studio Hibari': 1, 'Studio Take Off': 1, 'Bones, Production I.G, Studio 4°C, Toei Animation': 1, 'Future Planet, Beijing Huihuang Animation Company': 1, 'Indeprox': 1, 'Studio Korumi': 1, 'Asahi Production, Shochiku Animation Institute': 1, 'Stingray': 1, 'Arms, TNK': 1, 'Panda Factory, Studio PuYUKAI': 1, 'Hal Film Maker, TYO Animations': 1, 'Bee Media, 81 Produce': 1, 'Sunrise, Ascension': 1, 'Shogakukan Music & Digital Entertainment': 1, 'Studio World': 1, 'Panda Factory': 1, 'Animaruya': 1, 'Production I.G, animate Film': 1, 'Studio Bogey, Public & Basic': 1, 'Studio Unicorn': 1, 'Public & Basic, Ripple Film': 1, 'Primastea': 1, 'TAKI Corporation': 1, 'AIC Takarazuka': 1, 'Studio Pierrot, David Production': 1, 'Artland, Nippon Animation': 1, 'Bones, Production Reed': 1, 'Toei Animation, Studio Nue': 1, 'B&T': 1, 'Barnum Studio, Project No.9, Studio Blanc': 1, 'AIC, Remic': 1, 'Studio Fantasia, Rabbit Gate': 1, 'Satelight, ixtl': 1, 'TNK, Kinema Citrus': 1, 'Studio Flag, Studio Bogey': 1, 'Ordet, SANZIGEN': 1, 'Satelight, Encourage Films': 1, 'LMD': 1, 'Madhouse, Studio Gokumi': 1, 'Xebec, AIC': 1, 'J.C.Staff, Kitty Films': 1, 'Marvy Jack': 1, 'Gonzo, DLE': 1, 'Studio Rikka, Purple Cow Studio Japan': 1, 'Tezuka Productions, MAPPA': 1, 'Gonzo, LandQ studios': 1, 'Studio Pierrot, D.A.S.T.': 1, 'Odolttogi': 1, 'NHK': 1, 'Tomason': 1, 'Dongwoo A&E': 1, 'Production I.G, Studio 4°C, Shaft': 1, 'Shueisha': 1, 'Anpro, teamKG': 1, 'Gonzo, Hoods Entertainment': 1, 'Kazami Gakuen Koushiki Douga-bu': 1, 'Fifth Avenue': 1, 'Gonzo, ufotable': 1, 'Mary Jane': 1, 'Doga Kobo, Orange': 1, 'Hoods Entertainment, Production IMS': 1, 'Nippon Animation, Studio WHO': 1, 'J.C.Staff, Toei Animation': 1, 'Studio Pierrot, Gallop': 1, 'AIC Frontier': 1, 'Ordet, Encourage Films': 1, 'Hoods Entertainment, Felix Film': 1, 'MooGoo': 1, 'Studio Deen, Daume': 1, 'Studio Moriken': 1, 'Toei Animation, Tatsunoko Production': 1, 'Studio Pierrot, Kyoto Animation': 1, 'C2C, Lay-duce': 1, 'Shirogumi, Shin-Ei Animation': 1, 'Tatsunoko Production, 10Gauge': 1, 'Project No.9, Tri-Slash': 1, 'Studio Ghibli, Polygon Pictures': 1, 'Studio Comet, Zexcs': 1, 'Manglobe, Geno Studio': 1, 'Fuji TV': 1, 'Studio Deen, Wit Studio': 1, 'MMDGP': 1, 'Tonko House': 1, 'Ordet, Studio Moriken': 1, 'Production I.G, Zexcs': 1, 'Dynamo Pictures': 1, 'J.C.Staff, Nomad': 1, 'Production I.G, DLE': 1, 'J.C.Staff, SANZIGEN': 1, 'Artland, Hoods Entertainment': 1, \"Steve N' Steven, Rockwell Eyes\": 1, 'Khara, Trigger, Studio Colorido': 1, 'Toei Animation, Bridge': 1, 'Shirogumi, Encourage Films': 1, \"Brain's Base, Studio A-CAT\": 1, 'TMS Entertainment, 3xCube': 1, 'Kinema Citrus, White Fox': 1, 'Media Bank': 1, 'Lerche, 10Gauge': 1, 'SILVER LINK., Nexus': 1, 'Barnum Studio, SILVER LINK., Connect': 1, 'Production I.G, OLM': 1, 'Office DCI': 1, 'Arms, Asread': 1, 'ixtl, LIDENFILMS': 1, 'HS Pictures Studio': 1, 'Buemon': 1, 'Actas, Bee Media': 1, 'Orange, Studio 3Hz': 1, \"Brain's Base, Platinum Vision\": 1, 'OLM, OLM Digital, Sprite Animation Studios': 1, 'Larx Entertainment': 1, 'TYO Animations, LIDENFILMS': 1, 'AXsiZ': 1, 'Studio Meditation With a Pencil': 1, 'Pollyanna Graphics': 1, 'feel., Zexcs, Assez Finaud Fabric': 1, 'Kyotoma, Office Nobu': 1, 'KOO-KI': 1, 'Idea Factory': 1, 'Production IMS, Orange': 1, 'Sakura Create': 1, 'Shimogumi': 1, 'Gainax, Gaina': 1, 'Seven Arcs, Seven Arcs Pictures': 1, 'October Media': 1, 'OLM, OLM Digital': 1, 'Rabbit Machine': 1, 'CoMix Wave Films, FOREST Hunting One': 1, 'Ekura Animal': 1, 'Asura Film': 1, 'Ordet, W-Toon Studio': 1, 'Robot Communications': 1, 'EDGE': 1, 'Production I.G, OLM, Signal.MD': 1, 'TMS Entertainment, Shin-Ei Animation': 1, 'Namu Animation': 1, 'TUBA': 1, 'Ripromo': 1, 'feel., PRA': 1, 'Blade': 1, 'Studio Ppuri': 1, 'Creators in Pack, Namu Animation': 1, 'Chippai': 1, 'Beijing Rocen Digital': 1, 'Will Palette': 1, 'Kamikaze Douga, Nishiki Studio': 1, 'Toei Video': 1, 'Calf Studio': 1, 'TMS Entertainment, TOCSIS': 1, 'TMS Entertainment, Studio Comet': 1, 'Dwango, LIDENFILMS': 1, 'Zexcs, Studio A-CAT': 1, 'PRA': 1, 'Project No.9, A-Real': 1, 'Studio Ponoc': 1, 'Felix Film': 1, 'Studio Zealot': 1, 'Asahi Production, Success Corp.': 1, 'SELFISH': 1, 'Rockwell Eyes': 1, 'Tsukimidou': 1, 'Tomoyasu Murata Company': 1, 'Karasfilms': 1, 'Gonzo, DandeLion Animation Studio': 1, 'Chiptune': 1, 'A-Line': 1, 'Piso Studio': 1, 'Fanworks, Imagineer': 1, 'Shin-Ei Animation, DLE': 1, 'iDRAGONS Creative Studio': 1, 'Sanrio': 1, 'Shirogumi, EMT Squared': 1, 'Studio Khronos': 1, 'Orange': 1, 'Creators Dot Com': 1, 'Yokohama Animation Lab': 1, 'Diomedéa, Studio Blanc': 1, 'Emon, Blade': 1, 'Shaft, Tatsunoko Production': 1, 'Production I.G, Nut, Revoroot': 1, 'A-1 Pictures, Trigger, CloverWorks': 1, 'Oddjob, Kate Arrow': 1, 'Studio Flad': 1, 'G-Lam, Studio CA': 1, 'Kaname Productions': 1, 'Takara Tomy A.R.T.S': 1, 'DandeLion Animation Studio': 1, 'CygamesPictures': 1, 'Three-d': 1, 'Sunrise, The Answer Studio': 1, 'TNK, Zero-G': 1, 'OLM, Shin-Ei Animation': 1, 'KIZAWA Studio': 1, 'Coastline Animation Studio': 1, 'Trigger, dwarf': 1, 'Boyan Pictures': 1, 'Nice Boat Animation': 1, 'pH Studio, D & D Pictures': 1, 'HeART-BIT': 1, 'Strawberry Meets Pictures': 1, 'Creators in Pack, Studio Lings': 1, 'BOOTLEG': 1, 'Eiken, TYO Animations': 1, 'helo.inc': 1, 'Wawayu Animation': 1, 'EKACHI EPILKA': 1, 'Madhouse, DLE': 1, 'G-angle': 1})\n",
      "\n",
      "Are there still 'Unknown' values present?\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "print(\"Source counters: main and secondary:\")\n",
    "print(Counter(df_main[\"source_main\"]))\n",
    "print(Counter(df_main[\"source_secondary\"]))\n",
    "\n",
    "df_main.rename(columns={\"source_secondary\": \"source\"}, inplace=True)\n",
    "df_main.drop(columns=[\"source_main\"], inplace=True)\n",
    "\n",
    "print(\"\\nType counters: main and secondary:\")\n",
    "print(Counter(df_main[\"type_main\"]))\n",
    "print(Counter(df_main[\"type_secondary\"]))\n",
    "\n",
    "df_main.rename(columns={\"type_secondary\": \"type\"}, inplace=True)\n",
    "df_main.drop(columns=[\"type_main\"], inplace=True)\n",
    "\n",
    "print(\"\\nStudio counters: main and secondary:\")\n",
    "print(Counter(df_main[\"studios_main\"]))\n",
    "print(Counter(df_main[\"studios_secondary\"]))\n",
    "\n",
    "df_main.rename(columns={\"studios_secondary\": \"studio\"}, inplace=True)\n",
    "df_main.drop(columns=[\"studios_main\"], inplace=True)\n",
    "\n",
    "print(\"\\nAre there still 'Unknown' values present?\")\n",
    "print('Unknown' in df_main[\"studio\"], 'Unknown' in df_main[\"source\"], 'Unknown' in df_main[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d9f56",
   "metadata": {},
   "source": [
    "### NAME column cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81998af0",
   "metadata": {},
   "source": [
    "========================================================\n",
    "\n",
    "Try to choose one of the 'name'-related columns as base. In fact we need these columns only to orient through data, so try to choose the most filled & interpreteble column.\n",
    "\n",
    "In the result of some analysis below we see (and addition researching of sources proves), that columns 'title' and 'name' are the most consistent. They are both parsed from one field: main title from the webpage of the anime title. And they are mostly the same: difference - in 418 of 6656 titles. But this diference s only syntaxis or the rare characters encoding-decoding issues. In the **secondary** dataset these string values are postprocessed with more acuracy. \n",
    "\n",
    "========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac43324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "NaN: 0\n",
      "title\n",
      "False    6656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "title_english\n",
      "NaN: 3219\n",
      "title_english\n",
      "False    6656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "title_synonyms\n",
      "NaN: 2183\n",
      "title_synonyms\n",
      "False    6656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "name\n",
      "NaN: 0\n",
      "name\n",
      "False    6656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "english_name\n",
      "NaN: 0\n",
      "english_name\n",
      "False    3528\n",
      "True     3128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Beet the Vandel Buster | Bouken Ou Beet\n",
      "Ghost in the Shell | Koukaku Kidoutai\n",
      "Black Cat | Black Cat (TV)\n",
      "Mahou Shoujo Lyrical Nanoha A&#039;s | Mahou Shoujo Lyrical Nanoha A's\n",
      "Mobile Suit Gundam: The 08th MS Team - Miller&#039;s Report | Mobile Suit Gundam: The 08th MS Team - Miller's Report\n",
      "Mobile Suit Gundam: Char&#039;s Counterattack | Mobile Suit Gundam: Char's Counterattack\n",
      "Kidou Shin Seiki Gundam X | After War Gundam X\n",
      "Hunter x Hunter: Yorkshin City Kanketsu-hen | Hunter x Hunter: Original Video Animation\n",
      "Wolf&#039;s Rain | Wolf's Rain\n",
      "R.O.D OVA | R.O.D: Read or Die\n",
      "R.O.D the TV | R.O.D: The TV\n",
      "E&#039;s Otherwise | E's Otherwise\n",
      "Eureka Seven | Koukyoushihen Eureka Seven\n",
      "Mahoromatic 2 | Mahoromatic: Motto Utsukushii Mono\n",
      "Chiisana Obake Acchi, Kocchi, Socchi | Chiisana Obake: Acchi, Kocchi, Socchi\n",
      "Piano | Piano (TV)\n",
      "I&#039;ll/CKBC | I'll/CKBC\n",
      "Boogiepop wa Warawanai: Boogiepop Phantom | Boogiepop wa Warawanai\n",
      "Elfen Lied Special | Elfen Lied: Tooriame nite Arui wa, Shoujo wa Ikani Shite Sono Shinjou ni Itatta ka? - Regenschauer\n",
      "Fushigi Yuugi OVA 2 | Fushigi Yuugi: Dai Ni Bu\n",
      "InuYasha: Guren no Houraijima | InuYasha Movie 4: Guren no Houraijima\n",
      "InuYasha: Kagami no Naka no Mugenjo | InuYasha Movie 2: Kagami no Naka no Mugenjo\n",
      "InuYasha: Tenka Hadou no Ken | InuYasha Movie 3: Tenka Hadou no Ken\n",
      "InuYasha: Toki wo Koeru Omoi | InuYasha Movie 1: Toki wo Koeru Omoi\n",
      "Ghost in the Shell: Stand Alone Complex | Koukaku Kidoutai: Stand Alone Complex\n",
      "Ghost in the Shell 2: Innocence | Innocence\n",
      "Aria The Animation | Aria the Animation\n",
      "School Rumble Ichi Gakki Hoshuu | School Rumble: Ichi Gakki Hoshuu\n",
      "Pokemon: Mewtwo no Gyakushuu | Pokemon Movie 01: Mewtwo no Gyakushuu\n",
      "RahXephon Interlude: Her and Herself/Thatness and Thereness | RahXephon: Kansoukyoku/Kanojo to Kanojo Jishin to - Thatness and Thereness\n",
      "Kagihime Monogatari Eikyuu Alice Rinbukyoku | Kagi Hime Monogatari: Eikyuu Alice Rondo\n",
      "Sexy Commando Gaiden: Sugoiyo!! Masaru-san | Sexy Commando Gaiden: Sugoi yo!! Masaru-san\n",
      "Mahoutsukai Tai! OVA | Mahoutsukai Tai!\n",
      "Hikaru no Go: Journey to the North Star Cup | Hikaru no Go: Hokuto Hai e no Michi\n",
      "Uta Kata | Uta∽Kata\n",
      "Uta Kata Special | Uta∽Kata: Shotou no Futanatsu\n",
      "RahXephon: Pluralitas Concentio | RahXephon: Tagen Hensoukyoku\n",
      "Ghost in the Shell: Stand Alone Complex 2nd GIG | Koukaku Kidoutai: Stand Alone Complex 2nd GIG\n",
      "I&#039;&#039;s Pure | I''s Pure\n",
      "Jinki:Extend OVA | Jinki:Extend - Sorekara\n",
      "Joshikousei: Girl&#039;s High | Joshikousei: Girl's High\n",
      "Dragon Ball Movie 2: Majinjou no Nemuri Hime | Dragon Ball Movie 2: Majinjou no Nemurihime\n",
      "Dragon Ball Z Movie 13: Ryuuken Bakuhatsu!! Goku ga Yaraneba Dare ga Yaru | Dragon Ball Z Movie 13: Ryuuken Bakuhatsu!! Gokuu ga Yaraneba Dare ga Yaru\n",
      "Aria The Natural | Aria the Natural\n",
      "Glass no Kantai: La Legende du Vent de l&#039;Univers | Glass no Kantai: La Legende du Vent de l'Univers\n",
      "Dual Parallel! Trouble Adventures | Dual! Parallel Lun-Lun Monogatari\n",
      "Bishoujo Senshi Sailor Moon S: Kaguya Hime no Koibito | Bishoujo Senshi Sailor Moon S: Kaguya-hime no Koibito\n",
      "Tide-Line Blue Special | Tide-Line Blue: Kyoudai\n",
      "Ranma ½ Specials | Ranma ½: Yomigaeru Kioku\n",
      "Sexy Commando Gaiden: Sugoiyo!! Masaru-san Specials | Sexy Commando Gaiden: Sugoi yo!! Masaru-san Specials\n",
      "Le Chevalier D&#039;Eon | Le Chevalier D'Eon\n",
      "Wolf&#039;s Rain OVA | Wolf's Rain OVA\n",
      "Yakumo Tatsu | Yakumotatsu\n",
      "Naruto Narutimate Hero 3: Tsuini Gekitotsu! Jounin vs. Genin!! Musabetsu Dairansen taikai Kaisai!! | Naruto Narutimate Hero 3: Tsuini Gekitotsu! Jounin vs. Genin!! Musabetsu Dairansen Taikai Kaisai!!\n",
      "I&#039;&#039;s | I''s\n",
      "Monster Extra | Monster Extra: Hottan\n",
      "Pokemon: Maboroshi no Pokemon Lugia Bakutan | Pokemon Movie 02: Maboroshi no Pokemon Lugia Bakutan\n",
      "Pokemon: Kesshoutou no Teiou Entei | Pokemon Movie 03: Kesshoutou no Teiou Entei\n",
      "Pokemon: Celebi Toki wo Koeta Deai | Pokemon Movie 04: Celebi Toki wo Koeta Deai\n",
      "Pokemon: Mizu no Miyako no Mamorigami Latias to Latios | Pokemon Movie 05: Mizu no Miyako no Mamorigami Latias to Latios\n",
      "Pokemon Advanced Generation: Nanayo no Negaiboshi Jirachi | Pokemon Movie 06: Nanayo no Negaiboshi Jirachi\n",
      "Pokemon Advanced Generation: Rekkuu no Houmonsha Deoxys | Pokemon Movie 07: Rekkuu no Houmonsha Deoxys\n",
      "Beet the Vandel Buster Excellion | Bouken Ou Beet Excellion\n",
      "UFO Princess Valkyrie: Special | UFO Princess Valkyrie: Shichiten Battou Hanayome Shuugyou\n",
      "Eden&#039;s Bowy | Eden's Bowy\n",
      "3x3 Eyes Seima Densetsu | 3x3 Eyes: Seima Densetsu\n",
      "I&#039;&#039;s Pure Bonus | I''s Pure Bonus\n",
      "Aquarian Age: Saga II - Don&#039;t Forget Me... | Aquarian Age: Saga II - Don't Forget Me...\n",
      "Ohoshisama no Rail | Ohoshi-sama no Rail\n",
      "Nikutai Ten&#039;i | Nikutai Ten'i\n",
      "Kino no Tabi: Nanika wo Suru Tame ni - Life Goes On. | Kino no Tabi: The Beautiful World - Nanika wo Suru Tame ni - Life Goes On.\n",
      "City Hunter &#039;91 | City Hunter '91\n",
      "Pokemon Advanced Generation: Mew to Hadou no Yuusha Lucario | Pokemon Movie 08: Mew to Hadou no Yuusha Lucario\n",
      "Ghost in the Shell: Stand Alone Complex - Solid State Society | Koukaku Kidoutai: Stand Alone Complex - Solid State Society\n",
      "Eat-Man &#039;98 | Eat-Man '98\n",
      "Kekkaishi | Kekkaishi (TV)\n",
      "Eureka Seven: Navigation ray=out | Koukyoushihen Eureka Seven: Kinkyuu Tokuban Navigation ray=out\n",
      "Angel&#039;s Feather | Angel's Feather\n",
      "Bokurano | Bokura no\n",
      "DearS: Kin no Tama desu no? | DearS: Kin no Tama Desu no?\n",
      "Gedo Senki | Ged Senki\n",
      "Yomigaeru Sora: Rescue Wings Special | Yomigaeru Sora: Rescue Wings - Saigo no Shigoto\n",
      "Yawara! Sore Yuke Koshinuke Kids!! | Yawara!: Sore Yuke Koshinuke Kids!!\n",
      "Yawara! Special: Zutto Kimi no Koto ga... . | Yawara!: Zutto Kimi no Koto ga...\n",
      "Mermaid&#039;s Scar | Mermaid's Scar\n",
      "Cat&#039;s Eye | Cat's Eye\n",
      "Yuu☆Yuu☆Hakusho: Eizou Hakusho | Yuu☆Yuu☆Hakusho: Eizou Hakusho - Ankoku Bujutsukai no Shou\n",
      "Shin Seiki Inma Seiden | Shinseiki Inma Seiden\n",
      "Pokemon Advanced Generation: Pokemon Ranger to Umi no Ouji Manaphy | Pokemon Movie 09: Pokemon Ranger to Umi no Ouji Manaphy\n",
      "Be-Boy Kidnapp&#039;n Idol | Be-Boy Kidnapp'n Idol\n",
      "Dragon Quest: Dai no Daibouken | Dragon Quest: Dai no Daibouken (TV)\n",
      "Naruto: Dai Katsugeki!! Yuki Hime Shinobu Houjou Dattebayo! Special: Konoha Annual Sports Festival | Naruto: Dai Katsugeki!! Yuki Hime Shinobu Houjou Dattebayo! - Konoha no Sato no Dai Undoukai\n",
      "Mobile Suit SD Gundam&#039;s Counterattack | Mobile Suit SD Gundam's Counterattack\n",
      "Ghost Sweeper GS Mikami | GS Mikami\n",
      "Ghost in the Shell: Stand Alone Complex - The Laughing Man | Koukaku Kidoutai: Stand Alone Complex - The Laughing Man\n",
      "Detective Conan OVA 05: The Target is Kogoro! The Detective Boys&#039; Secret Investigation | Detective Conan OVA 05: The Target is Kogoro! The Detective Boys' Secret Investigation\n",
      "Princess Sara | Shoukoujo Sara\n",
      "Aria The OVA: Arietta | Aria the OVA: Arietta\n",
      "Black Cat: Toozakaru Neko | Black Cat (TV): Toozakaru Neko\n",
      "Shinreigari: Ghost Hound | Shinreigari\n",
      "Bakusou Kyoudai Let&#039;s & Go | Bakusou Kyoudai Let's & Go\n",
      "Bakusou Kyoudai Let&#039;s & Go WGP | Bakusou Kyoudai Let's & Go WGP\n",
      "Bakusou Kyoudai Let&#039;s & Go MAX | Bakusou Kyoudai Let's & Go MAX\n",
      "Dondon Domeru to Ron | Dondon Dommel to Ron\n",
      "Pokemon Diamond & Pearl: Dialga vs. Palkia vs. Darkrai | Pokemon Movie 10: Dialga vs. Palkia vs. Darkrai\n",
      "Ohayo! Spank | Ohayou! Spank\n",
      "Kimikiss Pure Rouge | KimiKiss Pure Rouge\n",
      "Sketchbook: Full Color&#039;s | Sketchbook: Full Color's\n",
      "Atashin&#039;chi | Atashin'chi\n",
      "Bishoujo Senshi Sailor Moon SuperS Special | Bishoujo Senshi Sailor Moon SuperS Specials\n",
      "Solty Rei Special | Solty Rei: Surechigau Kimochi de, Omoi Au Kokoro de.\n",
      "Gakuen Utopia Manabi Straight! Special | Gakuen Utopia Manabi Straight!: Natsu da! Manabi da! Kyouka Gasshuku da!\n",
      "Aria The Origination | Aria the Origination\n",
      "Fushigi no Umi no Nadia Specials | Fushigi no Umi no Nadia Omake Gekijou\n",
      "Ghost in the Shell: Stand Alone Complex - Solid State Society - Uchikoma no Hibi | Koukaku Kidoutai: Stand Alone Complex - Solid State Society - Uchikoma na Hibi\n",
      "Kino no Tabi: The Beautiful World - Tou no Kuni | Kino no Tabi: The Beautiful World - Tou no Kuni - Free Lance\n",
      "Kochira Katsushikaku Kameari Kouenmae Hashutsujo (TV) | Kochira Katsushikaku Kameari Kouenmae Hashutsujo\n",
      "Kochira Katsushikaku Kameari Kouenmae Hashutsujo | Kochira Katsushikaku Kameari Kouenmae Hashutsujo: Jump Festa Special\n",
      "Chi&#039;s Sweet Home | Chi's Sweet Home\n",
      "Ookiku Furikabutte Special | Ookiku Furikabutte: Kihon no Kihon\n",
      "H. P. Lovecraft&#039;s The Dunwich Horror and Other Stories | H. P. Lovecraft's The Dunwich Horror and Other Stories\n",
      "Nodame Cantabile Special | Nodame Cantabile: Nodame to Chiaki no Umi Monogatari\n",
      "Yu☆Gi☆Oh! 5D&#039;s | Yu☆Gi☆Oh! 5D's\n",
      "Arai Men to YuYu The Animation | Alignment You! You! The Animation\n",
      "Pokemon Diamond & Pearl: Giratina to Sora no Hanataba Sheimi | Pokemon Movie 11: Giratina to Sora no Hanataba Sheimi\n",
      "Grimm Masterpiece Theater | Grimm Meisaku Gekijou\n",
      "Super Robot Taisen OG: Divine Wars Special | Super Robot Taisen OG: Divine Wars - Sorezore no Michi\n",
      "Chiisana Pengin: Lolo no Bouken | Chiisana Penguin Lolo no Bouken\n",
      "Mahou Sensei Negima! Shiroki Tsubasa Ala Alba | Mahou Sensei Negima!: Shiroki Tsubasa Ala Alba\n",
      "Winter Sonata | Fuyu no Sonata\n",
      "Eureka Seven: Pocket ga Niji de Ippai | Koukyoushihen Eureka Seven: Pocket ga Niji de Ippai\n",
      "Code Geass: Hangyaku no Lelouch Special Edition Black Rebellion | Code Geass: Hangyaku no Lelouch Special Edition - Black Rebellion\n",
      "Ghost in the Shell 2.0 | Koukaku Kidoutai 2.0\n",
      "Shigofumi: Sorekara | Shigofumi: Sore kara\n",
      "Queen&#039;s Blade: Rurou no Senshi | Queen's Blade: Rurou no Senshi\n",
      "Grope: Yami no naka no Kotoritachi | Grope: Yami no naka no Kotori-tachi\n",
      "Black Lagoon: Roberta&#039;s Blood Trail | Black Lagoon: Roberta's Blood Trail\n",
      "Let&#039;s Nupu Nupu | Let's Nupu Nupu\n",
      "Kara no Kyoukai 7: Satsujin Kousatsu (Kou) | Kara no Kyoukai 7: Satsujin Kousatsu (Go)\n",
      "Koihime†Musou OVA | Koihime†Musou: Gunyuu, Seitoukaichou no Za wo Neratte Aiarasou no Koto - Ato, Porori mo Aru yo!\n",
      "Tales of Symphonia The Animation: Tethe&#039;alla-hen | Tales of Symphonia The Animation: Tethe'alla-hen\n",
      "Asura Cryin&#039; | Asura Cryin'\n",
      "Viper&#039;s Creed | Viper's Creed\n",
      "Nodame Cantabile Finale | Nodame Cantabile: Finale\n",
      "Tengen Toppa Gurren Lagann Movie: Lagann-hen Special - Viral no Amai Yume | Tengen Toppa Gurren Lagann Movie Zenyasai: Viral no Amai Yume\n",
      "Mizugi Kanojo: The Animation | Mizugi Kanojo The Animation\n",
      "Chi&#039;s Sweet Home: Atarashii Ouchi | Chi's Sweet Home: Atarashii Ouchi\n",
      "Mahou Sensei Negima! Mou Hitotsu no Sekai | Mahou Sensei Negima!: Mou Hitotsu no Sekai\n",
      "Ichigatsu ni wa Christmas | 1-gatsu ni wa Christmas\n",
      "Cookin&#039; Idol Ai! Mai! Main! | Cookin' Idol Ai! Mai! Main!\n",
      "Pokemon Diamond & Pearl: Arceus Choukoku no Jikuu e | Pokemon Movie 12: Arceus Choukoku no Jikuu e\n",
      "Detective Conan OVA 08: High School Girl Detective Sonoko Suzuki&#039;s Case Files | Detective Conan OVA 08: High School Girl Detective Sonoko Suzuki's Case Files\n",
      "True Tears Specials | True Tears: Mini Chara 4-koma Gekijou\n",
      "Sketchbook: Full Color&#039;s Picture Drama | Sketchbook: Full Color's Picture Drama\n",
      "Full Metal Panic! The Second Raid Episode 000 | Full Metal Panic! The Second Raid Episode 00\n",
      "Taiho Shichau zo: Full Throttle Special | Taiho Shichau zo: Full Throttle - Watashitachi no Iru Basho\n",
      "Queen&#039;s Blade: Gyokuza wo Tsugu Mono | Queen's Blade: Gyokuza wo Tsugu Mono\n",
      "Sengoku Basara Two | Sengoku Basara Ni\n",
      "Joshikousei: Girl&#039;s High Specials | Joshikousei: Girl's High Specials\n",
      "Asura Cryin&#039; 2 | Asura Cryin' 2\n",
      "Queen&#039;s Blade: Rurou no Senshi Specials | Queen's Blade: Rurou no Senshi Specials\n",
      "Mouryou no Hako Special | Mouryou no Hako: Chuuzenji Atsuko no Jikenbo - Hako no Yurei no Koto\n",
      "Code Geass: Hangyaku no Lelouch R2 Special Edition Zero Requiem | Code Geass: Hangyaku no Lelouch R2 Special Edition - Zero Requiem\n",
      "Ookami to Koushinryou II Specials | Ookami to Koushinryou II: Holo no Short Anime\n",
      "Pandora Hearts Specials | Pandora Hearts Omake\n",
      "Hellsing I: Digest for Freaks | Hellsing: Digest for Freaks\n",
      "Hyakko OVA | Hyakko Extra\n",
      "GA: Geijutsuka Art Design Class OVA | GA: Geijutsuka Art Design Class: Aozora ga Kakitai\n",
      "Mahoromatic: I&#039;m Home! | Mahoromatic: Tadaima Okaeri\n",
      "Mahou Sensei Negima! Anime Final | Mahou Sensei Negima! Movie: Anime Final\n",
      "Rose O&#039;Neill Kewpie | Rose O'Neill Kewpie\n",
      "Shakugan no Shana S Specials | Shakugan no Shana-tan S\n",
      "Winter Sonata Episode 0 | Fuyu no Sonata Episode 0\n",
      "Dragon Quest: Dai no Daibouken (1991) | Dragon Quest: Dai no Daibouken\n",
      "Ningen Shikkaku: Director&#039;s Cut-ban | Ningen Shikkaku: Director's Cut-ban\n",
      "Pokemon Diamond & Pearl: Genei no Hasha Zoroark | Pokemon Movie 13: Genei no Hasha Zoroark\n",
      "Hellsing: Psalm of Darkness | Hellsing: Psalm of the Darkness\n",
      "Queen&#039;s Blade: Gyokuza wo Tsugu Mono Specials | Queen's Blade: Gyokuza wo Tsugu Mono Specials\n",
      "Shin Koihime†Musou OVA | Shin Koihime†Musou: Gunyuu, Minami no Shima de Bakansu wo Suru no Koto - Ato, Porori mo Aru yo!\n",
      "Kemono no Souja Erin Recap | Kemono no Souja Erin Recaps\n",
      "Chi&#039;s Sweet Home OVA | Chi's Sweet Home: Chi to Kocchi, Deau.\n",
      "Soukou Kihei Votoms: Gen-ei Hen | Soukou Kihei Votoms: Genei-hen\n",
      "Nodame Cantabile Finale Special | Nodame Cantabile: Finale - Mine to Kiyora no Saikai\n",
      "Keroro Gunsou Movie 5: Tanjou! Kyuukyoku Keroro, Kiseki no Jikuu-jima, de arimasu!! | Keroro Gunsou Movie 5: Tanjou! Kyuukyoku Keroro, Kiseki no Jikuu-jima, de Arimasu!!\n",
      "Dance in the Vampire Bund Recap | Dance in the Vampire Bund: Special Edition\n",
      "Tekkamen wo Oe: \"d&#039;Artagnan Monogatari\" yori | Tekkamen wo Oe: \"d'Artagnan Monogatari\" yori\n",
      "Soukou Kihei Votoms: Case; Irvine | Soukou Kihei Votoms: Case;Irvine\n",
      "Damen&#039;s Walker | Damen's Walker\n",
      "Queen&#039;s Blade: Utsukushiki Toushitachi | Queen's Blade: Utsukushiki Toushi-tachi\n",
      "Mirai Nikki | Mirai Nikki OVA\n",
      "Katekyo Hitman Reborn! Special | Katekyo Hitman Reborn! Vongola Family Soutoujou! Vongola Shiki Shuugakuryokou, Kuru!!\n",
      "Code Geass: Hangyaku no Lelouch - Kiseki no Birthday Picture Drama | Code Geass: Hangyaku no Lelouch - Kiseki no Birthday\n",
      "Tales of Symphonia The Animation: Tethe&#039;alla-hen Specials | Tales of Symphonia The Animation: Tethe'alla-hen Specials\n",
      "Kuruneko Season 2 | Kuruneko 2nd Season\n",
      "Toaru Kagaku no Railgun: Misaka-san wa Ima Chuumoku no Mato desukara | Toaru Kagaku no Railgun: Misaka-san wa Ima Chuumoku no Mato Desu kara\n",
      "Ura Tegamibachi | Tegami Bachi: Omakebachi\n",
      "Bakusou Kyoudai Let&#039;s & Go Special | Bakusou Kyoudai Let's & Go Special\n",
      "Pokemon Best Wishes!: Victini to Kuroki Eiyuu Zekrom | Pokemon Movie 14 White: Victini to Kuroki Eiyuu Zekrom\n",
      "Gintama&#039; | Gintama'\n",
      "Senkou no Night Raid Picture Drama | Senkou no Night Raid: Aruku Hako\n",
      "Da Yu Hai Tang | Da Yu Hai Tang (Movie)\n",
      "Shin Koihime†Musou: Otome Tairan OVA | Shin Koihime†Musou: Otome Tairan - Gakuensai da yo! Zenin Shuugou no Koto\n",
      "Pokemon Diamond & Pearl Specials | Pocket Monsters: Diamond & Pearl Specials\n",
      "Suzy&#039;s Zoo: Daisuki! Witzy | Suzy's Zoo: Daisuki! Witzy\n",
      "Gosick Recap | Gosick: Utsukushiki Kaibutsu wa Konton no Sen wo Shimiru\n",
      "Working&#039;!! | Working'!!\n",
      "Working&#039;!! Announcement Specials | Working'!! Announcement Specials\n",
      "Kimi ni Todoke 2nd Season Specials | Kimi ni Todoke 2nd Season: Minitodo Gekijou\n",
      "Hidan no Aria Special | Hidan no Aria: Butei ga Kitarite Onsen Kenshuu\n",
      "Mirai Nikki (TV) | Mirai Nikki\n",
      "Mahou Sensei Negima! Mou Hitotsu no Sekai Extra: Mahou Shoujo Yue | Mahou Sensei Negima!: Mou Hitotsu no Sekai Extra - Mahou Shoujo Yue♥\n",
      "Pokemon Best Wishes!: Victini to Shiroki Eiyuu Reshiram | Pokemon Movie 14 Black: Victini to Shiroki Eiyuu Reshiram\n",
      "Hikaru no Go: Tokubetsu-hen - Sabaki no Ichikyoku! Inishie no Hana yo Sake!! | Hikaru no Go: Sabaki no Ikkyoku! Inishie no Hana yo Sake!!\n",
      "UN-GO | Un-Go\n",
      "Suite Precure♪ Movie: Torimodose! Kokoro ga Tsunaku Kiseki no Melody♪ | Suite Precure♪ Movie: Torimodose! Kokoro ga Tsunagu Kiseki no Melody♪\n",
      "Ghost in the Shell: Stand Alone Complex - Tachikoma na Hibi (TV) | Koukaku Kidoutai: Stand Alone Complex - Tachikoma na Hibi (TV)\n",
      "Queen&#039;s Blade OVA | Queen's Blade OVA\n",
      "Itsuka Tenma no Kuro Usagi OVA | Itsuka Tenma no Kuro Usagi: Kokoro Utsuri no Toukoubi - School Attendance Day\n",
      "UN-GO: Inga-ron | Un-Go: Inga-ron\n",
      "Dog Days&#039; | Dog Days'\n",
      "Queen&#039;s Blade: Rebellion | Queen's Blade: Rebellion\n",
      "What&#039;s Michael? 2 | What's Michael? 2\n",
      "What&#039;s Michael? (TV) | What's Michael? (TV)\n",
      "Queen&#039;s Blade OVA Specials | Queen's Blade OVA Specials\n",
      "UN-GO: Inga Nikki | Inga Nikki\n",
      "Guilty Crown Kiseki: Reassortment | Guilty Crown: Kiseki - Reassortment\n",
      "Yuu☆Yuu☆Hakusho: Mu Mu Hakusho | Yuu☆Yuu☆Hakusho: Mu Mu Hakusho - Nightmare Hakusho\n",
      "C³ Special | C³: Rikan Gakkou Confusion!\n",
      "Pokemon Best Wishes! Season 2: Kyurem vs. Seikenshi | Pokemon Movie 15: Kyurem vs. Seikenshi\n",
      "Bokurano Recap | Bokura no: Tsuisou\n",
      "Madang-Eul Naon Amtalg | Madang-eul Naon Amtalg\n",
      "Shibainuko-san | Shiba Inuko-san\n",
      "Guilty Crown: Lost Christmas | Guilty Crown: Lost Christmas - An Episode of Port Town\n",
      "Eureka Seven: New Order | Koukyoushihen Eureka Seven: New Order\n",
      "Sakurasou no Pet na Kanojo | Sakura-sou no Pet na Kanojo\n",
      "Queen&#039;s Blade: Rebellion Specials | Queen's Blade: Rebellion Specials\n",
      "Hayate no Gotoku! Can&#039;t Take My Eyes Off You | Hayate no Gotoku! Can't Take My Eyes Off You\n",
      "Queen&#039;s Blade Rebellion vs. Hagure Yuusha no Aesthetica | Queen's Blade Rebellion vs. Hagure Yuusha no Aesthetica\n",
      "Dakara Boku wa, H ga Dekinai. OVA | Dakara Boku wa, H ga Dekinai.: Mie Sugi! Mizugi Contest\n",
      "Gintama&#039;: Enchousen | Gintama': Enchousen\n",
      "Eureka Seven AO: Aratanari Fukaki Ao | Aratanari Fukaki Ao\n",
      "Nagi no Asukara | Nagi no Asu kara\n",
      "Dog Days&#039;&#039; | Dog Days''\n",
      "SKET Dance OVA | SKET Dance: Imouto no Nayami ni Nayamu Ani ni Nayamu Imouto to Sono Nakama-tachi\n",
      "Pokemon Best Wishes! Season 2: Shinsoku no Genosect - Mewtwo Kakusei | Pokemon Movie 16: Shinsoku no Genosect - Mewtwo Kakusei\n",
      "Ghost in the Shell: Arise - Border:1 Ghost Pain | Koukaku Kidoutai Arise: Ghost in the Shell - Border:1 Ghost Pain\n",
      "Suzy&#039;s Zoo: Daisuki! Witzy - Happy Birthday | Suzy's Zoo: Daisuki! Witzy - Happy Birthday\n",
      "Maoyuu Maou Yuusha: Kono Monogatari wa, Daniku dake Dewanai no ja! | Maoyuu Maou Yuusha: Kono Monogatari wa, Daniku dake Dewanai no Ja!\n",
      "Ouritsu Uchuugun: Honneamise no Tsubasa - Pilot Film | Ouritsu Uchuugun: Honneamise no Tsubasa Pilot\n",
      "Sparrow&#039;s Hotel | Sparrow's Hotel\n",
      "Namiuchigiwa no Muromi-san OVA | Namiuchigiwa no Muromi-san: Pangea Chou Tairiku no Muromi-san\n",
      "Yonhyakunijuu Renpai Girl | Yonhyaku-nijuu Renpai Girl\n",
      "Girls und Panzer: Kore ga Hontou no Anzio-sen Desu! | Girls & Panzer: Kore ga Hontou no Anzio-sen Desu!\n",
      "Chihayafuru 2: Waga Mi Yo ni Furu Nagame Seshi Ma ni | Chihayafuru 2: Waga Miyo ni Furu Nagame Shima ni\n",
      "Ghost in the Shell: Arise - Border:2 Ghost Whispers | Koukaku Kidoutai Arise: Ghost in the Shell - Border:2 Ghost Whispers\n",
      "Ghost in the Shell: Arise - Border:3 Ghost Tears | Koukaku Kidoutai Arise: Ghost in the Shell - Border:3 Ghost Tears\n",
      "Ghost in the Shell: Arise - Border:4 Ghost Stands Alone | Koukaku Kidoutai Arise: Ghost in the Shell - Border:4 Ghost Stands Alone\n",
      "Naruto Shippuuden: Sunny Side Battle | Naruto: Shippuuden - Sunny Side Battle\n",
      "Miss Monochrome: The Animation | Miss Monochrome The Animation\n",
      "Choujigen Game Neptune The Animation OVA | Choujigen Game Neptune The Animation: Yakusoku no Eien - True End\n",
      "JoJo no Kimyou na Bouken: Stardust Crusaders | JoJo no Kimyou na Bouken Part 3: Stardust Crusaders\n",
      "Kyoukai no Kanata: Mini Theater | Kyoukai no Kanata: Mini Gekijou\n",
      "Log Horizon Recap | Imanara Maniau! Log Horizon\n",
      "Gochuumon wa Usagi desu ka? | Gochuumon wa Usagi Desu ka?\n",
      "Mushishi Special: Hihamukage | Mushishi: Hihamukage\n",
      "Persona 3 the Movie 2: Midsummer Knight&#039;s Dream | Persona 3 the Movie 2: Midsummer Knight's Dream\n",
      "Pokemon XY: Hakai no Mayu to Diancie | Pokemon Movie 17: Hakai no Mayu to Diancie\n",
      "Ping Pong The Animation | Ping Pong the Animation\n",
      "Brothers Conflict Special | Brothers Conflict: Setsubou\n",
      "Date A Live: Encore | Date A Live II: Kurumi Star Festival\n",
      "D-Fang!: Water!! | D-Frag!*\n",
      "Love Stage!!: Chotto Janakutte | Love Stage!!: Chotto ja Nakutte\n",
      "Lance N&#039; Masques | Lance N' Masques\n",
      "Yamada-kun to 7-nin no Majo (OVA) | Yamada-kun to 7-nin no Majo: Mou Hitotsu no Suzaku-sai\n",
      "Gokukoku no Brynhildr Special | Gokukoku no Brynhildr: Kara Sawagi\n",
      "Miss Monochrome: The Animation - Soccer-hen | Miss Monochrome The Animation: Soccer-hen\n",
      "Kyoukai no Kanata Movie 1: I&#039;ll Be Here - Kako-hen | Kyoukai no Kanata Movie 1: I'll Be Here - Kako-hen\n",
      "Barakamon: Mijikamon | Mijikamon\n",
      "Ryuugajou Nanana no Maizoukin (TV) Specials | Ryuugajou Nanana no Maizoukin (TV): Nanana Nippon Mukashibanashi\n",
      "Kono Danshi, Sekika ni Nayandemasu. | Kono Danshi, Sekka ni Nayandemasu.\n",
      "Akame ga Kill! Theater | AkaKill! Gekijou\n",
      "Gintama&#039;: Futon ni Haitte kara Buki Nokoshi ni Kizuite Neru ni Nerenai Toki mo Aru | Gintama': Futon ni Haitte kara Buki Nokoshi ni Kizuite Neru ni Nerenai Toki mo Aru\n",
      "Dragon Ball Z Movie 15: Fukkatsu no F | Dragon Ball Z Movie 15: Fukkatsu no \"F\"\n",
      "Fate/stay night Movie: Heaven&#039;s Feel - I. Presage Flower | Fate/stay night Movie: Heaven's Feel - I. Presage Flower\n",
      "DRAMAtical Murder OVA: Data_xx_Transitory | DRAMAtical Murder: Data_xx_Transitory\n",
      "Pokemon the Movie XY: Ring no Choumajin Hoopa | Pokemon Movie 18: Ring no Choumajin Hoopa\n",
      "JoJo no Kimyou na Bouken: Stardust Crusaders 2nd Season | JoJo no Kimyou na Bouken Part 3: Stardust Crusaders 2nd Season\n",
      "Fate/kaleid liner Prisma☆Illya 2wei! OVA | Fate/kaleid liner Prisma☆Illya 2wei!: Mahou Shoujo in Onsen Ryokou\n",
      "Free!: Eternal Summer – Kindan no All Hard! | Free!: Eternal Summer - Kindan no All Hard!\n",
      "Ghost in the Shell (2015) | Koukaku Kidoutai: Shin Movie\n",
      "Sword Gai: The Animation | Sword Gai The Animation\n",
      "Fate/stay night: Unlimited Blade Works - Prologue | Fate/stay night: Unlimited Blade Works Prologue\n",
      "Dragon Nest: Warriors&#039; Dawn | Long Zhi Gu: Poxiao Qibing\n",
      "Senran Kagura Estival Versus: Shoujo-tachi no Sentaku | Senran Kagura Estival Versus: Mizugi-darake no Zenyasai\n",
      "Miss Monochrome: The Animation - Manager | Miss Monochrome The Animation: Manager\n",
      "Dr. Slump: Arale-chan &#039;92 Oshougatsu Special | Dr. Slump: Arale-chan '92 Oshougatsu Special\n",
      "Shingeki no Bahamut: Genesis Recap | Shingeki no Bahamut: Genesis - Roundup\n",
      "Subete ga F ni Naru: The Perfect Insider | Subete ga F ni Naru\n",
      "Kyoukai no Kanata Movie 2: I&#039;ll Be Here - Mirai-hen | Kyoukai no Kanata Movie 2: I'll Be Here - Mirai-hen\n",
      "Gunslinger Stratos: The Animation | Gunslinger Stratos The Animation\n",
      "Shinmai Maou no Testament OVA | Shinmai Maou no Testament: Toujou Basara no Hard Sweet na Nichijou\n",
      "Ghost in the Shell: Arise - Alternative Architecture | Koukaku Kidoutai Arise: Alternative Architecture\n",
      "Gochuumon wa Usagi desu ka?? | Gochuumon wa Usagi Desu ka??\n",
      "Meiji Tokyo Renka Movie: Yumihari no Serenade | Meiji Tokyo Renka Movie 1: Yumihari no Serenade\n",
      "Aria The Avvenire | Aria the Avvenire\n",
      "Shinmai Maou no Testament Burst OVA | Shinmai Maou no Testament Burst: Toujou Basara no Shigoku Heiwa na Nichijou\n",
      "Miss Monochrome: The Animation 2 | Miss Monochrome The Animation 2\n",
      "Shin Atashin&#039;chi | Shin Atashin'chi\n",
      "Queen&#039;s Blade: Grimoire | Queen's Blade: Grimoire\n",
      "Code Geass: Boukoku no Akito 5 - Itoshiki Monotachi e | Code Geass: Boukoku no Akito 5 - Itoshiki Mono-tachi e\n",
      "Owari no Seraph: Owaranai Seraph | Owaranai Seraph\n",
      "Kyoukai no Kanata Movie: I&#039;ll Be Here - Kako-hen - Yakusoku no Kizuna | Kyoukai no Kanata Movie: I'll Be Here - Kako-hen - Yakusoku no Kizuna\n",
      "Love Live! School Idol Project: μ&#039;s →NEXT LoveLive! 2014 - Endless Parade Encore Animation | Love Live! School Idol Project: μ's →NEXT LoveLive! 2014 - Endless Parade Encore Animation\n",
      "Tales of Zestiria the X | Tales of Zestiria the Cross\n",
      "Arslan Senki (TV) Specials | Arslan Senki (TV): Arsen 4-koma Gekijou\n",
      "Arslan Senki (TV): Dakkan no Yaiba | Arslan Senki (TV): Tsuioku no Shou - Dakkan no Yaiba\n",
      "Pokemon the Movie XY&Z: Volcanion to Karakuri no Magearna | Pokemon Movie 19: Volcanion to Karakuri no Magearna\n",
      "Grisaia no Meikyuu Special | Grisaia no Meikyuu: Caprice no Mayu 0 - Takizono Basketball Club no Nama Cream Party!\n",
      "Terra Formars Revenge | Terra Formars: Revenge\n",
      "Classroom☆Crisis Special | Classroom☆Crisis: Tabi no Haji wa Uwanuri\n",
      "Miss Monochrome: The Animation 3 | Miss Monochrome The Animation 3\n",
      "Ghost in the Shell: The New Movie Virtual Reality Diver | Koukaku Kidoutai: Shin Movie Virtual Reality Diver\n",
      "Dragon Nest: Throne of Elves | Long Zhi Gu: Jingling Wangzuo\n",
      "Danchigai Special | Danchigai: Juusan Goutou Sentou Ikitai!!\n",
      "Dog Days&#039;&#039;: Limone Resort Tenbou Onsen! | Dog Days'': Limone Resort Tenbou Onsen!\n",
      "Owari no Seraph: Nagoya Kessen-hen - Owaranai Seraph - Nagoya Kessen-hen | Owaranai Seraph: Nagoya-hen\n",
      "PriPara Movie: Mi~nna no Akogare♪ Let&#039;s Go☆Prix Paris | PriPara Movie: Mi~nna no Akogare♪ Let's Go☆Prix Paris\n",
      "JoJo no Kimyou na Bouken: Diamond wa Kudakenai | JoJo no Kimyou na Bouken Part 4: Diamond wa Kudakenai\n",
      "Dog Days&#039;&#039;: Gravure Talk | Dog Days'': Gravure Talk\n",
      "Okusama ga Seitokaichou! OVA | Okusama ga Seitokaichou!: Seitokaichou to Ofuro Asobi\n",
      "Hibike! Euphonium Movie: Kitauji Koukou Suisougaku-bu e Youkoso | Hibike! Euphonium Movie 1: Kitauji Koukou Suisougaku-bu e Youkoso\n",
      "Mobile Suit Gundam-san: Bouya Dakara sa | Mobile Suit Gundam-san (Movie)\n",
      "Bonobono (2016) | Bonobono (TV 2016)\n",
      "Kono Subarashii Sekai ni Shukufuku wo! OVA | Kono Subarashii Sekai ni Shukufuku wo!: Kono Subarashii Choker ni Shukufuku wo!\n",
      "Dimension W: W no Tobira Online | Dimension W: W no Tobira Online - Rose no Onayami Soudanshitsu\n",
      "Prison School OVA | Prison School: Mad Wax\n",
      "Sakamoto desu ga? | Sakamoto Desu ga?\n",
      "Zombie Clay Animation: I&#039;m Stuck!! | Zombie Clay Animation: I'm Stuck!!\n",
      "Hai to Gensou no Grimgar Special | Hai to Gensou no Grimgar: Furoagari no Kabe ni Kaketa Seishun - One More Centimeter\n",
      "Tama & Friends: Uchi no Tama Shirimasenka? | Tama & Friends: Uchi no Tama Shirimasen ka?\n",
      "Haikyuu!!: Karasuno Koukou VS Shiratorizawa Gakuen Koukou | Haikyuu!!: Karasuno Koukou vs. Shiratorizawa Gakuen Koukou\n",
      "Asa da yo! Kaishain | Asa Da yo! Kaishain\n",
      "Dimension W Special | Dimension W: Short Track/Robot wa Sentou no Yume wo Miruka\n",
      "Musaigen no Phantom World Special | Musaigen no Phantom World: Mizutama no Kiseki\n",
      "Meiji Tokyo Renka Movie: Hanakagami no Fantasia | Meiji Tokyo Renka Movie 2: Hanakagami no Fantasia\n",
      "Ojisan to Marshmallow Special | Ojisan to Marshmallow: Hige-san to Yume Mashmallow\n",
      "Committed RED | Committed Red\n",
      "Koneko no Chii: Ponponra Daibouken | Koneko no Chi: Ponponra Daibouken\n",
      "Arslan Senki (TV) OVA | Arslan Senki (TV) Gaiden\n",
      "Saiki Kusuo no Ψ-nan (TV) | Saiki Kusuo no Ψ-nan\n",
      "Gochuumon wa Usagi desu ka??: Dear My Sister | Gochuumon wa Usagi Desu ka??: Dear My Sister\n",
      "Overlord: Ple Ple Pleiades (OVA) | Overlord: Ple Ple Pleiades - Nazarick Saidai no Kiki\n",
      "Ganbare! Kickers: Hitoribocchi no Ace Striker | Ganbare! Kickers Specials\n",
      "Ganbare! Kickers Specials | Ganbare! Kickers: Bokutachi no Densetsu\n",
      "Shuumatsu Nani Shitemasu ka? Isogashii desu ka? Sukutte Moratte Ii desu ka? | Shuumatsu Nani Shitemasu ka? Isogashii Desu ka? Sukutte Moratte Ii Desu ka?\n",
      "Sakamoto desu ga? Special | Sakamoto Desu ga? Special\n",
      "Tales of Zestiria the X: Saiyaku no Jidai | Tales of Zestiria the Cross: Saiyaku no Jidai\n",
      "Baki | Baki: Most Evil Death Row Convicts Special Anime\n",
      "Trinity Seven Movie: Eternity Library to Alchemic Girl | Trinity Seven Movie 1: Eternity Library to Alchemic Girl\n",
      "Taka no Tsume 8: Yoshida-kun no X-Files | Taka no Tsume 8: Yoshida-kun no Batten File\n",
      "Godzilla: Kaijuu Wakusei | Godzilla 1: Kaijuu Wakusei\n",
      "91 Days Recap | 91 Days: Mijikai Rousoku\n",
      "Girls & Panzer: Saishuushou | Girls & Panzer: Saishuushou Part 1\n",
      "Hatsukoi Monster OVA | Hatsukoi Monster: Mou Chotto dake Tsuzukunja\n",
      "Akiba&#039;s Trip The Animation | Akiba's Trip The Animation\n",
      "Tales of Zestiria the X 2nd Season | Tales of Zestiria the Cross 2nd Season\n",
      "Knight&#039;s & Magic | Knight's & Magic\n",
      "Sentai Hero Sukiyaki Force | Sentai Hero Sukiyaki Force: Gunma no Heiwa wo Negau Season\n",
      "Pokemon: Kimi ni Kimeta! | Pokemon Movie 20: Kimi ni Kimeta!\n",
      "Saiki Kusuo no Ψ-nan (TV) 2 | Saiki Kusuo no Ψ-nan 2\n",
      "Kono Subarashii Sekai ni Shukufuku wo! 2 OVA | Kono Subarashii Sekai ni Shukufuku wo! 2: Kono Subarashii Geijutsu ni Shukufuku wo!\n",
      "Nekopara | Nekopara OVA\n",
      "Eureka Seven AO Final Episode: One More Time - Lord Don&#039;t Slow Me Down | Eureka Seven AO: One More Time - Lord Don't Slow Me Down\n",
      "91 Days Special | 91 Days: Toki no Asase/Subete no Kinou/Ashita, Mata Ashita\n",
      "Youjo Senki Recap | Youjo Senki: Senkyou Houkoku\n",
      "Hakuouki: Otogisoushi - Special | Hakuouki: Otogisoushi Special\n",
      "Yume Oukoku to Nemureru 100 Nin no Ouji-sama: Short Stories | Yume Oukoku to Nemureru 100-nin no Ouji-sama: Short Stories\n",
      "To Be Heroine | Tu Bian Ying Xiong Leaf\n",
      "Hibike! Euphonium Movie: Todoketai Melody | Hibike! Euphonium Movie 2: Todoketai Melody\n",
      "Bleach Colorful!: Gotei Juusan Yatai Daisakusen! | Bleach KaraBuri!: Gotei Juusan Yatai Daisakusen!\n",
      "Koukyoushihen Eureka Seven: Hi-Evolution 1 | Koukyoushihen Eureka Seven Hi-Evolution 1\n",
      "Kobayashi-san Chi no Maid Dragon Specials | Kobayashi-san Chi no OO Dragon\n",
      "DC Super Heroes vs Taka no Tsume Dan | DC Super Heroes vs Taka no Tsume-dan\n",
      "Tsuki ga Kirei Recap | Tsuki ga Kirei: Michinori\n",
      "Mon-Soni! D&#039;Artagnan no Idol Sengen | Mon-Soni! D'Artagnan no Idol Sengen\n",
      "Gin no Guardian 2nd Season | Gin no Guardian II\n",
      "Haikyuu!!: vs \"Akaten\" | Haikyuu!!: vs. \"Akaten\"\n",
      "Boku dake ga Inai Machi Recap | Boku dake ga Inai Machi Recaps\n",
      "Alice or Alice: Siscon Niisan to Futago no Imouto | Alice or Alice\n",
      "Air Soushuuhen | Air Recap\n",
      "Ito Junji: Collection | Itou Junji: Collection\n",
      "World&#039;s End Umbrella | World's End Umbrella\n",
      "Zhan Guo FAN | Zhanguo Fan\n",
      "Road to You: Kimi e to Tsuzuku Michi | Road to You\n",
      "Overlord: Ple Ple Pleiades Theatrical Version | Overlord Movie: Ple Ple Pleiades\n",
      "Arslan Senki (TV): Fuujin Ranbu - 4-koma Gekijou | Arslan Senki (TV): Fuujin Ranbu - Arsen 4-koma Gekijou\n",
      "MahoYome Episode 0 | MahoYome: Aisatsu\n",
      "Dungeon Meshi: Senshi no Kantan Cooking! | Dungeon Meshi\n",
      "Souten no Ken Re:Genesis | Souten no Ken: Regenesis\n",
      "Da Hu Fa | Dahufa\n",
      "Fate/Apocrypha: Seihai Taisen Douran-hen | Fate/Apocrypha Recaps\n",
      "Nothing&#039;s Gonna Change | Nothing's Gonna Change\n",
      "Ling Yu: Di San Ji | Ling Yu 3rd Season\n",
      "Caligula | Caligula (TV)\n",
      "Aishen Qiaokeli-ing... 2nd Season | Aishen Qiaokeli-ing...II\n",
      "Nariyuki: Papakatsu Girls!! The Animation | Nariyuki→Papakatsu Girls!! The Animation\n",
      "Shokugeki no Souma: San no Sara - Toutsuki Ressha-hen | Shokugeki no Souma: San no Sara - Tootsuki Ressha-hen\n",
      "Shokugeki no Souma: San no Sara - Toutsuki Ressha-hen OVA | Shokugeki no Souma: San no Sara - Kyokuseiryou no Erina\n",
      "Season&#039;s Greetings 2017 from Dwarf | Season's Greetings from dwarf\n",
      "Petit☆Dream Stars! Let&#039;s La Cooking? Showtime! | Petit☆Dream Stars! Let's La Cooking? Showtime!\n",
      "Dies Irae: Marie&#039;s Memory \"Michi ni Tsuuzu Kiseki\" | Dies Irae: Marie's Memory \"Michi ni Tsuuzu Kiseki\"\n",
      "DC Super Heroes vs Taka no Tsume Dan Promotion Eizou | DC Super Heroes vs Taka no Tsume-dan Promotion Eizou\n",
      "Wo De Ni Tian Shen Qi | Wo de Ni Tian Shen Qi\n",
      "Sukoyaka Oyako 21 x Taka no Tsume Dan | Sukoyaka Oyako 21 x Taka no Tsume-dan\n",
      "Medama Oyaji no April Fools&#039; | Medama Oyaji no April Fools'\n",
      "Golden Kamuy: Golden Douga Gekijou | Golden Douga Gekijou\n",
      "Inazuma Eleven Reloaded: Reformation of Soccer | Inazuma Eleven: Reloaded - Soccer no Henkaku\n",
      "Free! Movie 1: Timeless Medley - Kizuna: Character Butai Aisatsu | Free! Movie 1: Timeless Medley - Kizuna - Character Butai Aisatsu\n",
      "418 | 6656\n"
     ]
    }
   ],
   "source": [
    "for col in ['title', 'title_english', 'title_synonyms', 'name', 'english_name']:\n",
    "    print(col)\n",
    "    print(\"NaN:\", df_main[col].isna().sum())\n",
    "    print(df_main[col].isin(['Unknown']).value_counts())\n",
    "    print()\n",
    "\n",
    "neq, tot = 0, 0\n",
    "for _, row in df_main.iterrows():\n",
    "    if row['title'] != row['name']:\n",
    "        print(f\"{row['title']} | {row['name']}\")\n",
    "        neq += 1\n",
    "    tot += 1\n",
    "\n",
    "print(f\"{neq} | {tot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b3e46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.drop(columns=['title', 'title_english', 'title_synonyms', 'english_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f885301",
   "metadata": {},
   "source": [
    "### GENRES column cleaning\n",
    "\n",
    "To merge genres columns we make a union of genres sets of both columns: *genres_main* & *genres_secondary*. We see that two anime titles has 'Unknown' in both datasets. Remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e59e9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['genres_main', 'genres_secondary']:\n",
    "    df_main[col] = df_main[col].fillna(\"\").apply(lambda s: set(x.strip() for x in s.split(\", \")) if s else set())\n",
    "\n",
    "df_main['genres'] = df_main.apply(lambda r: r['genres_main'] | r['genres_secondary'], axis=1)\n",
    "df_main = df_main[df_main['genres'] != {'Unknown'}]\n",
    "df_main.drop(columns=['genres_main', 'genres_secondary'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073863cf",
   "metadata": {},
   "source": [
    "### RATING (age category) column cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2993ff5",
   "metadata": {},
   "source": [
    "First let us look on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22eb23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only_main_ds_NaNs:\t35\n",
      "only_sec_ds_NaNs:\t0\n",
      "both_nans:\t46\n",
      "both_not_nan_but_diff:\t47\n",
      "total:\t6654\n",
      "\n",
      "CANDIATES TO REMOVE (both nans: 47):\n",
      "max_score:\t7.42\n",
      "min_rank:\t1927.0\n",
      "\n",
      "Ratings set:\t{'G - All Ages', 'Rx - Hentai', 'R+ - Mild Nudity', 'PG-13 - Teens 13 or older', 'PG - Children', 'R - 17+ (violence & profanity)', nan, 'Unknown'}\n"
     ]
    }
   ],
   "source": [
    "nan_only_main, nan_only_sec, nan_both, both_not_nan_but_diff, tot = 0, 0, 0, 0, 0\n",
    "rating_set = set()\n",
    "max_score = 0\n",
    "min_rank = 999999\n",
    "\n",
    "for _, row in df_main.iterrows():\n",
    "    main_nan_cond = pd.isnull(row['rating_main'])\n",
    "    sec_nan_cond = (row['rating_secondary'] == 'Unknown')\n",
    "\n",
    "    nan_only_main += int(main_nan_cond and not sec_nan_cond)\n",
    "    nan_only_sec += int(sec_nan_cond and not main_nan_cond)\n",
    "    nan_both += int(main_nan_cond and sec_nan_cond)\n",
    "    both_not_nan_but_diff += not (main_nan_cond or sec_nan_cond) and (row['rating_main'] != row['rating_secondary'])\n",
    "\n",
    "    rating_set.add(row['rating_main'])\n",
    "    rating_set.add(row['rating_secondary'])\n",
    "    if main_nan_cond and sec_nan_cond:\n",
    "        if row['score_main'] > max_score:\n",
    "            max_score = row['score_main']\n",
    "        if row['rank'] < min_rank:\n",
    "            min_rank = row['rank']\n",
    "\n",
    "    tot += 1\n",
    "\n",
    "print(f\"only_main_ds_NaNs:\\t{nan_only_main}\\nonly_sec_ds_NaNs:\\t{nan_only_sec}\\nboth_nans:\\t{nan_both}\\nboth_not_nan_but_diff:\\t{both_not_nan_but_diff}\\ntotal:\\t{tot}\")\n",
    "print(f\"\\nCANDIATES TO REMOVE (both nans: {both_not_nan_but_diff}):\\nmax_score:\\t{max_score}\\nmin_rank:\\t{min_rank}\")\n",
    "print(f\"\\nRatings set:\\t{rating_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274115c",
   "metadata": {},
   "source": [
    "We see that if we apropriate following strategy:\n",
    "\n",
    "- if one of two ratings is missed, fill with the second one;\n",
    "\n",
    "- if both are missed, remove this anime title;\n",
    "\n",
    "- if both are koen but differ, than fill with more strong,\n",
    "\n",
    "then we will remove 46 rows. The minimal rank (the highest position) is 1927, and maximal score is 7.42.\n",
    "Obviously, these are not so culturaly significant titles. So we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "baedbe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6654\n",
      "6608\n"
     ]
    }
   ],
   "source": [
    "order = {\n",
    "    'G - All Ages': 0,\n",
    "    'PG - Children': 1,\n",
    "    'PG-13 - Teens 13 or older': 2,\n",
    "    'R - 17+ (violence & profanity)': 3,\n",
    "    'R+ - Mild Nudity': 4,\n",
    "    'Rx - Hentai': 5\n",
    "}\n",
    "\n",
    "def is_missing(x):\n",
    "    return pd.isna(x) or str(x).strip().lower() in ['unknown', 'nan', '']\n",
    "\n",
    "def merge_rating(main, sec):\n",
    "    if is_missing(main) and is_missing(sec):\n",
    "        return np.nan\n",
    "    if is_missing(main):\n",
    "        return sec\n",
    "    if is_missing(sec):\n",
    "        return main\n",
    "    return main if order[main] >= order[sec] else sec\n",
    "\n",
    "print(len(df_main))\n",
    "df_main['rating'] = df_main.apply(lambda r: merge_rating(r['rating_main'], r['rating_secondary']), axis=1)\n",
    "df_main.drop(columns=['rating_main', 'rating_secondary'], inplace=True)\n",
    "df_main = df_main[df_main['rating'].notna()]\n",
    "print(len(df_main))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ace31a",
   "metadata": {},
   "source": [
    "### MEMBERS, FAVORITE column cleaning\n",
    "\n",
    "These columns are clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d98e3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['members', 'favorites']:\n",
    "    df_main[col] = df_main[f\"{col}_main\"]\n",
    "    df_main.drop(columns=[f\"{col}_main\", f\"{col}_secondary\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5f28e",
   "metadata": {},
   "source": [
    "### AIRING, EPISODES column cleaning\n",
    "\n",
    "As airing we mean such columns: *'aired_main'*, *'premiered_main'*, *'aired_from_year'*, *'aired_secondary'*, *'premiered_secondary'*. In common they are so dirty.\n",
    "\n",
    "The most full information contains in *aired_* columns.\n",
    "\n",
    "*aired_main* column contains values such **{'from': '1998-04-01', 'to': '1998-09-30'}** (a string, not a dictionary!) where every value can be **None**.\n",
    "\n",
    "*aired_secondary* column contains values such **Apr 3, 1998 to Apr 24, 1999** or for one-episode items: **May 25, 2018**. Can also contain *'Unknown'*.\n",
    "\n",
    "We will prefer *aired_secondary* column in the cases of conflicts.\n",
    "\n",
    "Then drop all other columns related to dates.\n",
    "\n",
    "Column **episodes** is closely related to airing, because the number of episodes is changing with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d6bfe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anime_id', 'image_url', 'episodes_main', 'score_main', 'scored_by',\n",
       "       'rank', 'opening_theme', 'ending_theme', 'duration_min', 'name',\n",
       "       'score_secondary', 'type', 'episodes_secondary', 'studio', 'source',\n",
       "       'duration', 'genres', 'rating', 'members', 'favorites', 'year_start',\n",
       "       'year_finish', 'month_start', 'month_finish'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_date(x):\n",
    "    \"\"\"any string -> YYYY-MM-DD.\"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return None\n",
    "\n",
    "    x = x.strip()\n",
    "\n",
    "    # \"2000\"\n",
    "    if len(x) == 4 and x.isdigit():\n",
    "        return f\"{x}-01-01\"\n",
    "\n",
    "    try:\n",
    "        dt = parser.parse(x)\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_secondary(sec_str):\n",
    "    if not isinstance(sec_str, str):\n",
    "        return None, None\n",
    "\n",
    "    parts = sec_str.split(' to ')\n",
    "    start = parts[0].strip()\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return start, start\n",
    "\n",
    "    end = parts[1].strip()\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def parse_main(main_obj):\n",
    "    if isinstance(main_obj, str) and main_obj.startswith('{'):\n",
    "        main_obj = ast.literal_eval(main_obj)\n",
    "\n",
    "    if isinstance(main_obj, dict):\n",
    "        return main_obj.get('from'), main_obj.get('to')\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def merge_aired(row):\n",
    "    mf, mt = parse_main(row['aired_main'])\n",
    "    sf, st = parse_secondary(row['aired_secondary'])\n",
    "\n",
    "    start = sf if sf else mf\n",
    "    end   = st if (st and st != '?') else mt\n",
    "\n",
    "    if start and not end:\n",
    "        end = None  # may be it is ongoing\n",
    "\n",
    "    # normalize (to ISO)\n",
    "    start = normalize_date(start)\n",
    "    end   = normalize_date(end)\n",
    "\n",
    "    return start, end\n",
    "\n",
    "\n",
    "df_main[['air_start', 'air_end']] = df_main.apply(merge_aired, axis=1, result_type='expand')\n",
    "\n",
    "df_main['year_start'] = pd.to_datetime(df_main['air_start']).dt.year.astype('Int64')\n",
    "df_main['year_finish'] = pd.to_datetime(df_main['air_end']).dt.year.astype('Int64')\n",
    "\n",
    "df_main['month_start'] = pd.to_datetime(df_main['air_start']).dt.month_name()\n",
    "df_main['month_finish'] = pd.to_datetime(df_main['air_end']).dt.month_name()\n",
    "\n",
    "df_main.drop(columns=['aired_main', 'premiered_main', 'aired_from_year', 'aired_secondary', 'premiered_secondary', 'air_start', 'air_end'], inplace=True)\n",
    "df_main.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664ab91",
   "metadata": {},
   "source": [
    "### EPISODES column cleaning\n",
    "\n",
    "There are only 31 anime titles which have missed values both in **main** and **secondary** tables. Below we discuss how to fill them. Any strategy has it's own cons, but zeros will be worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16bc40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to fill through requests:  31\n"
     ]
    }
   ],
   "source": [
    "df_main['episodes'] = np.where(\n",
    "    df_main['episodes_main'] != 0,\n",
    "    df_main['episodes_main'],\n",
    "    np.where(\n",
    "        df_main['episodes_secondary'] != 'Unknown',\n",
    "        df_main['episodes_secondary'],\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "df_main.drop(columns=['episodes_main', 'episodes_secondary'], inplace=True)\n",
    "\n",
    "print(\"Need to fill through requests: \", (df_main['episodes'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848a077",
   "metadata": {},
   "source": [
    "We can notice one interesting thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eda4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad years of finish: 33\n",
      "bad episod numbers: 31\n",
      "crosssection: 31\n"
     ]
    }
   ],
   "source": [
    "bad_year_finish = set(df_main[df_main['year_finish'].isna()]['anime_id'])\n",
    "bad_episodes = set(df_main[df_main['episodes'].isna()]['anime_id'])\n",
    "\n",
    "print(f\"bad years of finish: {len(bad_year_finish)}\")\n",
    "print(f\"bad episod numbers: {len(bad_episodes)}\")\n",
    "print(f\"crosssection: {len(bad_year_finish.intersection(bad_episodes))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e0cd3",
   "metadata": {},
   "source": [
    "So, in most cases these are the same anime titles. It seems that they were ongoings at those moment of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed73b3",
   "metadata": {},
   "source": [
    "Big part of them have rank bigger than 10_000. But not all. For example, here we also have items with ranks 91, 272, 1032. It's so high. We can not remove these items. So we can parse these values from MAL website. It is very inefficient to do it without suitable API, but we don't need a big amount of data.\n",
    "\n",
    "But here is another problem.\n",
    "\n",
    "- Some of these 31-33 anime titles really have *'Unknown'* in the **episodes** field on the website.\n",
    "\n",
    "- Even if some titles has this field filled - is this title finished before 2018? In most of cases - no. Some of them are still airing. So field **episodes** will contain number of episodes for the moment of parsing - 2025.\n",
    "\n",
    "We have two options:\n",
    "\n",
    "1. To parse different anime fan-hub. But anyway - i there are no missing data there - there will be data for 2025.\n",
    "\n",
    "2. Second way seems more balanced:\n",
    "\n",
    "- 1. Find anime titles with mising **episodes** in both tables. Filter only those which was finished before 2018. Let it be set **P**.\n",
    "- 2. Parse the number of episodes for them from MAL webste (by the way will try to fill date of airing finish, if it earlier then 2018).\n",
    "- 3. Ask ChatGPT to fill missing episodes fields for ***all*** problem titles, ask it to find information about number of episodes in the 2018 (archives, forums, etc.).\n",
    "- 4. Compare ChatGPT's results with parsed results for animes from the set **P**. If in the most cases they will (almost) the same, take ChtGPT's results.\n",
    "\n",
    "- * by the way we can fill the date of finish airing if it is good and if it is before 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14a736f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_episodes_and_finish(anime_id: int, timeout: int = 10):\n",
    "    url = f\"https://myanimelist.net/anime/{anime_id}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    html = resp.text\n",
    "    \n",
    "    # look for <span class=\"dark_text\">Episodes:</span> ... \n",
    "    match = re.search(r'<span class=\"dark_text\">Episodes:</span>\\s*([^<\\n]+)', html)\n",
    "    if match:\n",
    "        value = match.group(1).strip()\n",
    "        if value.isdigit():\n",
    "            ep = int(value)\n",
    "        else:\n",
    "            ep = None\n",
    "\n",
    "    m = re.search(r'<span class=\"dark_text\">Aired:</span>\\s*([^<\\n]+)', html)\n",
    "    if m:\n",
    "        aired_str = m.group(1).strip()\n",
    "    return ep, aired_str\n",
    "\n",
    "\n",
    "def get_episodes_and_finish_from_list(anime_id_set: set, fake_run=True, verbose=False) -> dict:\n",
    "    real_dict = {\n",
    "        33922: [9, 'Apr 6, 2016 to Mar 27, 2023'], 6149: [None, 'Jan 8, 1995 to ?'],\n",
    "        34566: [293, 'Apr 5, 2017 to Mar 26, 2023'], 35590: [11, 'May 5, 2017 to Mar 15, 2020'],\n",
    "        22669: [None, 'Apr 25, 2011 to ?'], 8336: [None, 'Mar 29, 2010 to ?'],\n",
    "        24977: [58, 'Jul 26, 2014 to Mar 6, 2023'], 21: [None, 'Oct 20, 1999 to ?'],\n",
    "        35478: [10, 'May 6, 2017 to Mar 16, 2018'], 37020: [52, 'Oct 2, 2017 to Mar 22, 2019'],\n",
    "        34845: [None, 'Mar 13, 2014 to ?'], 35229: [82, 'Nov 29, 2013 to Jun 7, 2018'],\n",
    "        1960: [None, 'Oct 3, 1988 to ?'], 34092: [3, 'Sep 17, 2016 to Sep 25, 2017'],\n",
    "        28205: [1, 'Oct 30, 2014'], 1199: [None, 'Apr 10, 1993 to ?'],\n",
    "        29361: [1, 'Feb 6, 2019'], 36409: [6, 'Sep 22, 2017 to Aug 14, 2019'],\n",
    "        32956: [230, 'Apr 8, 2016 to Mar 4, 2022'], 37187: [20, 'Jul 19, 2014 to Dec 27, 2014'],\n",
    "        24773: [9, 'Oct 11, 2013 to Mar 7, 2020'], 966: [None, 'Apr 13, 1992 to ?'],\n",
    "        24775: [None, 'Jan 3, 2015 to ?'], 24793: [26, 'Jul 18, 2012 to ?'],\n",
    "        32353: [None, 'Apr 2, 2016 to ?'], 2406: [None, 'Oct 5, 1969 to ?'],\n",
    "        34535: [21, 'Mar 4, 2017 to 2020'], 37096: [None, 'Apr 2, 2018 to ?'],\n",
    "        235: [None, 'Jan 8, 1996 to ?'], 4459: [None, 'Oct 5, 1998 to ?'],\n",
    "        8687: [None, 'Apr 22, 2005 to ?'], 23539: [1717, 'Mar 31, 2014 to Dec 24, 2023'], 18941: [None, 'Apr 2, 2012 to ?']}\n",
    "    if fake_run:\n",
    "        return real_dict\n",
    "\n",
    "    dict_id_episodes = {}\n",
    "    for aid in anime_id_set:\n",
    "        if verbose: print(f\"Read for id {aid}...\")\n",
    "        ep, aired = get_episodes_and_finish(aid)\n",
    "        dict_id_episodes[aid] = [ep, aired]\n",
    "        if verbose: print(f\"id: {aid}, finish_date: {aired}, episodes: {ep}\")\n",
    "        time.sleep(1)\n",
    "    return dict_id_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "418fde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_episodes = get_episodes_and_finish_from_list(bad_year_finish.union(bad_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c926ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>episodes</th>\n",
       "      <th>month_finish</th>\n",
       "      <th>year_finish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33922</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34566</td>\n",
       "      <td>293.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35590</td>\n",
       "      <td>11.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24977</td>\n",
       "      <td>58.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35478</td>\n",
       "      <td>10.0</td>\n",
       "      <td>March</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37020</td>\n",
       "      <td>52.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35229</td>\n",
       "      <td>82.0</td>\n",
       "      <td>June</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34092</td>\n",
       "      <td>3.0</td>\n",
       "      <td>September</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>October</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36409</td>\n",
       "      <td>6.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32956</td>\n",
       "      <td>230.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37187</td>\n",
       "      <td>20.0</td>\n",
       "      <td>December</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24773</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24793</td>\n",
       "      <td>26.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34535</td>\n",
       "      <td>21.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23539</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    anime_id  episodes month_finish  year_finish\n",
       "0      33922       9.0     November         2025\n",
       "1       6149       NaN     November         2025\n",
       "2      34566     293.0     November         2025\n",
       "3      35590      11.0     November         2025\n",
       "4      22669       NaN     November         2025\n",
       "5       8336       NaN     November         2025\n",
       "6      24977      58.0     November         2025\n",
       "7         21       NaN     November         2025\n",
       "8      35478      10.0        March         2018\n",
       "9      37020      52.0     November         2025\n",
       "10     34845       NaN     November         2025\n",
       "11     35229      82.0         June         2018\n",
       "12      1960       NaN     November         2025\n",
       "13     34092       3.0    September         2017\n",
       "14     28205       1.0      October         2014\n",
       "15      1199       NaN     November         2025\n",
       "16     29361       1.0     November         2025\n",
       "17     36409       6.0     November         2025\n",
       "18     32956     230.0     November         2025\n",
       "19     37187      20.0     December         2014\n",
       "20     24773       9.0     November         2025\n",
       "21       966       NaN     November         2025\n",
       "22     24775       NaN     November         2025\n",
       "23     24793      26.0     November         2025\n",
       "24     32353       NaN     November         2025\n",
       "25      2406       NaN     November         2025\n",
       "26     34535      21.0     November         2025\n",
       "27     37096       NaN     November         2025\n",
       "28       235       NaN     November         2025\n",
       "29      4459       NaN     November         2025\n",
       "30      8687       NaN     November         2025\n",
       "31     23539    1717.0     November         2025\n",
       "32     18941       NaN     November         2025"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in dict_id_episodes.keys():\n",
    "    _, end = [normalize_date(parse_secondary(dict_id_episodes[k][1])[i]) for i in range(2)]\n",
    "    if (not end) or (pd.to_datetime(end).year > 2018):\n",
    "        end = date.today()\n",
    "    dict_id_episodes[k] = {\n",
    "        'episodes': dict_id_episodes[k][0],\n",
    "        'month_finish': pd.to_datetime(end).month_name(),\n",
    "        'year_finish': pd.to_datetime(end).year,\n",
    "        }\n",
    "\n",
    "df_ep = (\n",
    "    pd.DataFrame\n",
    "       .from_dict(dict_id_episodes, orient='index', columns=['episodes', 'month_finish', 'year_finish'])\n",
    "       .reset_index()\n",
    "       .rename(columns={'index': 'anime_id'})\n",
    ")\n",
    "\n",
    "df_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf16bca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>episodes</th>\n",
       "      <th>month_start</th>\n",
       "      <th>year_start</th>\n",
       "      <th>month_finish</th>\n",
       "      <th>year_finish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>October</td>\n",
       "      <td>1999</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>235</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>January</td>\n",
       "      <td>1996</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>966</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>1992</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>1199</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>1993</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1960</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>October</td>\n",
       "      <td>1988</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>2406</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>October</td>\n",
       "      <td>1969</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>4459</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>October</td>\n",
       "      <td>1998</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>6149</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>January</td>\n",
       "      <td>1995</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>8336</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>March</td>\n",
       "      <td>2010</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>8687</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>2005</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>18941</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>2012</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>22669</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>2011</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>23539</td>\n",
       "      <td>1717</td>\n",
       "      <td>March</td>\n",
       "      <td>2014</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>24773</td>\n",
       "      <td>9</td>\n",
       "      <td>October</td>\n",
       "      <td>2013</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>24775</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>24793</td>\n",
       "      <td>26</td>\n",
       "      <td>July</td>\n",
       "      <td>2012</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>24977</td>\n",
       "      <td>58</td>\n",
       "      <td>July</td>\n",
       "      <td>2014</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>28205</td>\n",
       "      <td>1</td>\n",
       "      <td>October</td>\n",
       "      <td>2014</td>\n",
       "      <td>October</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>29361</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>32353</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>32956</td>\n",
       "      <td>230</td>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>33922</td>\n",
       "      <td>9</td>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>34092</td>\n",
       "      <td>3</td>\n",
       "      <td>September</td>\n",
       "      <td>2016</td>\n",
       "      <td>September</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>34535</td>\n",
       "      <td>21</td>\n",
       "      <td>March</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6003</th>\n",
       "      <td>34566</td>\n",
       "      <td>293</td>\n",
       "      <td>April</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>34845</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>March</td>\n",
       "      <td>2014</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>35229</td>\n",
       "      <td>82</td>\n",
       "      <td>November</td>\n",
       "      <td>2013</td>\n",
       "      <td>June</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6223</th>\n",
       "      <td>35478</td>\n",
       "      <td>10</td>\n",
       "      <td>May</td>\n",
       "      <td>2017</td>\n",
       "      <td>March</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>35590</td>\n",
       "      <td>11</td>\n",
       "      <td>May</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6385</th>\n",
       "      <td>36409</td>\n",
       "      <td>6</td>\n",
       "      <td>September</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>37020</td>\n",
       "      <td>52</td>\n",
       "      <td>October</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>37096</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>April</td>\n",
       "      <td>2018</td>\n",
       "      <td>November</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6560</th>\n",
       "      <td>37187</td>\n",
       "      <td>20</td>\n",
       "      <td>July</td>\n",
       "      <td>2014</td>\n",
       "      <td>December</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anime_id  episodes month_start  year_start month_finish  year_finish\n",
       "11          21      <NA>     October        1999     November         2025\n",
       "197        235      <NA>     January        1996     November         2025\n",
       "745        966      <NA>       April        1992     November         2025\n",
       "909       1199      <NA>       April        1993     November         2025\n",
       "1351      1960      <NA>     October        1988     November         2025\n",
       "1583      2406      <NA>     October        1969     November         2025\n",
       "2239      4459      <NA>     October        1998     November         2025\n",
       "2573      6149      <NA>     January        1995     November         2025\n",
       "2948      8336      <NA>       March        2010     November         2025\n",
       "3022      8687      <NA>       April        2005     November         2025\n",
       "4144     18941      <NA>       April        2012     November         2025\n",
       "4420     22669      <NA>       April        2011     November         2025\n",
       "4502     23539      1717       March        2014     November         2025\n",
       "4592     24773         9     October        2013     November         2025\n",
       "4593     24775      <NA>     January        2015     November         2025\n",
       "4596     24793        26        July        2012     November         2025\n",
       "4613     24977        58        July        2014     November         2025\n",
       "4785     28205         1     October        2014      October         2014\n",
       "4883     29361         1     January        2015     November         2025\n",
       "5400     32353      <NA>       April        2016     November         2025\n",
       "5555     32956       230       April        2016     November         2025\n",
       "5820     33922         9       April        2016     November         2025\n",
       "5871     34092         3   September        2016    September         2017\n",
       "5990     34535        21       March        2017     November         2025\n",
       "6003     34566       293       April        2017     November         2025\n",
       "6077     34845      <NA>       March        2014     November         2025\n",
       "6171     35229        82    November        2013         June         2018\n",
       "6223     35478        10         May        2017        March         2018\n",
       "6249     35590        11         May        2017     November         2025\n",
       "6385     36409         6   September        2017     November         2025\n",
       "6526     37020        52     October        2017     November         2025\n",
       "6540     37096      <NA>       April        2018     November         2025\n",
       "6560     37187        20        July        2014     December         2014"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined = df_main.merge(\n",
    "    df_ep,\n",
    "    how=\"left\",\n",
    "    on=\"anime_id\",\n",
    "    suffixes=(\"\", \"_src\")\n",
    ")\n",
    "\n",
    "for col in [\"episodes\", \"month_finish\", \"year_finish\"]:\n",
    "    df_joined[col] = df_joined[col].fillna(df_joined[f\"{col}_src\"])\n",
    "\n",
    "df_joined = df_joined[[c for c in df_joined.columns if not c.endswith(\"_src\")]]\n",
    "df_joined[df_joined['anime_id'].isin(bad_year_finish)][[\"anime_id\", \"rank\", \"episodes\", \"month_finish\", \"year_finish\"]]\n",
    "df_joined[\"episodes\"] = (pd.to_numeric(df_joined[\"episodes\"], errors=\"coerce\").astype(\"Int64\"))\n",
    "df_joined[\"year_start\"] = df_joined[\"year_start\"].astype(\"Int64\")\n",
    "df_joined[\"year_finish\"] = df_joined[\"year_finish\"].astype(\"Int64\")\n",
    "\n",
    "df_joined[df_joined['anime_id'].isin(bad_year_finish)][[\"anime_id\", \"episodes\", \"month_start\", \"year_start\", \"month_finish\", \"year_finish\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca48421",
   "metadata": {},
   "source": [
    "We see that we managed to restore almost nothing. Unfortunately the most \"big\" titles are really have 'Unknown' values in the *'episodes'* variable, because they are still airing. We can not remove these items from the dataframe, because of their large influence, so will consider this result for *'episodes'* column as the final one.\n",
    "But now **month_finish** and **year_finish** columns are more or less OK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7911269",
   "metadata": {},
   "source": [
    "### SCORE column cleaning\n",
    "\n",
    "We must prefer *'score_main'* column. To this moment we have only 6 anime titles with zero problem in this column. All of them have rank ~ 10_000. We can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b378deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         rank\n",
      "6580   9913.0\n",
      "6599  10587.0\n",
      "6600  10118.0\n",
      "6601  10119.0\n",
      "6602  10120.0\n",
      "6603  12719.0\n"
     ]
    }
   ],
   "source": [
    "print(df_joined[df_joined['score_main'] == 0][['rank']])\n",
    "\n",
    "df_joined = df_joined[df_joined['score_main'] != 0]\n",
    "df_joined.rename(columns={\"score_main\": \"score\"}, inplace=True)\n",
    "df_joined.drop(columns=[\"score_secondary\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0563a2",
   "metadata": {},
   "source": [
    "What about duration? In reality all of them really *'Unknown'*. So, we can do nothing. We can hadle with them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a987ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unknown duration:  15\n",
      "Number of Unknown duration_min:  37\n",
      "Length of intersection:  15\n",
      "Length of union 37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4531</th>\n",
       "      <td>23943</td>\n",
       "      <td>5023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>25441</td>\n",
       "      <td>7337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>30232</td>\n",
       "      <td>12356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>30313</td>\n",
       "      <td>10706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>31244</td>\n",
       "      <td>8964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>31361</td>\n",
       "      <td>11424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>31562</td>\n",
       "      <td>11435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>32197</td>\n",
       "      <td>10702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>32275</td>\n",
       "      <td>11410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>32276</td>\n",
       "      <td>11409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5413</th>\n",
       "      <td>32402</td>\n",
       "      <td>11251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>32676</td>\n",
       "      <td>12219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>32916</td>\n",
       "      <td>12744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>33087</td>\n",
       "      <td>11216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>33280</td>\n",
       "      <td>1606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>34032</td>\n",
       "      <td>3603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>34151</td>\n",
       "      <td>11060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>34198</td>\n",
       "      <td>10733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5960</th>\n",
       "      <td>34439</td>\n",
       "      <td>962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>34490</td>\n",
       "      <td>12305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>34871</td>\n",
       "      <td>12193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>34944</td>\n",
       "      <td>672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>35070</td>\n",
       "      <td>9541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>35134</td>\n",
       "      <td>6695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6209</th>\n",
       "      <td>35416</td>\n",
       "      <td>10667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6261</th>\n",
       "      <td>35656</td>\n",
       "      <td>11659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6273</th>\n",
       "      <td>35740</td>\n",
       "      <td>10008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>35798</td>\n",
       "      <td>1519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6291</th>\n",
       "      <td>35842</td>\n",
       "      <td>3156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>36436</td>\n",
       "      <td>6626.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>36727</td>\n",
       "      <td>12050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>36737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6480</th>\n",
       "      <td>36803</td>\n",
       "      <td>3964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>36849</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538</th>\n",
       "      <td>37088</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>37283</td>\n",
       "      <td>5281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6579</th>\n",
       "      <td>37493</td>\n",
       "      <td>12704.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anime_id     rank\n",
       "4531     23943   5023.0\n",
       "4643     25441   7337.0\n",
       "4953     30232  12356.0\n",
       "4969     30313  10706.0\n",
       "5145     31244   8964.0\n",
       "5161     31361  11424.0\n",
       "5215     31562  11435.0\n",
       "5372     32197  10702.0\n",
       "5389     32275  11410.0\n",
       "5390     32276  11409.0\n",
       "5413     32402  11251.0\n",
       "5476     32676  12219.0\n",
       "5538     32916  12744.0\n",
       "5605     33087  11216.0\n",
       "5662     33280   1606.0\n",
       "5856     34032   3603.0\n",
       "5883     34151  11060.0\n",
       "5891     34198  10733.0\n",
       "5960     34439    962.0\n",
       "5973     34490  12305.0\n",
       "6085     34871  12193.0\n",
       "6106     34944    672.0\n",
       "6134     35070   9541.0\n",
       "6149     35134   6695.0\n",
       "6209     35416  10667.0\n",
       "6261     35656  11659.0\n",
       "6273     35740  10008.0\n",
       "6279     35798   1519.0\n",
       "6291     35842   3156.0\n",
       "6388     36436   6626.0\n",
       "6453     36727  12050.0\n",
       "6457     36737      NaN\n",
       "6480     36803   3964.0\n",
       "6494     36849      NaN\n",
       "6538     37088      NaN\n",
       "6570     37283   5281.0\n",
       "6579     37493  12704.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of Unknown duration: \", (df_joined['duration'] == 'Unknown').sum())\n",
    "print(\"Number of Unknown duration_min: \", (df_joined['duration_min'] == 0).sum())\n",
    "\n",
    "bad_dur_set = set(df_joined[df_joined['duration'] == 'Unknown']['anime_id'])\n",
    "bad_dur_min_set = set(df_joined[df_joined['duration_min'] == 0]['anime_id'])\n",
    "\n",
    "print(\"Length of intersection: \", len(bad_dur_min_set.intersection(bad_dur_set)))\n",
    "print(\"Length of union\", len(bad_dur_min_set.union(bad_dur_set)))\n",
    "\n",
    "df_joined[df_joined['anime_id'].isin(bad_dur_min_set)][['anime_id', 'rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e846a",
   "metadata": {},
   "source": [
    "After all we can make KS-statistics one more time - between **main** and new, cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fccac409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6656 6602\n",
      "Score:\tKstestResult(statistic=np.float64(0.0042547931279565635), pvalue=np.float64(0.9999999767841098), statistic_location=np.float64(6.27), statistic_sign=np.int8(1))\n",
      "Score:\t6.849902343749999, 6.863621629809148\n",
      "Score:\t0.9276754800859404, 0.9009127206381894\n",
      "\n",
      "members:\tKstestResult(statistic=np.float64(0.008205646453289214), pvalue=np.float64(0.9770012093408235), statistic_location=np.float64(0.5099158653846154), statistic_sign=np.int8(1))\n",
      "members:\t7.147072470399755, 7.263563403854101\n",
      "members:\t15.803898260326081, 15.985205525947693\n",
      "\n",
      "favorites:\tKstestResult(statistic=np.float64(0.06750685980938177), pvalue=np.float64(1.3937618179339253e-13), statistic_location=np.float64(0.00015024038461538462), statistic_sign=np.int8(1))\n",
      "favorites:\t0.10089695947410086, 0.10255273639917621\n",
      "favorites:\t0.5748385310223462, 0.581832576411304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(main_df), len(df_joined))\n",
    "before_metric = np.array(main_df['score_main'], dtype=float)\n",
    "after_metric = np.array(df_joined['score'], dtype=float)\n",
    "print(f\"{'Score'}:\\t{ks_2samp(before_metric, after_metric)}\")\n",
    "print(f\"{'Score'}:\\t{np.mean(before_metric)}, {np.mean(after_metric)}\")\n",
    "print(f\"{'Score'}:\\t{np.std(before_metric)}, {np.std(after_metric)}\\n\")\n",
    "\n",
    "for metric in ['members', 'favorites']:\n",
    "    before_metric = np.array(main_df[f\"{metric}_main\"], dtype=int) / len(main_df)\n",
    "    after_metric = np.array(df_joined[metric], dtype=int) / len(df_joined)\n",
    "    print(f\"{metric}:\\t{ks_2samp(before_metric, after_metric)}\")\n",
    "    print(f\"{metric}:\\t{np.mean(before_metric)}, {np.mean(after_metric)}\")\n",
    "    print(f\"{metric}:\\t{np.std(before_metric)}, {np.std(after_metric)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea792c0e",
   "metadata": {},
   "source": [
    "Columns 'score', 'members' are the same: pvalue is close to 1. But colmn 'favorites' has very small pvalue! Distribution of favorites is so diferent now corresponding to pvalue. But considering the value of *'statistic location'* (the maximum difference in CDF between two distributions).\n",
    "We see: statistic_location=np.float64(0.00015024038461538462).\n",
    "As we were making our statistics on the average *'favorites'*, the real region of biggest statistical difference will be:\n",
    "\n",
    "np.float64(0.00015024038461538462) * len(main_df) = 1.0   < == > region of the anime titles, which was added to *'favorites'* list only by one user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d73e09d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64(0.00015024038461538462) * len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d072bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anime_id', 'image_url', 'score', 'scored_by', 'rank', 'opening_theme',\n",
       "       'ending_theme', 'duration_min', 'name', 'type', 'studio', 'source',\n",
       "       'duration', 'genres', 'rating', 'members', 'favorites', 'year_start',\n",
       "       'year_finish', 'month_start', 'month_finish', 'episodes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ece83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.rename(columns={\"name\": \"title\"}, inplace=True)\n",
    "new_order = ['anime_id', 'name', 'rank', 'score', 'type', 'studio', 'genres', 'episodes',\n",
    "             'month_start', 'year_start', 'month_finish', 'year_finish', 'scored_by', 'rating',\n",
    "             'members', 'favorites', 'source', 'opening_theme', 'ending_theme', 'image_url',\n",
    "             'duration', 'duration_min']\n",
    "df_joined = df_joined[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387561ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.to_csv(\"data/helpers/anime_sterilized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6bd87e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3936"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del main_df\n",
    "del df_main\n",
    "del df_joined\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f005f",
   "metadata": {},
   "source": [
    "# **2. Users cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276dcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'user_id', 'user_watching', 'user_completed', 'user_onhold',\n",
       "       'user_dropped', 'user_plantowatch', 'user_days_spent_watching',\n",
       "       'gender', 'location', 'birth_date', 'access_rank', 'join_date',\n",
       "       'last_online', 'stats_mean_score', 'stats_rewatched', 'stats_episodes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = pd.read_csv(\"data/datasets/anime_azathoth42/users_cleaned.csv\")\n",
    "users_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de87c50",
   "metadata": {},
   "source": [
    "Drop columns we will not work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.drop(\n",
    "    columns=['user_id', 'user_watching', 'user_completed', 'user_onhold', 'user_dropped',\n",
    "             'user_plantowatch', 'access_rank', 'last_online', 'stats_rewatched'], inplace=True)\n",
    "users_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c5cd7",
   "metadata": {},
   "source": [
    "Implement special location cleaning. The such column is full of bad strings. We write only countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96af8c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users before cleaning: 88475\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users before cleaning: {users_df['username'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = LocationCleaner()\n",
    "users_df[\"country\"] = users_df[\"location\"].apply(cleaner.clean)\n",
    "\n",
    "users_df = cleaner.map_as_unknown(users_df, column=\"country\", min_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of users after cleaning: {users_df['username'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b3af1",
   "metadata": {},
   "source": [
    "Clean for value *'Unknown'* in the column **'country'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0211fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users_df[users_df['country'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc89c64",
   "metadata": {},
   "source": [
    "There is one user with clean data but without username. As he is from Serbia, we will call him Dragan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78171837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_days_spent_watching</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>join_date</th>\n",
       "      <th>stats_mean_score</th>\n",
       "      <th>stats_episodes</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51311</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.304167</td>\n",
       "      <td>Male</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>1996-12-30 00:00:00</td>\n",
       "      <td>2010-03-01 00:00:00</td>\n",
       "      <td>6.58</td>\n",
       "      <td>2524</td>\n",
       "      <td>Serbia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username  user_days_spent_watching gender location           birth_date  \\\n",
       "51311      NaN                 39.304167   Male   Serbia  1996-12-30 00:00:00   \n",
       "\n",
       "                 join_date  stats_mean_score  stats_episodes country  \n",
       "51311  2010-03-01 00:00:00              6.58            2524  Serbia  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[users_df['username'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "858dec96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_days_spent_watching</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>join_date</th>\n",
       "      <th>stats_mean_score</th>\n",
       "      <th>stats_episodes</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51311</th>\n",
       "      <td>Dragan</td>\n",
       "      <td>39.304167</td>\n",
       "      <td>Male</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>1996-12-30 00:00:00</td>\n",
       "      <td>2010-03-01 00:00:00</td>\n",
       "      <td>6.58</td>\n",
       "      <td>2524</td>\n",
       "      <td>Serbia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username  user_days_spent_watching gender location           birth_date  \\\n",
       "51311   Dragan                 39.304167   Male   Serbia  1996-12-30 00:00:00   \n",
       "\n",
       "                 join_date  stats_mean_score  stats_episodes country  \n",
       "51311  2010-03-01 00:00:00              6.58            2524  Serbia  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.loc[users_df['username'].isna(), 'username'] = 'Dragan'\n",
    "users_df[users_df['username'] == 'Dragan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.to_csv(\"data/helpers/users_sterilized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "044a07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del users_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfce371",
   "metadata": {},
   "source": [
    "# **3. User-Anime relational table cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafc815626bb422b9694d5da2563cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15616866,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197de98328014b2cb2b40f5596c5e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "con.execute(\"\"\"CREATE OR REPLACE TABLE anime AS SELECT * FROM read_csv_auto('data/helpers/anime_sterilized.csv')\"\"\")\n",
    "con.execute(\"\"\"CREATE OR REPLACE TABLE users AS SELECT * FROM read_csv_auto('data/helpers/users_sterilized.csv')\"\"\")\n",
    "con.execute(\"\"\"CREATE OR REPLACE TABLE animelist AS SELECT * FROM read_csv_auto('data/anime_azathoth42/animelists_cleaned.csv')\"\"\")\n",
    "\n",
    "t = con.execute(\"\"\"\n",
    "            SELECT count(*)\n",
    "            FROM animelist\n",
    "            WHERE username IN (SELECT username FROM users)\n",
    "            AND anime_id IN (SELECT anime_id FROM anime)\n",
    "            AND my_score != 0\"\"\").fetchone()\n",
    "\n",
    "print(t)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT username, anime_id, my_watched_episodes, my_score, my_last_updated\n",
    "    FROM animelist\n",
    "    WHERE username IN (SELECT username FROM users)\n",
    "      AND anime_id IN (SELECT anime_id FROM anime)\n",
    "      AND my_score != 0\n",
    "\"\"\"\n",
    "arrow_table = con.execute(query).fetch_arrow_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15616866\n",
      "Wrote rows 0 to 100000\n",
      "Wrote rows 100000 to 200000\n",
      "Wrote rows 200000 to 300000\n",
      "Wrote rows 300000 to 400000\n",
      "Wrote rows 400000 to 500000\n",
      "Wrote rows 500000 to 600000\n",
      "Wrote rows 600000 to 700000\n",
      "Wrote rows 700000 to 800000\n",
      "Wrote rows 800000 to 900000\n",
      "Wrote rows 900000 to 1000000\n",
      "Wrote rows 1000000 to 1100000\n",
      "Wrote rows 1100000 to 1200000\n",
      "Wrote rows 1200000 to 1300000\n",
      "Wrote rows 1300000 to 1400000\n",
      "Wrote rows 1400000 to 1500000\n",
      "Wrote rows 1500000 to 1600000\n",
      "Wrote rows 1600000 to 1700000\n",
      "Wrote rows 1700000 to 1800000\n",
      "Wrote rows 1800000 to 1900000\n",
      "Wrote rows 1900000 to 2000000\n",
      "Wrote rows 2000000 to 2100000\n",
      "Wrote rows 2100000 to 2200000\n",
      "Wrote rows 2200000 to 2300000\n",
      "Wrote rows 2300000 to 2400000\n",
      "Wrote rows 2400000 to 2500000\n",
      "Wrote rows 2500000 to 2600000\n",
      "Wrote rows 2600000 to 2700000\n",
      "Wrote rows 2700000 to 2800000\n",
      "Wrote rows 2800000 to 2900000\n",
      "Wrote rows 2900000 to 3000000\n",
      "Wrote rows 3000000 to 3100000\n",
      "Wrote rows 3100000 to 3200000\n",
      "Wrote rows 3200000 to 3300000\n",
      "Wrote rows 3300000 to 3400000\n",
      "Wrote rows 3400000 to 3500000\n",
      "Wrote rows 3500000 to 3600000\n",
      "Wrote rows 3600000 to 3700000\n",
      "Wrote rows 3700000 to 3800000\n",
      "Wrote rows 3800000 to 3900000\n",
      "Wrote rows 3900000 to 4000000\n",
      "Wrote rows 4000000 to 4100000\n",
      "Wrote rows 4100000 to 4200000\n",
      "Wrote rows 4200000 to 4300000\n",
      "Wrote rows 4300000 to 4400000\n",
      "Wrote rows 4400000 to 4500000\n",
      "Wrote rows 4500000 to 4600000\n",
      "Wrote rows 4600000 to 4700000\n",
      "Wrote rows 4700000 to 4800000\n",
      "Wrote rows 4800000 to 4900000\n",
      "Wrote rows 4900000 to 5000000\n",
      "Wrote rows 5000000 to 5100000\n",
      "Wrote rows 5100000 to 5200000\n",
      "Wrote rows 5200000 to 5300000\n",
      "Wrote rows 5300000 to 5400000\n",
      "Wrote rows 5400000 to 5500000\n",
      "Wrote rows 5500000 to 5600000\n",
      "Wrote rows 5600000 to 5700000\n",
      "Wrote rows 5700000 to 5800000\n",
      "Wrote rows 5800000 to 5900000\n",
      "Wrote rows 5900000 to 6000000\n",
      "Wrote rows 6000000 to 6100000\n",
      "Wrote rows 6100000 to 6200000\n",
      "Wrote rows 6200000 to 6300000\n",
      "Wrote rows 6300000 to 6400000\n",
      "Wrote rows 6400000 to 6500000\n",
      "Wrote rows 6500000 to 6600000\n",
      "Wrote rows 6600000 to 6700000\n",
      "Wrote rows 6700000 to 6800000\n",
      "Wrote rows 6800000 to 6900000\n",
      "Wrote rows 6900000 to 7000000\n",
      "Wrote rows 7000000 to 7100000\n",
      "Wrote rows 7100000 to 7200000\n",
      "Wrote rows 7200000 to 7300000\n",
      "Wrote rows 7300000 to 7400000\n",
      "Wrote rows 7400000 to 7500000\n",
      "Wrote rows 7500000 to 7600000\n",
      "Wrote rows 7600000 to 7700000\n",
      "Wrote rows 7700000 to 7800000\n",
      "Wrote rows 7800000 to 7900000\n",
      "Wrote rows 7900000 to 8000000\n",
      "Wrote rows 8000000 to 8100000\n",
      "Wrote rows 8100000 to 8200000\n",
      "Wrote rows 8200000 to 8300000\n",
      "Wrote rows 8300000 to 8400000\n",
      "Wrote rows 8400000 to 8500000\n",
      "Wrote rows 8500000 to 8600000\n",
      "Wrote rows 8600000 to 8700000\n",
      "Wrote rows 8700000 to 8800000\n",
      "Wrote rows 8800000 to 8900000\n",
      "Wrote rows 8900000 to 9000000\n",
      "Wrote rows 9000000 to 9100000\n",
      "Wrote rows 9100000 to 9200000\n",
      "Wrote rows 9200000 to 9300000\n",
      "Wrote rows 9300000 to 9400000\n",
      "Wrote rows 9400000 to 9500000\n",
      "Wrote rows 9500000 to 9600000\n",
      "Wrote rows 9600000 to 9700000\n",
      "Wrote rows 9700000 to 9800000\n",
      "Wrote rows 9800000 to 9900000\n",
      "Wrote rows 9900000 to 10000000\n",
      "Wrote rows 10000000 to 10100000\n",
      "Wrote rows 10100000 to 10200000\n",
      "Wrote rows 10200000 to 10300000\n",
      "Wrote rows 10300000 to 10400000\n",
      "Wrote rows 10400000 to 10500000\n",
      "Wrote rows 10500000 to 10600000\n",
      "Wrote rows 10600000 to 10700000\n",
      "Wrote rows 10700000 to 10800000\n",
      "Wrote rows 10800000 to 10900000\n",
      "Wrote rows 10900000 to 11000000\n",
      "Wrote rows 11000000 to 11100000\n",
      "Wrote rows 11100000 to 11200000\n",
      "Wrote rows 11200000 to 11300000\n",
      "Wrote rows 11300000 to 11400000\n",
      "Wrote rows 11400000 to 11500000\n",
      "Wrote rows 11500000 to 11600000\n",
      "Wrote rows 11600000 to 11700000\n",
      "Wrote rows 11700000 to 11800000\n",
      "Wrote rows 11800000 to 11900000\n",
      "Wrote rows 11900000 to 12000000\n",
      "Wrote rows 12000000 to 12100000\n",
      "Wrote rows 12100000 to 12200000\n",
      "Wrote rows 12200000 to 12300000\n",
      "Wrote rows 12300000 to 12400000\n",
      "Wrote rows 12400000 to 12500000\n",
      "Wrote rows 12500000 to 12600000\n",
      "Wrote rows 12600000 to 12700000\n",
      "Wrote rows 12700000 to 12800000\n",
      "Wrote rows 12800000 to 12900000\n",
      "Wrote rows 12900000 to 13000000\n",
      "Wrote rows 13000000 to 13100000\n",
      "Wrote rows 13100000 to 13200000\n",
      "Wrote rows 13200000 to 13300000\n",
      "Wrote rows 13300000 to 13400000\n",
      "Wrote rows 13400000 to 13500000\n",
      "Wrote rows 13500000 to 13600000\n",
      "Wrote rows 13600000 to 13700000\n",
      "Wrote rows 13700000 to 13800000\n",
      "Wrote rows 13800000 to 13900000\n",
      "Wrote rows 13900000 to 14000000\n",
      "Wrote rows 14000000 to 14100000\n",
      "Wrote rows 14100000 to 14200000\n",
      "Wrote rows 14200000 to 14300000\n",
      "Wrote rows 14300000 to 14400000\n",
      "Wrote rows 14400000 to 14500000\n",
      "Wrote rows 14500000 to 14600000\n",
      "Wrote rows 14600000 to 14700000\n",
      "Wrote rows 14700000 to 14800000\n",
      "Wrote rows 14800000 to 14900000\n",
      "Wrote rows 14900000 to 15000000\n",
      "Wrote rows 15000000 to 15100000\n",
      "Wrote rows 15100000 to 15200000\n",
      "Wrote rows 15200000 to 15300000\n",
      "Wrote rows 15300000 to 15400000\n",
      "Wrote rows 15400000 to 15500000\n",
      "Wrote rows 15500000 to 15600000\n",
      "Wrote rows 15600000 to 15616866\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 100_000\n",
    "num_rows = arrow_table.num_rows\n",
    "print(num_rows)\n",
    "start = 0\n",
    "first = True\n",
    "\n",
    "with open('data/helpers/animelists_sterilized.csv', 'wb') as f_out:\n",
    "    pac.write_csv(\n",
    "        arrow_table.slice(0, 0),   # пустая таблица, только для хедера\n",
    "        f_out,\n",
    "        write_options=pac.WriteOptions(include_header=True)\n",
    "    )\n",
    "\n",
    "with open('data/helpers/animelists_sterilized.csv', 'ab') as f_out:\n",
    "    start = 0\n",
    "    chunk_size = 100_000\n",
    "    while start < num_rows:\n",
    "        end = min(start + chunk_size, num_rows)\n",
    "        chunk = arrow_table.slice(start, end - start)\n",
    "        \n",
    "        pac.write_csv(\n",
    "            chunk,\n",
    "            f_out,\n",
    "            write_options=pac.WriteOptions(include_header=False)\n",
    "        )\n",
    "        \n",
    "        print(f\"Wrote rows {start} to {end}\")\n",
    "        start = end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MARS)",
   "language": "python",
   "name": "mars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
