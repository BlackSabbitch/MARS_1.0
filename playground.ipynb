{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea2a2ba",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed50085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import fds_tools\n",
    "reload(fds_tools)\n",
    "import common_tools\n",
    "reload(common_tools)\n",
    "import db_tools\n",
    "reload(db_tools)\n",
    "import duckdb\n",
    "reload(duckdb)\n",
    "import pandas as pd\n",
    "from db_tools.mySQL_tools import MySQLTools\n",
    "from fds_tools.fake_users_generator import FakeUsersGenerator\n",
    "from common_tools import CommonTools, PandasTools, DuckDBTools\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250709d",
   "metadata": {},
   "source": [
    "### Constants. Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eebd70e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m users_cleaned = {}\n\u001b[32m      2\u001b[39m users_vectors = {}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata_folder\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/rating_complete.csv\u001b[39m\u001b[33m\"\u001b[39m, chunksize=\u001b[32m100000\u001b[39m):\n\u001b[32m      5\u001b[39m     chunk_cleaned = chunk[chunk[\u001b[33m'\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m'\u001b[39m] != -\u001b[32m1\u001b[39m]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m chunk_cleaned.iterrows():\n",
      "\u001b[31mNameError\u001b[39m: name 'data_folder' is not defined"
     ]
    }
   ],
   "source": [
    "users_cleaned = {}\n",
    "users_vectors = {}\n",
    "\n",
    "for chunk in pd.read_csv(f\"{data_folder}/rating_complete.csv\", chunksize=100000):\n",
    "    chunk_cleaned = chunk[chunk['rating'] != -1]\n",
    "    for index, row in chunk_cleaned.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        anime_id = row['anime_id']\n",
    "        rating = row['rating']\n",
    "        users[user_id][anime_id] = rating\n",
    "\n",
    "anime_data_pd['rating_complete_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7fbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc3faebeb3d4c978ed6aa840f341aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0323cbc0599846ab9c5b76d2254bb65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "players = anime_data_db[\"rating_complete\"].filter(\"rating > 9\")\n",
    "# players = duckdb.query(f\"\"\"select * from {anime_data['rating_complete']}  where rating > 9\"\"\")\n",
    "# players\n",
    "\n",
    "al = anime_data_db[\"animelist\"]\n",
    "a  = anime_data_db[\"anime\"]\n",
    "f = duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT al.user_id) AS n_users\n",
    "FROM animelist AS al\n",
    "JOIN anime AS a\n",
    "    ON al.anime_id = a.MAL_ID\n",
    "WHERE\n",
    "    al.watching_status = 2\n",
    "    AND al.rating != 0\n",
    "    AND TRY_CAST(al.watched_episodes AS INTEGER) IS NOT NULL\n",
    "    AND TRY_CAST(al.watched_episodes AS INTEGER) < TRY_CAST(a.Episodes AS INTEGER);\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "f1 = duckdb.query(\"\"\"\n",
    "SELECT\n",
    "    COUNT( * ) AS n_rows\n",
    "FROM animelist AS al\n",
    "WHERE al.rating == 0\n",
    "\"\"\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e2a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104315 46827035\n",
      "0.0955049133691287\n",
      "0.4287218445102006\n"
     ]
    }
   ],
   "source": [
    "print(f, f1)\n",
    "print(f/109224747*100)\n",
    "print(f1/109224747)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv('data/anime.csv')\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6faaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6fca1137c648f895a054234d0218ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# profile.to_widgets()\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8626934",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidInputException",
     "evalue": "Invalid Input Error: Error when sniffing file \"data/anime_ranks/anime.csv\".\nIt was not possible to automatically detect the CSV parsing dialect\nThe search space used was:\nDelimiter Candidates: ',', '|', ';', '\t'\nQuote/Escape Candidates: ['(no quote)','(no escape)'],['\"','(no escape)'],['\"','\"'],['\"','''],['\"','\\'],[''','(no escape)'],[''','''],[''','\"'],[''','\\']\nComment Candidates: '\\0', '#'\nEncoding: utf-8\nPossible fixes:\n* Disable the parser's strict mode (strict_mode=false) to allow reading rows that do not comply with the CSV standard.\n* Columns are set as: \"columns = { 'MAL_id' : 'BIGINT', 'Episodes' : 'INTEGER'}\", and they contain: 2 columns. It does not match the number of columns found by the sniffer: 35. Verify the columns parameter is correctly set.\n* Make sure you are using the correct file encoding. If not, set it (e.g., encoding = 'utf-16').\n* Set delimiter (e.g., delim=',')\n* Set quote (e.g., quote='\"')\n* Set escape (e.g., escape='\"')\n* Set comment (e.g., comment='#')\n* Set skip (skip=${n}) to skip ${n} lines at the top of the file\n* Enable ignore errors (ignore_errors=true) to ignore potential errors\n* Enable null padding (null_padding=true) to pad missing columns with NULL values\n* Check you are using the correct file compression, otherwise set it (e.g., compression = 'zstd')\n* Be sure that the maximum line size is set to an appropriate value, otherwise set it (e.g., max_line_size=10000000)\n\n\nLINE 11:         JOIN read_csv_auto('data/anime_ranks/anime.csv', \n                      ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidInputException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# получаем уникальные user_id без загрузки всего файла\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m t = \u001b[43mCommonTools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_not_honest_users\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(t.head())\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FCUL/MARS_1.0/common_tools.py:307\u001b[39m, in \u001b[36mCommonTools.find_not_honest_users\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    286\u001b[39m query_find_bad = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[33mSELECT \u001b[39m\n\u001b[32m    288\u001b[39m \u001b[33m    l.user_id,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m \u001b[33m    AND l.watched_episodes < a.Episodes\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Загружаем только нужные строки\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m df_incomplete_votes = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_find_bad\u001b[49m\u001b[43m)\u001b[49m.fetchdf()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_incomplete_votes\n",
      "\u001b[31mInvalidInputException\u001b[39m: Invalid Input Error: Error when sniffing file \"data/anime_ranks/anime.csv\".\nIt was not possible to automatically detect the CSV parsing dialect\nThe search space used was:\nDelimiter Candidates: ',', '|', ';', '\t'\nQuote/Escape Candidates: ['(no quote)','(no escape)'],['\"','(no escape)'],['\"','\"'],['\"','''],['\"','\\'],[''','(no escape)'],[''','''],[''','\"'],[''','\\']\nComment Candidates: '\\0', '#'\nEncoding: utf-8\nPossible fixes:\n* Disable the parser's strict mode (strict_mode=false) to allow reading rows that do not comply with the CSV standard.\n* Columns are set as: \"columns = { 'MAL_id' : 'BIGINT', 'Episodes' : 'INTEGER'}\", and they contain: 2 columns. It does not match the number of columns found by the sniffer: 35. Verify the columns parameter is correctly set.\n* Make sure you are using the correct file encoding. If not, set it (e.g., encoding = 'utf-16').\n* Set delimiter (e.g., delim=',')\n* Set quote (e.g., quote='\"')\n* Set escape (e.g., escape='\"')\n* Set comment (e.g., comment='#')\n* Set skip (skip=${n}) to skip ${n} lines at the top of the file\n* Enable ignore errors (ignore_errors=true) to ignore potential errors\n* Enable null padding (null_padding=true) to pad missing columns with NULL values\n* Check you are using the correct file compression, otherwise set it (e.g., compression = 'zstd')\n* Be sure that the maximum line size is set to an appropriate value, otherwise set it (e.g., max_line_size=10000000)\n\n\nLINE 11:         JOIN read_csv_auto('data/anime_ranks/anime.csv', \n                      ^"
     ]
    }
   ],
   "source": [
    "# получаем уникальные user_id без загрузки всего файла\n",
    "t = CommonTools.find_not_honest_users()\n",
    "print(t.head())\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e86957",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MySQLTools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m         tools.show_databases()\n\u001b[32m     14\u001b[39m     fake_cursor.execute.assert_called_once_with(\u001b[33m\"\u001b[39m\u001b[33mSHOW DATABASES;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtest_show_databases\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_show_databases\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m fake_conn.cursor.return_value = fake_cursor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m patch(\u001b[33m\"\u001b[39m\u001b[33mmysql.connector.connect\u001b[39m\u001b[33m\"\u001b[39m, return_value=fake_conn):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     tools = \u001b[43mMySQLTools\u001b[49m()\n\u001b[32m     12\u001b[39m     tools.show_databases()\n\u001b[32m     14\u001b[39m fake_cursor.execute.assert_called_once_with(\u001b[33m\"\u001b[39m\u001b[33mSHOW DATABASES;\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MySQLTools' is not defined"
     ]
    }
   ],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "def test_show_databases():\n",
    "    fake_conn = MagicMock()\n",
    "    fake_cursor = MagicMock()\n",
    "    \n",
    "    fake_cursor.fetchall.return_value = [(\"test_db\",)]\n",
    "    fake_conn.cursor.return_value = fake_cursor\n",
    "\n",
    "    with patch(\"mysql.connector.connect\", return_value=fake_conn):\n",
    "        tools = MySQLTools()\n",
    "        tools.show_databases()\n",
    "\n",
    "    fake_cursor.execute.assert_called_once_with(\"SHOW DATABASES;\")\n",
    "\n",
    "test_show_databases()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcul",
   "language": "python",
   "name": "fcul"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
