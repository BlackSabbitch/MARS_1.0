{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5724f327",
   "metadata": {
    "id": "5724f327"
   },
   "source": [
    "# Advanced Databases 2025/2026\n",
    "# 66661\n",
    "# 66662\n",
    "# 66663\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc1311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\MARS_1.0\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffa869",
   "metadata": {
    "id": "85ffa869"
   },
   "source": [
    "Connect to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35458caa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1099,
     "status": "ok",
     "timestamp": 1762788389856,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "35458caa",
    "outputId": "6996a8c6-5667-46f3-a456-3fbe0ea4f6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "# define connection string\n",
    "uri = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "# create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567b49b",
   "metadata": {
    "id": "2567b49b"
   },
   "source": [
    "Connect to mySQL instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59c034a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "error",
     "timestamp": 1762788359837,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "c59c034a",
    "outputId": "f579ff02-110d-4a3b-e530-9f7c02aeea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL instance\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host=\"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\",\n",
    "        port=3306,\n",
    "        user=\"ysagan\",\n",
    "        password=\"Duckling25!\",\n",
    "        connection_timeout=10\n",
    "    )\n",
    "    if connection.is_connected():\n",
    "        print(\"Connected to MySQL instance\")\n",
    "except Error as e:\n",
    "    print(\"MySQL Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2bf8e",
   "metadata": {
    "id": "64b2bf8e"
   },
   "source": [
    "Show existing DBs and close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bb459a",
   "metadata": {
    "executionInfo": {
     "elapsed": 14805,
     "status": "aborted",
     "timestamp": 1762788344554,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "44bb459a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information_schema\n",
      "mars_db\n",
      "performance_schema\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SHOW DATABASES;\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row[0])\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2b130",
   "metadata": {
    "id": "31d2b130"
   },
   "source": [
    "Create schema for mySQL db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec5166",
   "metadata": {
    "executionInfo": {
     "elapsed": 14885,
     "status": "aborted",
     "timestamp": 1762788344637,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "98ec5166"
   },
   "outputs": [],
   "source": [
    "# 00_create_schema.py\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "HOST = \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\"\n",
    "USER = \"msamosudova\"\n",
    "PWD  = \"Duckling25!\"\n",
    "DB   = \"mars_db\"\n",
    "\n",
    "DDL = f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS {DB}\n",
    "  DEFAULT CHARACTER SET utf8mb4\n",
    "  COLLATE utf8mb4_0900_ai_ci;\n",
    "\n",
    "USE {DB};\n",
    "\n",
    "-- ---------- LOOKUPS (single-valued) ----------\n",
    "CREATE TABLE IF NOT EXISTS type (\n",
    "  typeID SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  type   VARCHAR(32) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS source (\n",
    "  sourceID SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  source   VARCHAR(64) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS age_rating (\n",
    "  age_ratingID TINYINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  age_rating   VARCHAR(16) NOT NULL UNIQUE,  -- G, PG-13, R-17+, R+, RX\n",
    "  descr        VARCHAR(128) NULL\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "-- ---------- LOOKUPS (multi-valued) ----------\n",
    "CREATE TABLE IF NOT EXISTS genre (\n",
    "  genreID SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  genre   VARCHAR(64) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS studio (\n",
    "  studioID INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  studio   VARCHAR(128) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS producer (\n",
    "  producerID INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  producer   VARCHAR(128) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS licensor (\n",
    "  licensorID INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  licensor   VARCHAR(128) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "-- ---------- CORE ----------\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "  userID INT UNSIGNED PRIMARY KEY,\n",
    "  sex         VARCHAR(16)  NULL,\n",
    "  age         SMALLINT     NULL,\n",
    "  geoLocation VARCHAR(128) NULL\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS anime (\n",
    "  MAL_ID       INT UNSIGNED PRIMARY KEY,\n",
    "  name         VARCHAR(255) NOT NULL,\n",
    "  episodes     INT NULL,\n",
    "  aired        VARCHAR(128) NULL,   #transform in date later\n",
    "  premiered    VARCHAR(32)  NULL,\n",
    "  duration     VARCHAR(64)  NULL,\n",
    "  sourceID     SMALLINT UNSIGNED NULL,\n",
    "  typeID       SMALLINT UNSIGNED NULL,\n",
    "  age_ratingID TINYINT  UNSIGNED NULL,\n",
    "  CONSTRAINT fk_anime_source    FOREIGN KEY (sourceID)     REFERENCES source(sourceID),\n",
    "  CONSTRAINT fk_anime_type      FOREIGN KEY (typeID)       REFERENCES type(typeID),\n",
    "  CONSTRAINT fk_anime_age       FOREIGN KEY (age_ratingID) REFERENCES age_rating(age_ratingID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "-- ---------- BRIDGES (M:N) ----------\n",
    "CREATE TABLE IF NOT EXISTS anime_genre (\n",
    "  MAL_ID  INT UNSIGNED NOT NULL,\n",
    "  genreID SMALLINT UNSIGNED NOT NULL,\n",
    "  PRIMARY KEY (MAL_ID, genreID),\n",
    "  CONSTRAINT fk_ag_anime  FOREIGN KEY (MAL_ID)  REFERENCES anime(MAL_ID),\n",
    "  CONSTRAINT fk_ag_genre  FOREIGN KEY (genreID) REFERENCES genre(genreID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS anime_studio (\n",
    "  MAL_ID   INT UNSIGNED NOT NULL,\n",
    "  studioID INT UNSIGNED NOT NULL,\n",
    "  PRIMARY KEY (MAL_ID, studioID),\n",
    "  CONSTRAINT fk_as_anime  FOREIGN KEY (MAL_ID)   REFERENCES anime(MAL_ID),\n",
    "  CONSTRAINT fk_as_studio FOREIGN KEY (studioID) REFERENCES studio(studioID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS anime_producer (\n",
    "  MAL_ID     INT UNSIGNED NOT NULL,\n",
    "  producerID INT UNSIGNED NOT NULL,\n",
    "  PRIMARY KEY (MAL_ID, producerID),\n",
    "  CONSTRAINT fk_ap_anime    FOREIGN KEY (MAL_ID)     REFERENCES anime(MAL_ID),\n",
    "  CONSTRAINT fk_ap_producer FOREIGN KEY (producerID) REFERENCES producer(producerID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS anime_licensor (\n",
    "  MAL_ID     INT UNSIGNED NOT NULL,\n",
    "  licensorID INT UNSIGNED NOT NULL,\n",
    "  PRIMARY KEY (MAL_ID, licensorID),\n",
    "  CONSTRAINT fk_al_anime     FOREIGN KEY (MAL_ID)     REFERENCES anime(MAL_ID),\n",
    "  CONSTRAINT fk_al_licensor  FOREIGN KEY (licensorID) REFERENCES licensor(licensorID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "-- ---------- FACTS ----------\n",
    "CREATE TABLE IF NOT EXISTS watching_status (\n",
    "  watching_statusID TINYINT UNSIGNED PRIMARY KEY,\n",
    "  watching_status   VARCHAR(32) NOT NULL UNIQUE\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS anime_user_rating (\n",
    "  MAL_ID  INT UNSIGNED NOT NULL,\n",
    "  userID  INT UNSIGNED NOT NULL,\n",
    "  user_rating TINYINT NULL CHECK (user_rating BETWEEN 1 AND 10),\n",
    "  watching_statusID TINYINT UNSIGNED NULL,\n",
    "  watched_episodes  INT NULL,\n",
    "  rated_at DATETIME NULL,\n",
    "  PRIMARY KEY (userID, MAL_ID),\n",
    "  KEY idx_rating_anime (MAL_ID, userID),\n",
    "  KEY idx_rating_user  (userID),\n",
    "  CONSTRAINT fk_aur_anime   FOREIGN KEY (MAL_ID)  REFERENCES anime(MAL_ID),\n",
    "  CONSTRAINT fk_aur_user    FOREIGN KEY (userID)  REFERENCES users(userID),\n",
    "  CONSTRAINT fk_aur_status  FOREIGN KEY (watching_statusID) REFERENCES watching_status(watching_statusID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "-- ---------- AGGREGATES ----------\n",
    "CREATE TABLE IF NOT EXISTS anime_statistics (\n",
    "  MAL_ID        INT UNSIGNED PRIMARY KEY,\n",
    "  score         DECIMAL(4,2) NULL,\n",
    "  `rank`        INT NULL,\n",
    "  popularity    INT NULL,\n",
    "  members       INT NULL,\n",
    "  favorites     INT NULL,\n",
    "  watching      INT NULL,\n",
    "  completed     INT NULL,\n",
    "  on_hold       INT NULL,\n",
    "  dropped       INT NULL,\n",
    "  plan_to_watch INT NULL,\n",
    "  CONSTRAINT fk_ast_anime FOREIGN KEY (MAL_ID) REFERENCES anime(MAL_ID)\n",
    ") ENGINE=InnoDB;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run():\n",
    "    connect = mysql.connector.connect(host=HOST, user=USER, password=PWD)\n",
    "    try:\n",
    "        cursor = connect.cursor()\n",
    "        for stmt in [s.strip() for s in DDL.split(\";\\n\") if s.strip()]:\n",
    "            cursor.execute(stmt)\n",
    "        connect.commit()\n",
    "        print(\"Schema 'mars_db' created/verified.\")\n",
    "    except Error as e:\n",
    "        print(\"MySQL Error:\", e)\n",
    "    finally:\n",
    "        try: cursor.close(); connect.close()\n",
    "        except: pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7f724",
   "metadata": {
    "id": "bce7f724"
   },
   "source": [
    "Create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afedfe",
   "metadata": {
    "executionInfo": {
     "elapsed": 14885,
     "status": "aborted",
     "timestamp": 1762788344641,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "f6afedfe"
   },
   "outputs": [],
   "source": [
    "def ensure_index(cur, table, index_name, columns_csv):\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT 1\n",
    "        FROM information_schema.statistics\n",
    "        WHERE table_schema = %s\n",
    "          AND table_name = %s\n",
    "          AND index_name = %s\n",
    "        LIMIT 1\n",
    "    \"\"\", (DB, table, index_name))\n",
    "    if cur.fetchone() is None:\n",
    "        cur.execute(f\"ALTER TABLE {table} ADD INDEX {index_name} ({columns_csv})\")\n",
    "        print(f\"Created index {index_name} on {table}({columns_csv})\")\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists on {table}\")\n",
    "\n",
    "conn = mysql.connector.connect(host=HOST, user=USER, password=PWD, database=DB, connection_timeout=10)\n",
    "conn.ping(reconnect=True, attempts=3, delay=2)\n",
    "cur = conn.cursor()\n",
    "\n",
    "try:\n",
    "    ensure_index(cur, \"anime_genre\",    \"idx_ag_by_genre\",    \"genreID, MAL_ID\")\n",
    "    ensure_index(cur, \"anime_studio\",   \"idx_as_by_studio\",   \"studioID, MAL_ID\")\n",
    "    ensure_index(cur, \"anime_producer\", \"idx_ap_by_producer\", \"producerID, MAL_ID\")\n",
    "    ensure_index(cur, \"anime_licensor\", \"idx_al_by_licensor\", \"licensorID, MAL_ID\")\n",
    "    ensure_index(cur, \"anime\",          \"idx_anime_name\",     \"name\")\n",
    "    conn.commit()\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd0793",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1762788344669,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "2edd0793"
   },
   "outputs": [],
   "source": [
    "# 01_load_anime_meta.py\n",
    "# Loads anime.csv into: type, source, age_rating, genre, studio, producer, licensor,\n",
    "# and fills anime, anime_statistics, plus bridges: anime_genre, anime_studio, anime_producer, anime_licensor.\n",
    "\n",
    "import os, csv, re\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "HOST = \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\"\n",
    "USER = \"msamosudova\"\n",
    "PWD  = \"Duckling25!\"\n",
    "DB   = \"mars_db\"\n",
    "DATA_DIR = r\"C:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\DataSet\"\n",
    "\n",
    "ANIME_CSV   = os.path.join(DATA_DIR, \"anime.csv\")\n",
    "#ANIME_SYN   = os.path.join(DATA_DIR, \"anime_with_synopsis.csv\")   #optional gor MongoDB\n",
    "\n",
    "BATCH = 1000\n",
    "\n",
    "# ---------- UTILS ----------\n",
    "def split_list(cell):\n",
    "    \"\"\"split comma- or pipe-separated lists from dataset\"\"\"\n",
    "    if cell is None:\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s or s.upper() in {\"UNKNOWN\",\"NONE\",\"NULL\",\"N/A\"}:\n",
    "        return []\n",
    "\n",
    "    parts = re.split(r'[|;,]', s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def clean_str(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_float2(s):\n",
    "    try:\n",
    "        return round(float(s), 2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ---------- DB ----------\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=HOST, user=USER, password=PWD, database=DB, port=3306, connection_timeout=10\n",
    "    )\n",
    "\n",
    "def insert_ignore(cur, table, col, val):\n",
    "    cur.execute(f\"INSERT IGNORE INTO {table} ({col}) VALUES (%s)\", (val,))\n",
    "\n",
    "def get_id(cur, table, id_col, name_col, val, cache):\n",
    "    \"\"\"Upsert into lookup and return id (cached).\"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    if val in cache:\n",
    "        return cache[val]\n",
    "    # try select\n",
    "    cur.execute(f\"SELECT {id_col} FROM {table} WHERE {name_col}=%s\", (val,))\n",
    "    row = cur.fetchone()\n",
    "    if row:\n",
    "        cache[val] = row[0]\n",
    "        return row[0]\n",
    "    # insert\n",
    "    cur.execute(f\"INSERT INTO {table} ({name_col}) VALUES (%s)\", (val,))\n",
    "    cache[val] = cur.lastrowid\n",
    "    return cache[val]\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "def load_anime():\n",
    "    if not os.path.exists(ANIME_CSV):\n",
    "        raise FileNotFoundError(ANIME_CSV)\n",
    "\n",
    "    conn = connect_db()\n",
    "    conn.autocommit = False\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    type_cache = {}\n",
    "    source_cache = {}\n",
    "    age_cache = {}\n",
    "    genre_cache = {}\n",
    "    studio_cache = {}\n",
    "    producer_cache = {}\n",
    "    licensor_cache = {}\n",
    "\n",
    "    # Counters\n",
    "    n_anime = n_stats = n_gen = n_st = n_prod = n_lic = 0\n",
    "\n",
    "    with open(ANIME_CSV, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # columns vary slightly across exports; be defensive\n",
    "    def col(name, row):\n",
    "        # try exact, then case-insensitive\n",
    "        if name in row: return row[name]\n",
    "        for k in row.keys():\n",
    "            if k.lower() == name.lower():\n",
    "                return row[k]\n",
    "        return None\n",
    "\n",
    "    anime_batch = []\n",
    "    stats_batch = []\n",
    "    gen_bridge, st_bridge, prod_bridge, lic_bridge = [], [], [], []\n",
    "\n",
    "    for r in rows:\n",
    "        mal_id  = to_int(col(\"MAL_ID\", r))\n",
    "        if not mal_id:  # skip bad rows\n",
    "            continue\n",
    "\n",
    "        name    = clean_str(col(\"Name\", r))\n",
    "        episodes= to_int(col(\"Episodes\", r))\n",
    "        aired   = clean_str(col(\"Aired\", r))\n",
    "        premiered = clean_str(col(\"Premiered\", r))\n",
    "        duration = clean_str(col(\"Duration\", r))\n",
    "\n",
    "        # lookups single-valued\n",
    "        type_val   = clean_str(col(\"Type\", r))\n",
    "        type_id    = get_id(cur, \"type\", \"typeID\", \"type\", type_val, type_cache) if type_val else None\n",
    "\n",
    "        source_val = clean_str(col(\"Source\", r))\n",
    "        source_id  = get_id(cur, \"source\", \"sourceID\", \"source\", source_val, source_cache) if source_val else None\n",
    "\n",
    "        age_val    = clean_str(col(\"Rating\", r)) or clean_str(col(\"age_rating\", r))\n",
    "        age_id     = get_id(cur, \"age_rating\", \"age_ratingID\", \"age_rating\", age_val, age_cache) if age_val else None\n",
    "\n",
    "        # upsert anime\n",
    "        anime_batch.append((\n",
    "            mal_id, name, episodes, aired, premiered, duration, source_id, type_id, age_id\n",
    "        ))\n",
    "\n",
    "        # stats (denormalized snapshot in anime_statistics)\n",
    "        stats_batch.append((\n",
    "            mal_id,\n",
    "            to_float2(col(\"Score\", r)),\n",
    "            to_int(col(\"Rank\", r)),\n",
    "            to_int(col(\"Popularity\", r)),\n",
    "            to_int(col(\"Members\", r)),\n",
    "            to_int(col(\"Favorites\", r)),\n",
    "            to_int(col(\"Watching\", r)),\n",
    "            to_int(col(\"Completed\", r)),\n",
    "            to_int(col(\"On-Hold\", r)) or to_int(col(\"On_Hold\", r)),\n",
    "            to_int(col(\"Dropped\", r)),\n",
    "            to_int(col(\"Plan to Watch\", r)) or to_int(col(\"Plan_to_Watch\", r))\n",
    "        ))\n",
    "\n",
    "        # multi-valued lists -> lookups + bridges\n",
    "        for g in split_list(col(\"Genres\", r)):\n",
    "            gid = get_id(cur, \"genre\", \"genreID\", \"genre\", g, genre_cache)\n",
    "            gen_bridge.append((mal_id, gid))\n",
    "\n",
    "        for st in split_list(col(\"Studios\", r)):\n",
    "            sid = get_id(cur, \"studio\", \"studioID\", \"studio\", st, studio_cache)\n",
    "            st_bridge.append((mal_id, sid))\n",
    "\n",
    "        for pr in split_list(col(\"Producers\", r)):\n",
    "            pid = get_id(cur, \"producer\", \"producerID\", \"producer\", pr, producer_cache)\n",
    "            prod_bridge.append((mal_id, pid))\n",
    "\n",
    "        for lc in split_list(col(\"Licensors\", r)):\n",
    "            lid = get_id(cur, \"licensor\", \"licensorID\", \"licensor\", lc, licensor_cache)\n",
    "            lic_bridge.append((mal_id, lid))\n",
    "\n",
    "        if len(anime_batch) >= BATCH:\n",
    "            cur.executemany(\"\"\"\n",
    "                INSERT INTO anime (MAL_ID, name, episodes, aired, premiered, duration, sourceID, typeID, age_ratingID)\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                  name=VALUES(name), episodes=VALUES(episodes), aired=VALUES(aired),\n",
    "                  premiered=VALUES(premiered), duration=VALUES(duration),\n",
    "                  sourceID=VALUES(sourceID), typeID=VALUES(typeID), age_ratingID=VALUES(age_ratingID)\n",
    "            \"\"\", anime_batch)\n",
    "            n_anime += len(anime_batch); anime_batch.clear()\n",
    "\n",
    "        if len(stats_batch) >= BATCH:\n",
    "            cur.executemany(\"\"\"\n",
    "                INSERT INTO anime_statistics\n",
    "                  (MAL_ID, score, `rank`, popularity, members, favorites, watching, completed, on_hold, dropped, plan_to_watch)\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                  score=VALUES(score), `rank`=VALUES(`rank`), popularity=VALUES(popularity),\n",
    "                  members=VALUES(members), favorites=VALUES(favorites),\n",
    "                  watching=VALUES(watching), completed=VALUES(completed),\n",
    "                  on_hold=VALUES(on_hold), dropped=VALUES(dropped), plan_to_watch=VALUES(plan_to_watch)\n",
    "            \"\"\", stats_batch)\n",
    "            n_stats += len(stats_batch); stats_batch.clear()\n",
    "\n",
    "        # Bridges batched\n",
    "        if len(gen_bridge) >= BATCH:\n",
    "            cur.executemany(\"INSERT IGNORE INTO anime_genre (MAL_ID, genreID) VALUES (%s,%s)\", gen_bridge)\n",
    "            n_gen += len(gen_bridge); gen_bridge.clear()\n",
    "\n",
    "        if len(st_bridge) >= BATCH:\n",
    "            cur.executemany(\"INSERT IGNORE INTO anime_studio (MAL_ID, studioID) VALUES (%s,%s)\", st_bridge)\n",
    "            n_st += len(st_bridge); st_bridge.clear()\n",
    "\n",
    "        if len(prod_bridge) >= BATCH:\n",
    "            cur.executemany(\"INSERT IGNORE INTO anime_producer (MAL_ID, producerID) VALUES (%s,%s)\", prod_bridge)\n",
    "            n_prod += len(prod_bridge); prod_bridge.clear()\n",
    "\n",
    "        if len(lic_bridge) >= BATCH:\n",
    "            cur.executemany(\"INSERT IGNORE INTO anime_licensor (MAL_ID, licensorID) VALUES (%s,%s)\", lic_bridge)\n",
    "            n_lic += len(lic_bridge); lic_bridge.clear()\n",
    "\n",
    "    # flush remaining\n",
    "    if anime_batch:\n",
    "        cur.executemany(\"\"\"\n",
    "            INSERT INTO anime (MAL_ID, name, episodes, aired, premiered, duration, sourceID, typeID, age_ratingID)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "              name=VALUES(name), episodes=VALUES(episodes), aired=VALUES(aired),\n",
    "              premiered=VALUES(premiered), duration=VALUES(duration),\n",
    "              sourceID=VALUES(sourceID), typeID=VALUES(typeID), age_ratingID=VALUES(age_ratingID)\n",
    "        \"\"\", anime_batch)\n",
    "        n_anime += len(anime_batch)\n",
    "\n",
    "    if stats_batch:\n",
    "        cur.executemany(\"\"\"\n",
    "            INSERT INTO anime_statistics\n",
    "              (MAL_ID, score, `rank`, popularity, members, favorites, watching, completed, on_hold, dropped, plan_to_watch)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "              score=VALUES(score), `rank`=VALUES(`rank`), popularity=VALUES(popularity),\n",
    "              members=VALUES(members), favorites=VALUES(favorites),\n",
    "              watching=VALUES(watching), completed=VALUES(completed),\n",
    "              on_hold=VALUES(on_hold), dropped=VALUES(dropped), plan_to_watch=VALUES(plan_to_watch)\n",
    "        \"\"\", stats_batch)\n",
    "        n_stats += len(stats_batch)\n",
    "\n",
    "    if gen_bridge:\n",
    "        cur.executemany(\"INSERT IGNORE INTO anime_genre (MAL_ID, genreID) VALUES (%s,%s)\", gen_bridge)\n",
    "        n_gen += len(gen_bridge)\n",
    "    if st_bridge:\n",
    "        cur.executemany(\"INSERT IGNORE INTO anime_studio (MAL_ID, studioID) VALUES (%s,%s)\", st_bridge)\n",
    "        n_st += len(st_bridge)\n",
    "    if prod_bridge:\n",
    "        cur.executemany(\"INSERT IGNORE INTO anime_producer (MAL_ID, producerID) VALUES (%s,%s)\", prod_bridge)\n",
    "        n_prod += len(prod_bridge)\n",
    "    if lic_bridge:\n",
    "        cur.executemany(\"INSERT IGNORE INTO anime_licensor (MAL_ID, licensorID) VALUES (%s,%s)\", lic_bridge)\n",
    "        n_lic += len(lic_bridge)\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close(); conn.close()\n",
    "    print(f\"anime upserted: {n_anime}, stats: {n_stats}, links â€” genres:{n_gen} studios:{n_st} producers:{n_prod} licensors:{n_lic}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_anime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e0a23",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1762788344674,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "ca0e0a23"
   },
   "outputs": [],
   "source": [
    "# 02_load_users_and_ratings.py\n",
    "# - autocommit=True (without long transactions)\n",
    "# - ensure users before facts\n",
    "# - validate MAL_ID against anime\n",
    "# - validate watching_statusID against lookup; unknown/0 -> NULL\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "HOST = \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\"\n",
    "USER = \"msamosudova\"\n",
    "PWD  = \"Duckling25!\"\n",
    "DB   = \"mars_db\"\n",
    "\n",
    "DATA_DIR   = r\"C:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\DataSet\"\n",
    "USERS_CSV  = os.path.join(DATA_DIR, \"users.csv\")\n",
    "ANIMELIST  = os.path.join(DATA_DIR, \"animelist.csv\")\n",
    "RATING_CSV = os.path.join(DATA_DIR, \"rating_complete.csv\")\n",
    "WATCH_CSV  = os.path.join(DATA_DIR, \"watching_status.csv\")\n",
    "\n",
    "BATCH_USERS   = 20000\n",
    "BATCH_RATINGS = 5000\n",
    "\n",
    "# ---------- DB ----------\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=HOST, user=USER, password=PWD, database=DB,\n",
    "        port=3306, connection_timeout=10, autocommit=True\n",
    "    )\n",
    "\n",
    "# ---------- UTIL ----------\n",
    "def to_int_safe(v):\n",
    "    try:\n",
    "        if v is None:\n",
    "            return None\n",
    "        s = str(v).strip()\n",
    "        if s == \"\":\n",
    "            return None\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ---------- LOOKUPS FROM DB ----------\n",
    "def load_known_anime_ids(cur):\n",
    "    cur.execute(\"SELECT MAL_ID FROM anime\")\n",
    "    return {row[0] for row in cur.fetchall()}\n",
    "\n",
    "def load_known_status_ids(cur):\n",
    "    cur.execute(\"SELECT watching_statusID FROM watching_status\")\n",
    "    return {row[0] for row in cur.fetchall()}\n",
    "\n",
    "# ---------- LOAD lookup files ----------\n",
    "def upsert_watching_status(cur):\n",
    "    \"\"\"Load watching_status.csv if present.\"\"\"\n",
    "    if not os.path.exists(WATCH_CSV):\n",
    "        return 0\n",
    "    rows = []\n",
    "    with open(WATCH_CSV, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            sid = to_int_safe(r.get(\"status_id\"))\n",
    "            status = (r.get(\"status\") or \"\").strip()\n",
    "            if sid is None or not status:\n",
    "                continue\n",
    "            rows.append((sid, status))\n",
    "    if rows:\n",
    "        cur.executemany(\n",
    "            \"INSERT INTO watching_status (watching_statusID, watching_status) VALUES (%s,%s) \"\n",
    "            \"ON DUPLICATE KEY UPDATE watching_status=VALUES(watching_status)\",\n",
    "            rows\n",
    "        )\n",
    "    return len(rows)\n",
    "\n",
    "def insert_users_from_users_csv(cur):\n",
    "    if not os.path.exists(USERS_CSV):\n",
    "        return 0\n",
    "    buf, total = [], 0\n",
    "    with open(USERS_CSV, encoding=\"utf-8\") as f:\n",
    "        for r in csv.DictReader(f):\n",
    "            uid = to_int_safe(r.get(\"user_id\") or r.get(\"userID\"))\n",
    "            if uid is None:\n",
    "                continue\n",
    "            buf.append((uid,))\n",
    "            if len(buf) >= BATCH_USERS:\n",
    "                cur.executemany(\"INSERT IGNORE INTO users (userID) VALUES (%s)\", buf)\n",
    "                total += len(buf); buf.clear()\n",
    "    if buf:\n",
    "        cur.executemany(\"INSERT IGNORE INTO users (userID) VALUES (%s)\", buf)\n",
    "        total += len(buf)\n",
    "    return total\n",
    "\n",
    "def collect_user_ids_from_file(path):\n",
    "    users = set()\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for r in csv.DictReader(f):\n",
    "            uid = to_int_safe(r.get(\"user_id\") or r.get(\"userID\"))\n",
    "            if uid is not None:\n",
    "                users.add(uid)\n",
    "    return users\n",
    "\n",
    "def ensure_users_for_source(cur, source_path):\n",
    "    if not os.path.exists(source_path):\n",
    "        return 0\n",
    "    users = sorted(collect_user_ids_from_file(source_path))\n",
    "    total, buf = 0, []\n",
    "    for uid in users:\n",
    "        buf.append((uid,))\n",
    "        if len(buf) >= BATCH_USERS:\n",
    "            cur.executemany(\"INSERT IGNORE INTO users (userID) VALUES (%s)\", buf)\n",
    "            total += len(buf); buf.clear()\n",
    "    if buf:\n",
    "        cur.executemany(\"INSERT IGNORE INTO users (userID) VALUES (%s)\", buf)\n",
    "        total += len(buf)\n",
    "    return total\n",
    "\n",
    "# ---------- FACT LOADERS ----------\n",
    "def load_ratings_from_animelist(cur, known_anime, known_status):\n",
    "    \"\"\"\n",
    "    animelist.csv: user_id, anime_id, rating, watching_status, watched_episodes\n",
    "    - rating 0 -> NULL\n",
    "    - watching_status not in lookup (including 0) -> NULL\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ANIMELIST):\n",
    "        return 0\n",
    "    cnt, batch, unknown_ws = 0, [], 0\n",
    "\n",
    "    with open(ANIMELIST, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            uid = to_int_safe(r.get(\"user_id\") or r.get(\"userID\"))\n",
    "            aid = to_int_safe(r.get(\"anime_id\") or r.get(\"MAL_ID\"))\n",
    "            if uid is None or aid is None or aid not in known_anime:\n",
    "                continue\n",
    "\n",
    "            rating = to_int_safe(r.get(\"rating\") or r.get(\"score\"))\n",
    "            if rating is not None and rating == 0:\n",
    "                rating = None\n",
    "\n",
    "            ws = to_int_safe(r.get(\"watching_status\"))\n",
    "            if ws is None or ws not in known_status:\n",
    "                ws = None\n",
    "                unknown_ws += 1\n",
    "\n",
    "            we = to_int_safe(r.get(\"watched_episodes\"))\n",
    "\n",
    "            batch.append((aid, uid, rating, ws, we))\n",
    "\n",
    "            if len(batch) >= BATCH_RATINGS:\n",
    "                batch.sort(key=lambda x: (x[1], x[0]))\n",
    "                cur.executemany(\"\"\"\n",
    "                    INSERT INTO anime_user_rating (MAL_ID, userID, user_rating, watching_statusID, watched_episodes)\n",
    "                    VALUES (%s,%s,%s,%s,%s)\n",
    "                    ON DUPLICATE KEY UPDATE\n",
    "                      user_rating=COALESCE(VALUES(user_rating), user_rating),\n",
    "                      watching_statusID=COALESCE(VALUES(watching_statusID), watching_statusID),\n",
    "                      watched_episodes=COALESCE(VALUES(watched_episodes), watched_episodes)\n",
    "                \"\"\", batch)\n",
    "                cnt += len(batch); batch.clear()\n",
    "\n",
    "    if batch:\n",
    "        batch.sort(key=lambda x: (x[1], x[0]))\n",
    "        cur.executemany(\"\"\"\n",
    "            INSERT INTO anime_user_rating (MAL_ID, userID, user_rating, watching_statusID, watched_episodes)\n",
    "            VALUES (%s,%s,%s,%s,%s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "              user_rating=COALESCE(VALUES(user_rating), user_rating),\n",
    "              watching_statusID=COALESCE(VALUES(watching_statusID), watching_statusID),\n",
    "              watched_episodes=COALESCE(VALUES(watched_episodes), watched_episodes)\n",
    "        \"\"\", batch)\n",
    "        cnt += len(batch)\n",
    "\n",
    "    if unknown_ws:\n",
    "        print(f\"Note: {unknown_ws} rows had unknown/zero watching_status -> stored as NULL.\")\n",
    "    return cnt\n",
    "\n",
    "def load_ratings_from_rating_complete(cur, known_anime):\n",
    "    \"\"\"\n",
    "    rating_complete.csv: user_id, anime_id, rating (1..10)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(RATING_CSV):\n",
    "        return 0\n",
    "    cnt, batch = 0, []\n",
    "\n",
    "    with open(RATING_CSV, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            uid = to_int_safe(r.get(\"user_id\") or r.get(\"userID\"))\n",
    "            aid = to_int_safe(r.get(\"anime_id\") or r.get(\"MAL_ID\"))\n",
    "            rating = to_int_safe(r.get(\"rating\") or r.get(\"score\"))\n",
    "            if uid is None or aid is None or rating is None or rating == 0:\n",
    "                continue\n",
    "            if aid not in known_anime:\n",
    "                continue\n",
    "\n",
    "            batch.append((aid, uid, rating))\n",
    "\n",
    "            if len(batch) >= BATCH_RATINGS:\n",
    "                batch.sort(key=lambda x: (x[1], x[0]))\n",
    "                cur.executemany(\"\"\"\n",
    "                    INSERT INTO anime_user_rating (MAL_ID, userID, user_rating)\n",
    "                    VALUES (%s,%s,%s)\n",
    "                    ON DUPLICATE KEY UPDATE user_rating=VALUES(user_rating)\n",
    "                \"\"\", batch)\n",
    "                cnt += len(batch); batch.clear()\n",
    "\n",
    "    if batch:\n",
    "        batch.sort(key=lambda x: (x[1], x[0]))\n",
    "        cur.executemany(\"\"\"\n",
    "            INSERT INTO anime_user_rating (MAL_ID, userID, user_rating)\n",
    "            VALUES (%s,%s,%s)\n",
    "            ON DUPLICATE KEY UPDATE user_rating=VALUES(user_rating)\n",
    "        \"\"\", batch)\n",
    "        cnt += len(batch)\n",
    "\n",
    "    return cnt\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    conn = connect_db()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # session setup\n",
    "    cur.execute(\"SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED\")\n",
    "    cur.execute(\"SET SESSION innodb_lock_wait_timeout = 50\")\n",
    "\n",
    "    # lookup from db\n",
    "    known_anime = load_known_anime_ids(cur)\n",
    "    print(\"known anime:\", len(known_anime))\n",
    "\n",
    "    # watching_status.csv (if exists)\n",
    "    ws_loaded = upsert_watching_status(cur)\n",
    "    known_status = load_known_status_ids(cur)\n",
    "    print(\"watching_status upserted:\", ws_loaded, \"known statuses:\", sorted(known_status))\n",
    "\n",
    "    # users from users.csv (if exist)\n",
    "    u_from_users = insert_users_from_users_csv(cur)\n",
    "    print(\"users from users.csv inserted/ignored:\", u_from_users)\n",
    "\n",
    "    # ensure users from retings sourse + load facts\n",
    "    if os.path.exists(ANIMELIST):\n",
    "        added = ensure_users_for_source(cur, ANIMELIST)\n",
    "        print(\"ensured users from animelist.csv:\", added)\n",
    "        n = load_ratings_from_animelist(cur, known_anime, known_status)\n",
    "        print(\"animelist rows upserted:\", n)\n",
    "    elif os.path.exists(RATING_CSV):\n",
    "        added = ensure_users_for_source(cur, RATING_CSV)\n",
    "        print(\"ensured users from rating_complete.csv:\", added)\n",
    "        n = load_ratings_from_rating_complete(cur, known_anime)\n",
    "        print(\"rating_complete rows upserted:\", n)\n",
    "    else:\n",
    "        print(\"No ratings file found (expected animelist.csv or rating_complete.csv).\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32247143",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1762788344678,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "32247143"
   },
   "outputs": [],
   "source": [
    "# 03_load_mongo_anime.py\n",
    "# Load Kaggle Anime dataset into MongoDB (collection: anime_db.animes)\n",
    "# Sources:\n",
    "#   - anime_with_synopsis.csv   (required; provides synopsis and often duplicated columns)\n",
    "#   - anime.csv                 (optional; enrich with fields and stats if present)\n",
    "#\n",
    "# Document shape (example):\n",
    "# { _id: 5114, name: \"...\", synopsis: \"...\",\n",
    "#   genres: [...], studios: [...], producers: [...], licensors: [...],\n",
    "#   type: \"TV\", episodes: 64, aired: \"...\", premiered: \"...\", duration: \"...\",\n",
    "#   age_rating: \"PG-13\",\n",
    "#   stats: { score: 9.21, rank: 1, popularity: 1, members: 2850000, favorites: 230000 }\n",
    "# }\n",
    "\n",
    "import os, csv, re, math\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = r\"C:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\DataSet\"\n",
    "CSV_SYN  = os.path.join(DATA_DIR, \"anime_with_synopsis.csv\")  # obligatory\n",
    "CSV_META = os.path.join(DATA_DIR, \"anime.csv\")                # optional\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "DB_NAME   = \"anime_db\"\n",
    "COLL_NAME = \"animes\"\n",
    "\n",
    "BATCH = 5000\n",
    "PROGRESS_EVERY = 50_000\n",
    "\n",
    "# --------------- HELPERS ----------------\n",
    "def clean_str(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def to_int(s):\n",
    "    try:\n",
    "        if s is None or str(s).strip() == \"\":\n",
    "            return None\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_float2(s):\n",
    "    try:\n",
    "        if s is None or str(s).strip() == \"\":\n",
    "            return None\n",
    "        return round(float(s), 2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def split_list(cell):\n",
    "    \"\"\"Split comma/pipe/semicolon separated values into a normalized string array.\"\"\"\n",
    "    if cell is None:\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s or s.upper() in {\"UNKNOWN\",\"NONE\",\"NULL\",\"N/A\"}:\n",
    "        return []\n",
    "    parts = re.split(r'[|;,]', s)\n",
    "    # normalize spaces, drop empty/dups while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        v = p.strip()\n",
    "        if not v:\n",
    "            continue\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            out.append(v)\n",
    "    return out\n",
    "\n",
    "def lower_keys(d):\n",
    "    \"\"\"Return a dict with case-insensitive access via lowercased keys.\"\"\"\n",
    "    return { (k.lower() if isinstance(k, str) else k): v for k,v in d.items() }\n",
    "\n",
    "# --------------- LOAD META (optional enrich) ---------------\n",
    "def load_meta_map():\n",
    "    \"\"\"Read anime.csv if present; return dict MAL_ID -> meta fields.\"\"\"\n",
    "    if not os.path.exists(CSV_META):\n",
    "        print(\"Note: anime.csv not found, will load from anime_with_synopsis.csv only.\")\n",
    "        return {}\n",
    "\n",
    "    meta = {}\n",
    "    with open(CSV_META, encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            r = lower_keys(row)\n",
    "            mid = to_int(r.get(\"mal_id\"))\n",
    "            if not mid:\n",
    "                continue\n",
    "            meta[mid] = {\n",
    "                \"name\"      : clean_str(r.get(\"name\")),\n",
    "                \"episodes\"  : to_int(r.get(\"episodes\")),\n",
    "                \"type\"      : clean_str(r.get(\"type\")),\n",
    "                \"aired\"     : clean_str(r.get(\"aired\")),\n",
    "                \"premiered\" : clean_str(r.get(\"premiered\")),\n",
    "                \"duration\"  : clean_str(r.get(\"duration\")),\n",
    "                \"age_rating\": clean_str(r.get(\"rating\")),  # age code in anime.csv\n",
    "                \"genres\"    : split_list(r.get(\"genres\")),\n",
    "                \"studios\"   : split_list(r.get(\"studios\")),\n",
    "                \"producers\" : split_list(r.get(\"producers\")),\n",
    "                \"licensors\" : split_list(r.get(\"licensors\")),\n",
    "                \"stats\"     : {\n",
    "                    \"score\"     : to_float2(r.get(\"score\")),\n",
    "                    \"rank\"      : to_int(r.get(\"rank\")),\n",
    "                    \"popularity\": to_int(r.get(\"popularity\")),\n",
    "                    \"members\"   : to_int(r.get(\"members\")),\n",
    "                    \"favorites\" : to_int(r.get(\"favorites\")),\n",
    "                }\n",
    "            }\n",
    "    print(f\"Loaded meta for {len(meta):,} anime from anime.csv\")\n",
    "    return meta\n",
    "\n",
    "# --------------- MAIN LOAD ----------------\n",
    "def main():\n",
    "    if not os.path.exists(CSV_SYN):\n",
    "        raise FileNotFoundError(f\"File not found: {CSV_SYN}\")\n",
    "\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db  = client[DB_NAME]\n",
    "    col = db[COLL_NAME]\n",
    "\n",
    "    # indexes (create once; safe to re-run)\n",
    "    # text index: name + synopsis\n",
    "    try:\n",
    "        col.create_index([(\"name\", \"text\"), (\"synopsis\", \"text\")])\n",
    "    except Exception as e:\n",
    "        print(\"create text index:\", e)\n",
    "    # filter indexes\n",
    "    col.create_index([(\"genres\", 1), (\"type\", 1)])\n",
    "    col.create_index([(\"studios\", 1)])\n",
    "    col.create_index([(\"producers\", 1)])\n",
    "    col.create_index([(\"licensors\", 1)])\n",
    "    col.create_index([(\"stats.score\", -1)])\n",
    "\n",
    "    meta_map = load_meta_map()\n",
    "\n",
    "    ops = []\n",
    "    total = 0\n",
    "    upserts = 0\n",
    "\n",
    "    with open(CSV_SYN, encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            r = lower_keys(row)\n",
    "\n",
    "            mal_id = to_int(r.get(\"mal_id\"))\n",
    "            if not mal_id:\n",
    "                continue\n",
    "\n",
    "            # base fields: take from synopsis CSV, fallback to meta_map if missing\n",
    "            name       = clean_str(r.get(\"name\")) or (meta_map.get(mal_id, {}).get(\"name\"))\n",
    "            synopsis   = clean_str(r.get(\"synopsis\"))\n",
    "            episodes   = to_int(r.get(\"episodes\")) if r.get(\"episodes\") is not None else meta_map.get(mal_id, {}).get(\"episodes\")\n",
    "            type_      = clean_str(r.get(\"type\")) or (meta_map.get(mal_id, {}).get(\"type\"))\n",
    "            aired      = clean_str(r.get(\"aired\")) or (meta_map.get(mal_id, {}).get(\"aired\"))\n",
    "            premiered  = clean_str(r.get(\"premiered\")) or (meta_map.get(mal_id, {}).get(\"premiered\"))\n",
    "            duration   = clean_str(r.get(\"duration\")) or (meta_map.get(mal_id, {}).get(\"duration\"))\n",
    "            age_rating = clean_str(r.get(\"rating\")) or (meta_map.get(mal_id, {}).get(\"age_rating\"))\n",
    "\n",
    "            # multi-value arrays (prefer synopsis csv; if empty use meta_map)\n",
    "            genres    = split_list(r.get(\"genres\"))    or meta_map.get(mal_id, {}).get(\"genres\")    or []\n",
    "            studios   = split_list(r.get(\"studios\"))   or meta_map.get(mal_id, {}).get(\"studios\")   or []\n",
    "            producers = split_list(r.get(\"producers\")) or meta_map.get(mal_id, {}).get(\"producers\") or []\n",
    "            licensors = split_list(r.get(\"licensors\")) or meta_map.get(mal_id, {}).get(\"licensors\") or []\n",
    "\n",
    "            # stats\n",
    "            stats = {\n",
    "                \"score\"     : to_float2(r.get(\"score\"))      if \"score\" in r else meta_map.get(mal_id, {}).get(\"stats\", {}).get(\"score\"),\n",
    "                \"rank\"      : to_int(r.get(\"rank\"))          if \"rank\" in r else meta_map.get(mal_id, {}).get(\"stats\", {}).get(\"rank\"),\n",
    "                \"popularity\": to_int(r.get(\"popularity\"))    if \"popularity\" in r else meta_map.get(mal_id, {}).get(\"stats\", {}).get(\"popularity\"),\n",
    "                \"members\"   : to_int(r.get(\"members\"))       if \"members\" in r else meta_map.get(mal_id, {}).get(\"stats\", {}).get(\"members\"),\n",
    "                \"favorites\" : to_int(r.get(\"favorites\"))     if \"favorites\" in r else meta_map.get(mal_id, {}).get(\"stats\", {}).get(\"favorites\"),\n",
    "            }\n",
    "            # drop empty stats keys\n",
    "            stats = {k:v for k,v in stats.items() if v is not None}\n",
    "            if not stats:\n",
    "                stats = None\n",
    "\n",
    "            doc = {\n",
    "                \"_id\": mal_id,\n",
    "                \"name\": name,\n",
    "                \"synopsis\": synopsis,\n",
    "                \"type\": type_,\n",
    "                \"episodes\": episodes,\n",
    "                \"aired\": aired,\n",
    "                \"premiered\": premiered,\n",
    "                \"duration\": duration,\n",
    "                \"age_rating\": age_rating,\n",
    "                \"genres\": genres,\n",
    "                \"studios\": studios,\n",
    "                \"producers\": producers,\n",
    "                \"licensors\": licensors\n",
    "            }\n",
    "            if stats: doc[\"stats\"] = stats\n",
    "\n",
    "            # clean None fields to keep docs compact\n",
    "            doc = {k:v for k,v in doc.items() if v is not None}\n",
    "\n",
    "            ops.append(\n",
    "                UpdateOne({\"_id\": mal_id}, {\"$set\": doc}, upsert=True)\n",
    "            )\n",
    "\n",
    "            total += 1\n",
    "            if len(ops) >= BATCH:\n",
    "                try:\n",
    "                    res = col.bulk_write(ops, ordered=False)\n",
    "                    upserts += (res.upserted_count or 0)\n",
    "                except BulkWriteError as bwe:\n",
    "                    print(\"Bulk error:\", bwe.details.get(\"writeErrors\", [])[:3])\n",
    "                    raise\n",
    "                ops.clear()\n",
    "\n",
    "            if total % PROGRESS_EVERY == 0:\n",
    "                print(f\"processed={total:,} upsertsâ‰ˆ{upserts:,}\")\n",
    "\n",
    "    if ops:\n",
    "        res = col.bulk_write(ops, ordered=False)\n",
    "        upserts += (res.upserted_count or 0)\n",
    "        ops.clear()\n",
    "\n",
    "    print(f\"Done. processed={total:,}, upsertsâ‰ˆ{upserts:,}\")\n",
    "    # simple sanity check\n",
    "    print(\"docs in collection:\", col.estimated_document_count())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ca102",
   "metadata": {
    "executionInfo": {
     "elapsed": 14916,
     "status": "aborted",
     "timestamp": 1762788344681,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "942ca102"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = r\"C:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\DataSet\"\n",
    "CSV_SYN  = os.path.join(DATA_DIR, \"anime_with_synopsis.csv\")\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "COLL_NAME = \"animes\"\n",
    "\n",
    "BATCH = 3000\n",
    "OVERWRITE_EXISTING   = True  # True -> rewrite existing synopsis\n",
    "UPSERT_MISSING_DOCS  = True  # True -> create document if it does not exist\n",
    "\n",
    "# --- helpers ---\n",
    "def to_int(val):\n",
    "    try:\n",
    "        if val is None or str(val).strip() == \"\":\n",
    "            return None\n",
    "        return int(val)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    t = str(s).strip()\n",
    "    return t if t else None\n",
    "\n",
    "def lower_dict(d):\n",
    "    return { (k.lower() if isinstance(k,str) else k): v for k,v in d.items() }\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(CSV_SYN):\n",
    "        raise FileNotFoundError(CSV_SYN)\n",
    "\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    col = client[DB_NAME][COLL_NAME]\n",
    "\n",
    "    # indexes, safe to rerun\n",
    "    col.create_index([(\"name\",\"text\"), (\"synopsis\",\"text\")])\n",
    "\n",
    "    ops, total_rows, upserts = [], 0, 0\n",
    "    updated, skipped = 0, 0\n",
    "\n",
    "    with open(CSV_SYN, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            r = lower_dict(row)\n",
    "            mal_id   = to_int(r.get(\"mal_id\") or r.get(\"id\"))\n",
    "            synopsis = clean_text(r.get(\"synopsis\") or r.get(\"synopsys\") or r.get(\"synopsis_text\") or r.get(\"sypnopsis\"))\n",
    "\n",
    "            if not mal_id:\n",
    "                continue  # no key\n",
    "            if not synopsis:\n",
    "                skipped += 1\n",
    "                continue  # nothing to write\n",
    "\n",
    "            if OVERWRITE_EXISTING:\n",
    "                filt = {\"_id\": mal_id}\n",
    "            else:\n",
    "                filt = {\"_id\": mal_id, \"$or\": [{\"synopsis\": {\"$exists\": False}}, {\"synopsis\": \"\"}]}\n",
    "\n",
    "            ops.append(UpdateOne(filt, {\"$set\": {\"synopsis\": synopsis}}, upsert=UPSERT_MISSING_DOCS))\n",
    "            total_rows += 1\n",
    "\n",
    "            if len(ops) >= BATCH:\n",
    "                try:\n",
    "                    res = col.bulk_write(ops, ordered=False)\n",
    "                    updated += (res.modified_count or 0)\n",
    "                    upserts += (res.upserted_count or 0)\n",
    "                except BulkWriteError as bwe:\n",
    "                    print(\"Bulk error sample:\", bwe.details.get(\"writeErrors\", [])[:3])\n",
    "                    raise\n",
    "                finally:\n",
    "                    ops.clear()\n",
    "\n",
    "    # flush\n",
    "    if ops:\n",
    "        res = col.bulk_write(ops, ordered=False)\n",
    "        updated += (res.modified_count or 0)\n",
    "        upserts += (res.upserted_count or 0)\n",
    "        ops.clear()\n",
    "\n",
    "    print(f\"Done. read_rows={total_rows:,}, updated={updated:,}, upserts={upserts:,}, skipped_empty={skipped:,}\")\n",
    "\n",
    "    # sanity-check\n",
    "    have = col.count_documents({\"synopsis\": {\"$exists\": True, \"$ne\": \"\"}})\n",
    "    total_docs = col.estimated_document_count()\n",
    "    print(f\"Docs with synopsis: {have:,} / {total_docs:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f69ba8",
   "metadata": {
    "executionInfo": {
     "elapsed": 14917,
     "status": "aborted",
     "timestamp": 1762788344684,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "83f69ba8"
   },
   "outputs": [],
   "source": [
    "# 04_load_mongo_alt_names.py\n",
    "# Build anime_db.anime_alternate_names from CSVs (anime_with_synopsis.csv / anime.csv)\n",
    "# Fields:\n",
    "#   _id = MAL_ID\n",
    "#   english_name: str | None\n",
    "#   japanese_name: str | None\n",
    "#   synonyms: [str]\n",
    "\n",
    "import os, csv, re\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = r\"C:\\MariaSamosudova\\Projects\\UNIVER\\ADB\\Project\\DataSet\"\n",
    "CSV_SYN  = os.path.join(DATA_DIR, \"anime_with_synopsis.csv\")\n",
    "CSV_META = os.path.join(DATA_DIR, \"anime.csv\")  # optional\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "DB_NAME   = \"anime_db\"\n",
    "ALT_COLL  = \"anime_alternate_names\"\n",
    "ANIME_COLL= \"animes\"   # optional copying\n",
    "\n",
    "BATCH = 5000\n",
    "PROGRESS_EVERY = 50_000\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def lower_keys(d): return { (k.lower() if isinstance(k,str) else k): v for k,v in d.items() }\n",
    "\n",
    "def get_first(d, keys):\n",
    "    \"\"\"returns first non-empty value by key (case-insensitive).\"\"\"\n",
    "    for k in keys:\n",
    "        v = d.get(k.lower())\n",
    "        if v is not None:\n",
    "            s = str(v).strip()\n",
    "            if s and s.upper() not in {\"NULL\",\"NONE\",\"N/A\",\"UNKNOWN\"}:\n",
    "                return s\n",
    "    return None\n",
    "\n",
    "def split_synonyms(val):\n",
    "    \"\"\"takes in account different delimiters\"\"\"\n",
    "    if val is None:\n",
    "        return []\n",
    "    parts = re.split(r'[|;,]', str(val))\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        t = p.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def load_map_from_csv(path):\n",
    "    \"\"\"returns dict MAL_ID -> {english_name, japanese_name, synonyms[]} from file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    out = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            r = lower_keys(row)\n",
    "            # MAL_ID\n",
    "            mal_id = get_first(r, [\"mal_id\", \"id\", \"malid\"])\n",
    "            if not mal_id or not mal_id.isdigit():\n",
    "                continue\n",
    "            mal_id = int(mal_id)\n",
    "\n",
    "            english = get_first(r, [\"english name\", \"english_name\", \"english\", \"title_english\"])\n",
    "            japanese= get_first(r, [\"japanese name\", \"japanese_name\", \"japanese\", \"title_japanese\"])\n",
    "            # synonyms could be named differently; try typical variants\n",
    "            syn_raw = get_first(r, [\"synonyms\", \"other name\", \"other names\", \"other_names\", \"title_synonyms\"])\n",
    "            synonyms = split_synonyms(syn_raw)\n",
    "\n",
    "            out[mal_id] = {\n",
    "                \"english_name\": english,\n",
    "                \"japanese_name\": japanese,\n",
    "                \"synonyms\": synonyms\n",
    "            }\n",
    "    return out\n",
    "\n",
    "def merge_alt(a, b):\n",
    "    \"\"\"merge records of alternative titles with deduplication\"\"\"\n",
    "    if not a: return b or {}\n",
    "    if not b: return a or {}\n",
    "    english = a.get(\"english_name\") or b.get(\"english_name\")\n",
    "    japanese= a.get(\"japanese_name\") or b.get(\"japanese_name\")\n",
    "    s1 = a.get(\"synonyms\") or []\n",
    "    s2 = b.get(\"synonyms\") or []\n",
    "    seen, syn = set(), []\n",
    "    for x in s1 + s2:\n",
    "        if not x:\n",
    "            continue\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            syn.append(x)\n",
    "    return {\"english_name\": english, \"japanese_name\": japanese, \"synonyms\": syn}\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    alt = db[ALT_COLL]\n",
    "    animes = db[ANIME_COLL]\n",
    "\n",
    "    # indexes (safe to re-run)\n",
    "    alt.create_index([(\"english_name\", 1)])\n",
    "    alt.create_index([(\"japanese_name\", 1)])\n",
    "    alt.create_index([(\"synonyms\", 1)])\n",
    "\n",
    "    # load both maps (order: synopsis csv has priority, then anime.csv as fallback)\n",
    "    map_syn  = load_map_from_csv(CSV_SYN)\n",
    "    map_meta = load_map_from_csv(CSV_META)\n",
    "\n",
    "    # union of keys\n",
    "    all_ids = set(map_syn.keys()) | set(map_meta.keys())\n",
    "\n",
    "    ops_alt, ops_embed = [], []\n",
    "    total = upserts = 0\n",
    "\n",
    "    for mal_id in all_ids:\n",
    "        a = map_syn.get(mal_id)\n",
    "        b = map_meta.get(mal_id)\n",
    "        rec = merge_alt(a, b)\n",
    "\n",
    "        # clean empty\n",
    "        rec[\"synonyms\"] = [s for s in (rec.get(\"synonyms\") or []) if s]\n",
    "        if not rec.get(\"english_name\") and not rec.get(\"japanese_name\") and not rec[\"synonyms\"]:\n",
    "            # if no info exists\n",
    "            continue\n",
    "\n",
    "        # upsert into dedicated alt collection\n",
    "        doc = {\"_id\": mal_id}\n",
    "        if rec.get(\"english_name\"):  doc[\"english_name\"]  = rec[\"english_name\"]\n",
    "        if rec.get(\"japanese_name\"): doc[\"japanese_name\"] = rec[\"japanese_name\"]\n",
    "        if rec[\"synonyms\"]:          doc[\"synonyms\"]      = rec[\"synonyms\"]\n",
    "\n",
    "        ops_alt.append(UpdateOne({\"_id\": mal_id}, {\"$set\": doc}, upsert=True))\n",
    "\n",
    "        total += 1\n",
    "        if len(ops_alt) >= BATCH:\n",
    "            res = alt.bulk_write(ops_alt, ordered=False)\n",
    "            upserts += (res.upserted_count or 0)\n",
    "            ops_alt.clear()\n",
    "\n",
    "        if len(ops_embed) >= BATCH:\n",
    "            animes.bulk_write(ops_embed, ordered=False)\n",
    "            ops_embed.clear()\n",
    "\n",
    "        if total % PROGRESS_EVERY == 0:\n",
    "            print(f\"processed={total:,} upserts_altâ‰ˆ{upserts:,}\")\n",
    "\n",
    "    if ops_alt:\n",
    "        res = alt.bulk_write(ops_alt, ordered=False)\n",
    "        upserts += (res.upserted_count or 0)\n",
    "        ops_alt.clear()\n",
    "\n",
    "    if ops_embed:\n",
    "        animes.bulk_write(ops_embed, ordered=False)\n",
    "        ops_embed.clear()\n",
    "\n",
    "    print(f\"Done. processed={total:,}, alt upsertsâ‰ˆ{upserts:,}\")\n",
    "    print(\"docs in alt collection:\", alt.estimated_document_count())\n",
    "\n",
    "    # helpful text index for searching alt names\n",
    "    try:\n",
    "        alt.create_index([(\"english_name\", \"text\"), (\"japanese_name\", \"text\"), (\"synonyms\", \"text\")])\n",
    "    except Exception as e:\n",
    "        print(\"create text index (alt):\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80adbf4",
   "metadata": {
    "executionInfo": {
     "elapsed": 14914,
     "status": "aborted",
     "timestamp": 1762788344686,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "f80adbf4"
   },
   "outputs": [],
   "source": [
    "# 100 best rated\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "db_config = {\n",
    "    \"host\": \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\",\n",
    "    \"user\": \"msamosudova\",           # or ysagan / amaksymchuk\n",
    "    \"password\": \"Duckling25!\",\n",
    "    \"database\": \"mars_db\",\n",
    "    \"port\": 3306\n",
    "}\n",
    "\n",
    "# ---------- SQL QUERY ----------\n",
    "query = \"\"\"\n",
    "-- Get top 100 best rated anime (based on MAL score, fallback-safe)\n",
    "SELECT\n",
    "    a.MAL_ID,\n",
    "    a.name,\n",
    "    COALESCE(s.score, 0) AS rating,\n",
    "    s.members,\n",
    "    s.favorites,\n",
    "    s.popularity\n",
    "FROM anime a\n",
    "JOIN anime_statistics s USING (MAL_ID)\n",
    "WHERE s.score IS NOT NULL\n",
    "  AND COALESCE(s.members, 0) >= 1000     -- small popularity guard\n",
    "ORDER BY s.score DESC, s.members DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "# ---------- EXECUTION ----------\n",
    "try:\n",
    "    conn = mysql.connector.connect(**db_config)\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"\\n=== TOP 100 BEST-RATED ANIME ===\")\n",
    "    print(df.head(20).to_string(index=False))  # show first 20\n",
    "    print(f\"\\nTotal rows fetched: {len(df)}\")\n",
    "\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"MySQL Error:\", e)\n",
    "\n",
    "finally:\n",
    "    if 'conn' in locals() and conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcc638",
   "metadata": {
    "executionInfo": {
     "elapsed": 14916,
     "status": "aborted",
     "timestamp": 1762788344688,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "9cbcc638"
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "sql_config = {\n",
    "    \"host\": \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\",\n",
    "    \"user\": \"msamosudova\",\n",
    "    \"password\": \"Duckling25!\",\n",
    "    \"database\": \"mars_db\"\n",
    "}\n",
    "mongo_uri = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "mongo_db = \"anime_db\"\n",
    "mongo_coll = \"animes\"\n",
    "\n",
    "# ---------- STEP 1 â€” Top 100 IDs from MySQL ----------\n",
    "sql = mysql.connector.connect(**sql_config)\n",
    "query = \"\"\"\n",
    "SELECT a.MAL_ID, a.name, s.score\n",
    "FROM anime a\n",
    "JOIN anime_statistics s USING (MAL_ID)\n",
    "WHERE s.score IS NOT NULL\n",
    "ORDER BY s.score DESC, s.members DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, sql)\n",
    "sql.close()\n",
    "\n",
    "# ---------- STEP 2 â€” Fetch synopsis from MongoDB ----------\n",
    "client = MongoClient(mongo_uri)\n",
    "coll = client[mongo_db][mongo_coll]\n",
    "\n",
    "syn_map = {\n",
    "    doc[\"_id\"]: doc.get(\"synopsis\", \"\")\n",
    "    for doc in coll.find(\n",
    "        {\"_id\": {\"$in\": df[\"MAL_ID\"].tolist()}},\n",
    "        {\"_id\": 1, \"synopsis\": 1}\n",
    "    )\n",
    "}\n",
    "\n",
    "df[\"synopsis\"] = df[\"MAL_ID\"].map(syn_map)\n",
    "\n",
    "# ---------- STEP 3 â€” Display ----------\n",
    "print(\"\\n=== TOP 100 ANIME WITH SYNOPSIS (MySQL + Mongo) ===\")\n",
    "for _, row in df.iterrows():\n",
    "    text = (row[\"synopsis\"] or \"\")[:400]\n",
    "    print(f\"\\n{row['name']}  ({row['score']})\\n{text}...\")\n",
    "\n",
    "print(f\"\\nTotal rows fetched: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319671d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 14916,
     "status": "aborted",
     "timestamp": 1762788344690,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "319671d4"
   },
   "outputs": [],
   "source": [
    "# fetch_top_r_anime_with_genres.py\n",
    "# Top 30 best-rated anime with age rating starting with â€œRâ€\n",
    "# It also shows all associated genres for each title from MongoDB\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ---------- MySQL CONFIG ----------\n",
    "MYSQL = {\n",
    "    \"host\": \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\",\n",
    "    \"user\": \"msamosudova\",\n",
    "    \"password\": \"Duckling25!\",\n",
    "    \"database\": \"mars_db\",\n",
    "    \"port\": 3306,\n",
    "}\n",
    "\n",
    "# ---------- MongoDB CONFIG ----------\n",
    "MONGO_URI = \"mongodb+srv://msamosudova:Duckling@mars-cluster.8ruotdw.mongodb.net/?appName=MARS-Cluster\"\n",
    "MONGO_DB  = \"anime_db\"\n",
    "MONGO_COL = \"animes\"\n",
    "\n",
    "# ---------- PARAMETERS ----------\n",
    "AGE_RATING_FILTER = \"R%\"   # matches R-17+ and R+\n",
    "MIN_MEMBERS = 1000\n",
    "LIMIT = 30\n",
    "\n",
    "SQL_TOP_R = \"\"\"\n",
    "SELECT\n",
    "    a.MAL_ID,\n",
    "    a.name,\n",
    "    s.score  AS rating,\n",
    "    s.members\n",
    "FROM anime a\n",
    "JOIN anime_statistics s USING (MAL_ID)\n",
    "JOIN age_rating ar ON ar.age_ratingID = a.age_ratingID\n",
    "WHERE ar.age_rating LIKE %s\n",
    "  AND s.score IS NOT NULL\n",
    "  AND COALESCE(s.members, 0) >= %s\n",
    "ORDER BY s.score DESC, s.members DESC\n",
    "LIMIT %s;\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # 1) Fetch candidates from MySQL\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**MYSQL)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(SQL_TOP_R, (AGE_RATING_FILTER, MIN_MEMBERS, LIMIT))\n",
    "        rows = cur.fetchall()  # list of tuples: (MAL_ID, name, rating, members)\n",
    "    except Error as e:\n",
    "        print(\"MySQL Error:\", e); return\n",
    "    finally:\n",
    "        try: cur.close(); conn.close()\n",
    "        except: pass\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No results from MySQL.\"); return\n",
    "\n",
    "    # 2) Pull genres from Mongo for those MAL_IDs\n",
    "    ids = [int(r[0]) for r in rows]\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    coll = client[MONGO_DB][MONGO_COL]\n",
    "\n",
    "    cursor = coll.find({\"_id\": {\"$in\": ids}}, {\"_id\": 1, \"genres\": 1})\n",
    "    genres_map = {}\n",
    "    for d in cursor:\n",
    "        g = d.get(\"genres\")\n",
    "        # normalize to list[str]\n",
    "        if isinstance(g, list):\n",
    "            genres_map[int(d[\"_id\"])] = [str(x) for x in g if x]\n",
    "        elif isinstance(g, str):\n",
    "            # in case genres accidentally stored as comma-separated string\n",
    "            genres_map[int(d[\"_id\"])] = [s.strip() for s in g.split(\",\") if s.strip()]\n",
    "        else:\n",
    "            genres_map[int(d[\"_id\"])] = []\n",
    "\n",
    "    # 3) Print\n",
    "    print(f\"\\nTop {LIMIT} R-rated anime (genres from Mongo):\\n\")\n",
    "    for mal_id, name, score, members in rows:\n",
    "        glist = genres_map.get(int(mal_id), [])\n",
    "        genres_str = \", \".join(sorted(set(glist))) if glist else \"â€”\"\n",
    "        print(f\"- [{mal_id}] {name} â€” {score}| members={members:,} | genres: {genres_str}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7ee877",
   "metadata": {
    "executionInfo": {
     "elapsed": 14914,
     "status": "aborted",
     "timestamp": 1762788344691,
     "user": {
      "displayName": "Iaroslav Sagan",
      "userId": "05986993173717006978"
     },
     "user_tz": 0
    },
    "id": "2d7ee877"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samos\\AppData\\Local\\Temp\\ipykernel_21192\\3878846789.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended anime for user 21 based on their favorite genres:\n",
      "\n",
      " 1. Fullmetal Alchemist: Brotherhood (9.19, 2,248,456 members)\n",
      "   Genres: Drama\n",
      "\n",
      " 2. Shingeki no Kyojin Season 3 Part 2 (9.1, 1,073,626 members)\n",
      "   Genres: Drama, Mystery\n",
      "\n",
      " 3. 3-gatsu no Lion 2nd Season (9.0, 266,243 members)\n",
      "   Genres: Drama, Slice of Life\n",
      "\n",
      " 4. Owarimonogatari 2nd Season (8.93, 270,878 members)\n",
      "   Genres: Mystery\n",
      "\n",
      " 5. Shigatsu wa Kimi no Uso (8.74, 1,442,330 members)\n",
      "   Genres: Drama\n",
      "\n",
      " 6. Haikyuu!! Second Season (8.73, 874,150 members)\n",
      "   Genres: Drama\n",
      "\n",
      " 7. Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai (8.68, 359,086 members)\n",
      "   Genres: Drama\n",
      "\n",
      " 8. Mushishi Zoku Shou: Suzu no Shizuku (8.63, 96,435 members)\n",
      "   Genres: Mystery, Slice of Life\n",
      "\n",
      " 9. Yuru Campâ–³ Season 2 (8.61, 101,971 members)\n",
      "   Genres: Slice of Life\n",
      "\n",
      "10. Shouwa Genroku Rakugo Shinjuu (8.6, 223,244 members)\n",
      "   Genres: Drama\n",
      "\n",
      "Total recommendations: 10\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DB = {\n",
    "    \"host\": \"mars-db.cdcmiuuwqo5z.eu-north-1.rds.amazonaws.com\",\n",
    "    \"user\": \"msamosudova\",        \n",
    "    \"password\": \"Duckling25!\",\n",
    "    \"database\": \"mars_db\",\n",
    "    \"port\": 3306,\n",
    "}\n",
    "\n",
    "USER_ID = 21\n",
    "LIMIT = 10\n",
    "\n",
    "# ---------- SQL QUERY ----------\n",
    "query = f\"\"\"\n",
    "WITH\n",
    "user_avg AS (\n",
    "  SELECT AVG(user_rating) AS mu\n",
    "  FROM anime_user_rating\n",
    "  WHERE userID = {USER_ID} AND user_rating IS NOT NULL\n",
    "),\n",
    "user_genre AS (\n",
    "  SELECT g.genreID,\n",
    "         AVG(r.user_rating - (SELECT mu FROM user_avg)) AS lift,\n",
    "         COUNT(*) AS n_in_genre\n",
    "  FROM anime_user_rating r\n",
    "  JOIN anime_genre ag ON ag.MAL_ID = r.MAL_ID\n",
    "  JOIN genre g ON g.genreID = ag.genreID\n",
    "  WHERE r.userID = {USER_ID} AND r.user_rating IS NOT NULL\n",
    "  GROUP BY g.genreID\n",
    "),\n",
    "top_genres AS (\n",
    "  SELECT genreID\n",
    "  FROM user_genre\n",
    "  WHERE n_in_genre >= 5\n",
    "  ORDER BY lift DESC, n_in_genre DESC\n",
    "  LIMIT 5\n",
    "),\n",
    "seen AS (\n",
    "  SELECT MAL_ID FROM anime_user_rating WHERE userID = {USER_ID}\n",
    ")\n",
    "SELECT\n",
    "  a.MAL_ID,\n",
    "  a.name,\n",
    "  s.score,\n",
    "  s.members,\n",
    "  GROUP_CONCAT(DISTINCT g.genre ORDER BY g.genre SEPARATOR ', ') AS genres\n",
    "FROM anime a\n",
    "JOIN anime_statistics s USING (MAL_ID)\n",
    "JOIN anime_genre ag ON ag.MAL_ID = a.MAL_ID\n",
    "JOIN genre g ON g.genreID = ag.genreID\n",
    "WHERE ag.genreID IN (SELECT genreID FROM top_genres)\n",
    "  AND a.MAL_ID NOT IN (SELECT MAL_ID FROM seen)\n",
    "  AND s.score IS NOT NULL\n",
    "  AND COALESCE(s.members, 0) >= 1000\n",
    "GROUP BY a.MAL_ID, a.name, s.score, s.members\n",
    "ORDER BY s.score DESC, s.members DESC\n",
    "LIMIT {LIMIT};\n",
    "\"\"\"\n",
    "\n",
    "# ---------- EXECUTION ----------\n",
    "try:\n",
    "    conn = mysql.connector.connect(**DB)\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    print(f\"\\nRecommended anime for user {USER_ID} based on their favorite genres:\\n\")\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"{i+1:2d}. {row['name']} ({row['score']}, {row['members']:,} members)\\n   Genres: {row['genres']}\\n\")\n",
    "\n",
    "    print(f\"Total recommendations: {len(df)}\")\n",
    "\n",
    "except Error as e:\n",
    "    print(\"MySQL Error:\", e)\n",
    "\n",
    "finally:\n",
    "    if 'conn' in locals() and conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"Connection closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
